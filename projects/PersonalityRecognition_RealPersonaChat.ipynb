{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "# Big Five Personality Recognition using LUKE on RealPersonaChat\n\n**Project Overview:**\n- Dataset: RealPersonaChat (14,000 dialogues, 233 speakers) **← Using full dataset**\n- Task: Big Five personality trait regression prediction\n- Model: LUKE (studio-ousia/luke-japanese-base)\n- Setting: Monologue (speaker's utterances only)\n\n**Evaluation Metrics:**\n- Regression: MAE, RMSE, Pearson, Spearman correlation\n- Classification: Accuracy, Balanced Accuracy, Precision, Recall, F1\n\n**Memory Optimization (T4 GPU):**\n- Batch size: 4\n- Max Length: 256 tokens\n- Gradient accumulation: 8 steps (effective batch size 32)\n- Gradient checkpointing: Enabled\n- Mixed Precision (FP16): Enabled\n\n**Execution Instructions:**\n\nExecute all cells in order from top to bottom.\n\n**Data Loading:**\n- Speaker data: Download from GitHub (233 speakers)\n- Dialogue data: Download from GitHub (14,000 dialogues, takes 30-40 minutes)\n\n**Recommended Environment:**\n- Google Colab with T4 GPU (15GB VRAM)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpQBruNV2eB1"
   },
   "source": "## 1. Environment Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIUAvxtp2eB2",
    "outputId": "f6b94ff4-c49a-4937-9e2c-ef3221b5d1a6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# Import libraries\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import (\n    AutoTokenizer,\n    LukeModel,\n    get_linear_schedule_with_warmup\n)\nimport json\nimport requests\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    mean_absolute_error,\n    mean_squared_error,\n    accuracy_score,\n    balanced_accuracy_score,\n    precision_recall_fscore_support\n)\nfrom scipy.stats import pearsonr, spearmanr\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configure device (GPU/CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\n# Model and data configuration\nMODEL_NAME = \"studio-ousia/luke-japanese-base\"\nBIG_FIVE_TRAITS = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\nCHECKPOINT_DIR = \"/content/checkpoints\""
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "Ka9sYMGW2eB2",
    "outputId": "847a76ce-9f3c-476f-bba5-f08768e71c5d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "## 2. Data Loading\n\nLoad RealPersonaChat dataset directly from GitHub.\n- Speaker data: `interlocutors.json` (233 speakers)\n- Dialogue data: `dialogues/*.json` (14,000 dialogues)\n\n**Data Size:**\n- **Production (Recommended)**: `num_dialogues = 14000` - Full dataset\n- Test: `num_dialogues = 1000` - Partial dataset for quick testing\n\n**Note**: \n- Initial data loading takes 30-40 minutes for full dataset (14,000 dialogues)\n- Using full data is strongly recommended for better results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJX4RnZ-2eB2",
    "outputId": "a8674c97-c2fe-4ca5-f9b9-8a03ca63a2ab",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245,
     "referenced_widgets": [
      "75a35ded60e14c808d30e1aa34deaf21",
      "37d8c688def642fd82d42cc2d9a5596a",
      "3cc59d8c4a8f4e5196d7a50e7b2bc7a8",
      "2b1e1e0a40534d8f9e8da461b9cd553c",
      "5de5e2385c274dcd8cd5db3dad22ddbc",
      "cfd48a54d7e84e3097a4addc1b60a26b",
      "1197f1dbde094b7ca5ed306563df6a54",
      "7f66d87c60d74ad68c2b5735fca22a03",
      "d48297d248ce4ef49a970a48befa4160",
      "ce2206154e124bb9be4f1cf62d302b98",
      "a858b2b2667449bc9aa7424ec193e72c",
      "ac2fc283071b403197529c05b3abaf5b",
      "f7b585d985d443038124bd4d1304e0ed",
      "34b18acc56c74d3e889bd9acb90a231e",
      "8269f7e4cb2f4b3280f13576cc8a9067",
      "2d50ea127381455e8d7087d286ed3d20",
      "f66af1fbccb04908a56cd7368511984e",
      "f71b84d7ed6c4c2fbeee3589d6d7bc0b",
      "5e269279a2984893abef447bf7fb83aa",
      "4ddea5a3fc844e0b83a19066bd864497",
      "5d162464fa974a478f1905f85afa8fdb",
      "009698e6dd0449088018ec89e7e860ea",
      "e5c8dba9d5e24866aa24d306eb24c5c7",
      "4ea4e265c1614d03b5607ecd5bee5238",
      "ac7efa2870894d589ef40439be24cefb",
      "70621441faec49269faeb0bce389030c",
      "05c81806aeb04c22b37fadb32eb39f81",
      "6c841fa9eecc45ccb37be28ad1d7d9f7",
      "fefd716796ac4f7aa7d7811d4bd1b143",
      "0a8d9203c073495481ab2563165922b0",
      "6ead8437c0ed4a7e8b58d88441a30c06",
      "7c75e7cee224402eb9264a5d6d2fece8",
      "a51bd7ef950e45c0a2ec74f85a9acba3",
      "1b900fe53a904f40941269536ae34cd9",
      "9102faf79ad4469298373707c36f043e",
      "b2b62472255c4115bfc275d120643268",
      "d10c2d2f4d1746dfb4f6d5432f41aad3",
      "7e76ff6433c14cf2b5791e7368fe1374",
      "3467d3c29e92473794ab7c649ac9dc8c",
      "626973975b8e4ae5a7c58fdadc027fc7",
      "61f3cac7ddab4397be0fe7cd61a6b2a8",
      "56699d9d97c44325884844a005214fef",
      "23a06c0f189e4fa79b21f3f1df444494",
      "a163ee10499b46dd8e74ec8b99b78ca4",
      "2793fe8dc84d48ff933b06d8a3a420c5",
      "d064a00f082946d7a309ef9b78a8a539",
      "973ce485786749b08aea4c957558a605",
      "36dc5a931b4645b29d7630a010d025af",
      "451947b6e980488f8a251cd719e5e274",
      "2e260159c16f483da0362e699d130970",
      "ff7a609e77e34585bd9545b6e01454d4",
      "5293ef9264f1448db087d651a50ef922",
      "4d82e06da71447d5b343c5067ff623f5",
      "346659e76a69418e8eadb1bdf2468847",
      "5384f1c07a46425aa2848ebb2515e1df",
      "0d99fc2b221446939b7f1a7188e9753c",
      "8479d3087bf0402b9a59fec1021007ca",
      "c385f952ee9a4f57b3a9f35a093761e0",
      "5bcf7c04c7864726a17fd259f5b54e3a",
      "a2104007458b4f18ae8c5bef8cd4446f",
      "de9866967c574a068327cbce2215da65",
      "31615ffb4b574603970f840a090e9a38",
      "a129819275474f2fbc41d44922acff2f",
      "de9e6efb43594e9ba99bb5def2165ecc",
      "103ed724b09c4d09a62fb4b892c12692",
      "fa00c020c3f944f0bcb0f602d319be46"
     ]
    }
   },
   "outputs": [],
   "source": "# Load dataset directly from GitHub\nimport json\nimport requests\nfrom tqdm.auto import tqdm\n\nBASE_URL = \"https://raw.githubusercontent.com/nu-dialogue/real-persona-chat/main/real_persona_chat\"\n\n# Load speaker data (interlocutors)\nprint(\"=\"*70)\nprint(\"Loading speaker data...\")\nprint(\"=\"*70)\n\ninterlocutors_url = f\"{BASE_URL}/interlocutors.json\"\nresponse = requests.get(interlocutors_url)\ninterlocutors_raw = response.json()\n\n# Convert data structure to dictionary\nif isinstance(interlocutors_raw, dict):\n    interlocutor_dict = interlocutors_raw\nelif isinstance(interlocutors_raw, list):\n    interlocutor_dict = {\n        item['interlocutor_id']: item\n        for item in interlocutors_raw\n    }\nelse:\n    print(f\"Unexpected data type: {type(interlocutors_raw)}\")\n    interlocutor_dict = {}\n\nprint(f\"Loaded {len(interlocutor_dict)} speakers\")\n\n# Display sample\nif interlocutor_dict:\n    first_speaker_id = list(interlocutor_dict.keys())[0]\n    sample_speaker = interlocutor_dict[first_speaker_id]\n    print(f\"\\nSample speaker (ID: {first_speaker_id}):\")\n    print(f\"  Keys: {list(sample_speaker.keys())}\")\n    if 'personality' in sample_speaker:\n        print(f\"  Personality keys: {list(sample_speaker['personality'].keys())[:10]}\")\n\n# Load dialogue data\nprint(\"\\n\" + \"=\"*70)\nprint(\"Loading dialogue data...\")\nprint(\"=\"*70)\n\nnum_dialogues = 1000  # Set to 14000 for full dataset\n\nprint(f\"Downloading {num_dialogues} dialogues from GitHub...\")\nprint(\"(This may take 30-40 minutes for full dataset)\")\n\ndialogue_data = []\nfailed_downloads = 0\n\nfor i in tqdm(range(1, num_dialogues + 1), desc=\"Downloading dialogues\"):\n    dialogue_id = f\"{i:05d}\"\n    dialogue_url = f\"{BASE_URL}/dialogues/{dialogue_id}.json\"\n    \n    try:\n        response = requests.get(dialogue_url)\n        if response.status_code == 200:\n            dialogue = response.json()\n            dialogue_data.append(dialogue)\n        else:\n            failed_downloads += 1\n    except Exception as e:\n        failed_downloads += 1\n        if i <= 10:  # Only show first 10 errors\n            print(f\"Failed to download {dialogue_id}: {e}\")\n\nif failed_downloads > 0:\n    print(f\"\\nDownload failed: {failed_downloads} dialogues\")\n\nprint(f\"Loaded {len(dialogue_data)} dialogues\")\n\n# Display sample\nif dialogue_data:\n    sample = dialogue_data[0]\n    print(f\"\\nSample dialogue:\")\n    print(f\"  Keys: {list(sample.keys())}\")\n    print(f\"  Dialogue ID: {sample.get('dialogue_id', 'N/A')}\")\n    print(f\"  Interlocutors: {sample.get('interlocutors', [])}\")\n    print(f\"  Utterances: {len(sample.get('utterances', []))}\")\n\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "source": "def create_monologue_dataset(dialogue_data, interlocutor_dict):\n    \"\"\"\n    Extract monologues for each speaker from dialogue data\n\n    Args:\n        dialogue_data: List of dialogue dictionaries\n        interlocutor_dict: Dictionary of speaker ID -> speaker information\n\n    Returns:\n        monologues: List[Dict]\n            - 'speaker_id': str\n            - 'text': str (concatenated utterances)\n            - 'personality': Dict[str, float] (Big Five scores)\n    \"\"\"\n    monologues = []\n\n    # Process each dialogue\n    for dialogue in tqdm(dialogue_data, desc=\"Processing dialogues\"):\n        speaker_utterances = {}\n\n        # Collect utterances by speaker\n        for utterance in dialogue.get('utterances', []):\n            speaker_id = utterance.get('interlocutor_id')\n            text = utterance.get('text', '')\n\n            if not speaker_id or not text:\n                continue\n\n            if speaker_id not in speaker_utterances:\n                speaker_utterances[speaker_id] = []\n            speaker_utterances[speaker_id].append(text)\n\n        # Create monologue for each speaker\n        for speaker_id, utterances in speaker_utterances.items():\n            # Skip if speaker info not available\n            if speaker_id not in interlocutor_dict:\n                continue\n\n            speaker_info = interlocutor_dict[speaker_id]\n            personality_data = speaker_info.get('personality', {})\n\n            # Extract Big Five scores (1-7 scale)\n            big_five_scores = {\n                'Openness': personality_data.get('big_five_openness', personality_data.get('openness', 4.0)),\n                'Conscientiousness': personality_data.get('big_five_conscientiousness', personality_data.get('conscientiousness', 4.0)),\n                'Extraversion': personality_data.get('big_five_extraversion', personality_data.get('extraversion', 4.0)),\n                'Agreeableness': personality_data.get('big_five_agreeableness', personality_data.get('agreeableness', 4.0)),\n                'Neuroticism': personality_data.get('big_five_neuroticism', personality_data.get('neuroticism', 4.0)),\n            }\n\n            # Check if scores are properly obtained (not all default values)\n            if all(score == 4.0 for score in big_five_scores.values()):\n                # Try alternative key names\n                for key, value in personality_data.items():\n                    if 'openness' in key.lower():\n                        big_five_scores['Openness'] = value\n                    elif 'conscientiousness' in key.lower():\n                        big_five_scores['Conscientiousness'] = value\n                    elif 'extraversion' in key.lower():\n                        big_five_scores['Extraversion'] = value\n                    elif 'agreeableness' in key.lower():\n                        big_five_scores['Agreeableness'] = value\n                    elif 'neuroticism' in key.lower():\n                        big_five_scores['Neuroticism'] = value\n\n            # Add monologue sample\n            monologues.append({\n                'speaker_id': speaker_id,\n                'text': ' '.join(utterances),\n                'personality': big_five_scores\n            })\n\n    return monologues\n\n# Create monologue dataset\nmonologue_data = create_monologue_dataset(dialogue_data, interlocutor_dict)\n\nprint(f\"\\nMonologue samples: {len(monologue_data)}\")\nif monologue_data:\n    print(f\"Sample: Speaker {monologue_data[0]['speaker_id']}, Text length: {len(monologue_data[0]['text'])} chars\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def split_by_speaker(monologue_data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n    \"\"\"\n    Split data ensuring no speaker overlap between train/val/test sets\n    Following the paper's approach (Train/Val/Test = 8:1:1)\n    \"\"\"\n    # Group samples by speaker\n    speaker_groups = {}\n    for sample in monologue_data:\n        speaker_id = sample['speaker_id']\n        if speaker_id not in speaker_groups:\n            speaker_groups[speaker_id] = []\n        speaker_groups[speaker_id].append(sample)\n\n    # Shuffle speakers\n    speakers = list(speaker_groups.keys())\n    np.random.seed(seed)\n    np.random.shuffle(speakers)\n\n    # Split speakers into train/val/test\n    n_speakers = len(speakers)\n    n_train = int(n_speakers * train_ratio)\n    n_val = int(n_speakers * val_ratio)\n\n    train_speakers = speakers[:n_train]\n    val_speakers = speakers[n_train:n_train+n_val]\n    test_speakers = speakers[n_train+n_val:]\n\n    # Extract samples for each split\n    train_data = [s for spk in train_speakers for s in speaker_groups[spk]]\n    val_data = [s for spk in val_speakers for s in speaker_groups[spk]]\n    test_data = [s for spk in test_speakers for s in speaker_groups[spk]]\n\n    print(f\"\\nData split:\")\n    print(f\"  Train: {len(train_speakers)} speakers, {len(train_data)} samples\")\n    print(f\"  Val:   {len(val_speakers)} speakers, {len(val_data)} samples\")\n    print(f\"  Test:  {len(test_speakers)} speakers, {len(test_data)} samples\")\n\n    return train_data, val_data, test_data\n\n# Split data\ntrain_data, val_data, test_data = split_by_speaker(monologue_data)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Data Preprocessing\n\n- Encode text with LUKE tokenizer\n- Normalize Big Five scores (1-7 → 0-1)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nclass PersonalityDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for personality prediction\n    Tokenizes text and normalizes Big Five scores\n    \"\"\"\n    def __init__(self, data, tokenizer, max_length=256):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        \n        # Tokenize text\n        encoding = self.tokenizer(\n            sample['text'],\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        # Normalize Big Five scores (1-7 → 0-1)\n        personality_scores = torch.tensor([\n            (sample['personality'][trait] - 1) / 6\n            for trait in BIG_FIVE_TRAITS\n        ], dtype=torch.float32)\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'labels': personality_scores\n        }\n\n# Create datasets and dataloaders\nBATCH_SIZE = 4  # Small batch size for memory efficiency\n\ntrain_dataset = PersonalityDataset(train_data, tokenizer)\nval_dataset = PersonalityDataset(val_data, tokenizer)\ntest_dataset = PersonalityDataset(test_data, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\nprint(f\"\\nDataset ready: {len(train_loader)} train batches, {len(val_loader)} val batches, {len(test_loader)} test batches\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "9K06k2jZ2eB3",
    "outputId": "c6963d1e-a16b-4257-a0a6-b4818bb81a6f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "## 4. Model Construction\n\nLUKE + 5 regression heads (one for each Big Five trait)\n\n**Note**: Clear GPU memory before model construction."
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9vOr3Le32eB3"
   },
   "source": "# Clear GPU memory before model initialization\nimport gc\ntorch.cuda.empty_cache()\ngc.collect()\n\nclass LukePersonalityModel(nn.Module):\n    \"\"\"\n    LUKE-based personality prediction model\n    Architecture: LUKE encoder + 5 regression heads (one per trait)\n    \"\"\"\n    def __init__(self, model_name=MODEL_NAME, num_traits=5):\n        super().__init__()\n        # Load pre-trained LUKE model\n        self.luke = LukeModel.from_pretrained(model_name)\n        \n        # Enable gradient checkpointing for memory efficiency\n        self.luke.gradient_checkpointing_enable()\n        \n        self.hidden_size = self.luke.config.hidden_size\n        \n        # Create regression head for each Big Five trait\n        self.regression_heads = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(self.hidden_size, 256),\n                nn.ReLU(),\n                nn.Dropout(0.1),\n                nn.Linear(256, 1)\n            )\n            for _ in range(num_traits)\n        ])\n    \n    def forward(self, input_ids, attention_mask):\n        # Encode with LUKE\n        outputs = self.luke(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        # Extract [CLS] token representation\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        \n        # Predict each trait\n        predictions = []\n        for head in self.regression_heads:\n            pred = head(pooled_output)\n            predictions.append(pred)\n        \n        # Concatenate predictions and apply sigmoid (0-1 range)\n        predictions = torch.cat(predictions, dim=1)\n        predictions = torch.sigmoid(predictions)\n        \n        return predictions\n\n# Initialize model\nmodel = LukePersonalityModel().to(device)\n\nprint(f\"Model initialized: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M parameters\")\nif torch.cuda.is_available():\n    print(f\"GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f}GB allocated\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31mAYDid2eB3"
   },
   "source": "## 5. Training Configuration"
  },
  {
   "cell_type": "code",
   "source": "# Training hyperparameters\nLEARNING_RATE = 1e-5\nNUM_EPOCHS = 20\nWARMUP_STEPS = 150\nEARLY_STOPPING_PATIENCE = 5\n\n# Initialize optimizer\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n\n# Learning rate scheduler with warmup\ntotal_steps = len(train_loader) * NUM_EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=WARMUP_STEPS,\n    num_training_steps=total_steps\n)\n\n# Loss function (Mean Absolute Error)\ncriterion = nn.L1Loss()\n\nprint(f\"Training config: LR={LEARNING_RATE}, Epochs={NUM_EPOCHS}, Steps={total_steps}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EO2lf4K72eB3",
    "outputId": "ab90abbb-cbdd-40da-b23a-5e442aea60a2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "def evaluate_model(model, dataloader, device):\n    \"\"\"\n    Evaluate model on given dataloader\n    Returns predictions, labels, and average loss\n    \"\"\"\n    model.eval()\n    all_predictions = []\n    all_labels = []\n    total_loss = 0\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            # Forward pass\n            predictions = model(input_ids, attention_mask)\n            loss = criterion(predictions, labels)\n\n            # Collect results\n            total_loss += loss.item()\n            all_predictions.append(predictions.cpu().numpy())\n            all_labels.append(labels.cpu().numpy())\n\n    # Stack all batches\n    all_predictions = np.vstack(all_predictions)\n    all_labels = np.vstack(all_labels)\n    avg_loss = total_loss / len(dataloader)\n\n    return all_predictions, all_labels, avg_loss"
  },
  {
   "cell_type": "code",
   "source": "# Training loop with gradient accumulation and mixed precision\nhistory = {\n    'train_loss': [],\n    'val_loss': [],\n    'val_mae': []\n}\n\nbest_val_loss = float('inf')\npatience_counter = 0\n\n# Gradient accumulation for larger effective batch size (4 * 8 = 32)\nACCUMULATION_STEPS = 8\n\n# Mixed precision training for memory efficiency\nfrom torch.cuda.amp import autocast, GradScaler\nscaler = GradScaler()\n\n# Create checkpoint directory\nPath(CHECKPOINT_DIR).mkdir(exist_ok=True)\n\nprint(f\"Training started: {NUM_EPOCHS} epochs, effective batch size {BATCH_SIZE * ACCUMULATION_STEPS}\")\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n    print(\"-\" * 70)\n    \n    # Training phase\n    model.train()\n    train_loss = 0\n    optimizer.zero_grad()\n    \n    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        # Forward pass with mixed precision\n        with autocast():\n            predictions = model(input_ids, attention_mask)\n            loss = criterion(predictions, labels)\n            loss = loss / ACCUMULATION_STEPS  # Scale for gradient accumulation\n        \n        # Backward pass\n        scaler.scale(loss).backward()\n        \n        # Update weights every ACCUMULATION_STEPS\n        if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            optimizer.zero_grad()\n        \n        train_loss += loss.item() * ACCUMULATION_STEPS\n        \n        # Periodic memory cleanup\n        if (batch_idx + 1) % 10 == 0:\n            torch.cuda.empty_cache()\n    \n    # Handle remaining gradients\n    if (len(train_loader) % ACCUMULATION_STEPS) != 0:\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n    \n    avg_train_loss = train_loss / len(train_loader)\n    history['train_loss'].append(avg_train_loss)\n    \n    # Clear memory before validation\n    torch.cuda.empty_cache()\n    \n    # Validation phase\n    val_predictions, val_labels, val_loss = evaluate_model(model, val_loader, device)\n    val_mae = mean_absolute_error(val_labels, val_predictions)\n    \n    history['val_loss'].append(val_loss)\n    history['val_mae'].append(val_mae)\n    \n    # Print results\n    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val MAE: {val_mae:.4f}\")\n    \n    # Early stopping and checkpoint\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience_counter = 0\n        # Save best model\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': val_loss,\n        }, f\"{CHECKPOINT_DIR}/best_model.pt\")\n        print(\"Best model saved\")\n    else:\n        patience_counter += 1\n        print(f\"Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n        if patience_counter >= EARLY_STOPPING_PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\nprint(\"\\nTraining complete\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-q5Bk_v2eB3"
   },
   "source": "## 6. Training Loop\n\nStart training."
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(predictions, labels, trait_names=BIG_FIVE_TRAITS):\n    \"\"\"\n    Compute comprehensive metrics for personality prediction\n    Includes both regression metrics and classification metrics (binarized by median)\n    \"\"\"\n    results = {}\n    medians = np.median(labels, axis=0)\n\n    print(\"\\n\" + \"=\"*70)\n    print(\"Personality Recognition Results\")\n    print(\"=\"*70)\n\n    # Regression metrics\n    print(\"\\nRegression Metrics:\")\n    print(\"-\"*70)\n    print(f\"{'Trait':<18} {'MAE':>8} {'RMSE':>8} {'Pearson':>10} {'Spearman':>10}\")\n    print(\"-\"*70)\n\n    regression_metrics = []\n    for i, trait in enumerate(trait_names):\n        pred = predictions[:, i]\n        true = labels[:, i]\n\n        # Compute regression metrics\n        mae = mean_absolute_error(true, pred)\n        rmse = np.sqrt(mean_squared_error(true, pred))\n        pearson_corr, _ = pearsonr(true, pred)\n        spearman_corr, _ = spearmanr(true, pred)\n\n        regression_metrics.append({\n            'trait': trait,\n            'mae': mae,\n            'rmse': rmse,\n            'pearson': pearson_corr,\n            'spearman': spearman_corr\n        })\n\n        print(f\"{trait:<18} {mae:>8.4f} {rmse:>8.4f} {pearson_corr:>10.3f} {spearman_corr:>10.3f}\")\n\n    # Compute averages\n    avg_mae = np.mean([m['mae'] for m in regression_metrics])\n    avg_rmse = np.mean([m['rmse'] for m in regression_metrics])\n    avg_pearson = np.mean([m['pearson'] for m in regression_metrics])\n    avg_spearman = np.mean([m['spearman'] for m in regression_metrics])\n\n    print(\"-\"*70)\n    print(f\"{'Average':<18} {avg_mae:>8.4f} {avg_rmse:>8.4f} {avg_pearson:>10.3f} {avg_spearman:>10.3f}\")\n\n    # Classification metrics (binarize by median)\n    print(\"\\n\\nClassification Metrics (High/Low by median):\")\n    print(\"-\"*70)\n    print(f\"{'Trait':<18} {'Acc':>6} {'Bal Acc':>8} {'Prec':>6} {'Rec':>6} {'F1':>6}\")\n    print(\"-\"*70)\n\n    classification_metrics = []\n    for i, trait in enumerate(trait_names):\n        pred = predictions[:, i]\n        true = labels[:, i]\n        median = medians[i]\n\n        # Binarize predictions and labels\n        pred_binary = (pred > median).astype(int)\n        true_binary = (true > median).astype(int)\n\n        # Compute classification metrics\n        acc = accuracy_score(true_binary, pred_binary)\n        bal_acc = balanced_accuracy_score(true_binary, pred_binary)\n        prec, rec, f1, _ = precision_recall_fscore_support(\n            true_binary, pred_binary, average='binary', zero_division=0\n        )\n\n        classification_metrics.append({\n            'trait': trait,\n            'accuracy': acc,\n            'balanced_accuracy': bal_acc,\n            'precision': prec,\n            'recall': rec,\n            'f1': f1\n        })\n\n        print(f\"{trait:<18} {acc:>6.1%} {bal_acc:>8.1%} {prec:>6.2f} {rec:>6.2f} {f1:>6.2f}\")\n\n    # Compute classification averages\n    avg_acc = np.mean([m['accuracy'] for m in classification_metrics])\n    avg_bal_acc = np.mean([m['balanced_accuracy'] for m in classification_metrics])\n    avg_prec = np.mean([m['precision'] for m in classification_metrics])\n    avg_rec = np.mean([m['recall'] for m in classification_metrics])\n    avg_f1 = np.mean([m['f1'] for m in classification_metrics])\n\n    print(\"-\"*70)\n    print(f\"{'Average':<18} {avg_acc:>6.1%} {avg_bal_acc:>8.1%} {avg_prec:>6.2f} {avg_rec:>6.2f} {avg_f1:>6.2f}\")\n\n    # Compare with paper\n    print(\"\\n\\nComparison with Paper (Fu et al. 2024):\")\n    print(\"-\"*70)\n    print(f\"Paper Balanced Accuracy (Monologue): 60.4%\")\n    print(f\"Our Balanced Accuracy:                {avg_bal_acc:.1%}\")\n    print(\"=\"*70)\n\n    # Package results\n    results['regression'] = regression_metrics\n    results['classification'] = classification_metrics\n    results['averages'] = {\n        'mae': avg_mae,\n        'rmse': avg_rmse,\n        'pearson': avg_pearson,\n        'spearman': avg_spearman,\n        'accuracy': avg_acc,\n        'balanced_accuracy': avg_bal_acc\n    }\n\n    return results\n\n# Compute and display metrics\nresults = compute_metrics(test_predictions, test_labels)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Test Set Evaluation",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load best model and evaluate on test set\ncheckpoint = torch.load(f\"{CHECKPOINT_DIR}/best_model.pt\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\nprint(f\"Best model loaded: Epoch {checkpoint['epoch']+1}, Val Loss {checkpoint['val_loss']:.4f}\")\n\n# Evaluate on test set\ntest_predictions, test_labels, test_loss = evaluate_model(model, test_loader, device)\n\nprint(f\"\\nTest Loss: {test_loss:.4f}\")\nprint(f\"Test MAE: {mean_absolute_error(test_labels, test_predictions):.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Visualization",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def plot_predictions(predictions, labels, trait_names=BIG_FIVE_TRAITS):\n    \"\"\"Plot prediction vs true value scatter plots for each trait\"\"\"\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    axes = axes.flatten()\n\n    for i, trait in enumerate(trait_names):\n        ax = axes[i]\n\n        pred = predictions[:, i]\n        true = labels[:, i]\n\n        # Scatter plot\n        ax.scatter(true, pred, alpha=0.5, s=20)\n\n        # Ideal line (perfect prediction)\n        min_val = min(true.min(), pred.min())\n        max_val = max(true.max(), pred.max())\n        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Ideal')\n\n        # Compute metrics for title\n        pearson, _ = pearsonr(true, pred)\n        mae = mean_absolute_error(true, pred)\n\n        ax.set_xlabel('True Score')\n        ax.set_ylabel('Predicted Score')\n        ax.set_title(f'{trait}\\nPearson: {pearson:.3f}, MAE: {mae:.3f}')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n    # Hide unused subplot\n    axes[-1].axis('off')\n\n    plt.tight_layout()\n    plt.savefig(f'{CHECKPOINT_DIR}/predictions_scatter.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Plot predictions\nplot_predictions(test_predictions, test_labels)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyUhZgMa2eB3"
   },
   "outputs": [],
   "source": "## 5. Training Loop\n\nStart training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zj5mQ0Ij2eB3"
   },
   "outputs": [],
   "source": "from sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrices(predictions, labels, trait_names=BIG_FIVE_TRAITS):\n    \"\"\"Plot confusion matrices for binarized predictions (High/Low by median)\"\"\"\n    medians = np.median(labels, axis=0)\n\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    axes = axes.flatten()\n\n    for i, trait in enumerate(trait_names):\n        ax = axes[i]\n\n        pred = predictions[:, i]\n        true = labels[:, i]\n        median = medians[i]\n\n        # Binarize by median\n        pred_binary = (pred > median).astype(int)\n        true_binary = (true > median).astype(int)\n\n        # Compute confusion matrix\n        cm = confusion_matrix(true_binary, pred_binary)\n\n        # Plot heatmap\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n                    xticklabels=['Low', 'High'],\n                    yticklabels=['Low', 'High'])\n        ax.set_xlabel('Predicted')\n        ax.set_ylabel('True')\n        ax.set_title(f'{trait} - Confusion Matrix')\n\n    # Hide unused subplot\n    axes[-1].axis('off')\n\n    plt.tight_layout()\n    plt.savefig(f'{CHECKPOINT_DIR}/confusion_matrices.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Plot confusion matrices\nplot_confusion_matrices(test_predictions, test_labels)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}