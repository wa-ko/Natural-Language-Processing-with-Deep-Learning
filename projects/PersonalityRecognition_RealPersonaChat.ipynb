{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type># Big Five Personality Recognition using LUKE on RealPersonaChat\n\n**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦:**\n- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: RealPersonaChat (14,000å¯¾è©±ã€233è©±è€…)\n- ã‚¿ã‚¹ã‚¯: Big Fiveæ€§æ ¼ç‰¹æ€§ã®å›å¸°äºˆæ¸¬\n- ãƒ¢ãƒ‡ãƒ«: LUKE (studio-ousia/luke-japanese-base)\n- è¨­å®š: ãƒ¢ãƒãƒ­ãƒ¼ã‚°ï¼ˆè©±è€…ã®ç™ºè©±ã®ã¿ï¼‰\n\n**è©•ä¾¡æŒ‡æ¨™:**\n- å›å¸°: MAE, RMSE, Pearson, Spearmanç›¸é–¢\n- åˆ†é¡: Accuracy, Balanced Accuracy, Precision, Recall, F1\n\n**ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–:**\n- ãƒãƒƒãƒã‚µã‚¤ã‚º: 8\n- Max Length: 256 tokens\n- å‹¾é…è“„ç©: 4ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆå®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚º32ç›¸å½“ï¼‰\n\n**æ¨å¥¨ç’°å¢ƒ:**\n- Google Colab with T4 GPU (15GB VRAM)\n- ãƒ©ãƒ³ã‚¿ã‚¤ãƒ å†èµ·å‹•å¾Œã«å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æ¨å¥¨"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install -q transformers datasets torch scikit-learn pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW  # transformersã§ã¯ãªãtorch.optimã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\nfrom transformers import (\n    AutoTokenizer,  # LukeTokenizerã®ä»£ã‚ã‚Šã«AutoTokenizerã‚’ä½¿ç”¨\n    LukeModel,\n    get_linear_schedule_with_warmup\n)\nimport json\nimport requests\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    mean_absolute_error,\n    mean_squared_error,\n    accuracy_score,\n    balanced_accuracy_score,\n    precision_recall_fscore_support\n)\nfrom scipy.stats import pearsonr, spearmanr\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive ãƒã‚¦ãƒ³ãƒˆï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ç”¨ï¼‰\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å…ˆ\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/personality_recognition'\n",
    "!mkdir -p {CHECKPOINT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n\nRealPersonaChatãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’GitHubã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿ã¾ã™ã€‚\n- è©±è€…ãƒ‡ãƒ¼ã‚¿: `interlocutors.json` (233å)\n- å¯¾è©±ãƒ‡ãƒ¼ã‚¿: `dialogues/*.json` (æœ€å¤§14,000ä»¶)\n\n**æ³¨æ„**: å…¨14,000ä»¶ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚\n- ãƒ†ã‚¹ãƒˆç”¨: `num_dialogues = 1000` (ç´„2-3åˆ†)\n- æœ¬ç•ªç”¨: `num_dialogues = 14000` (ç´„30-40åˆ†)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ï¼ˆGitHubã‹ã‚‰ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰\nimport json\nimport requests\nfrom pathlib import Path\n\nprint(\"GitHubã‹ã‚‰RealPersonaChatãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­...\")\n\n# ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\ndata_dir = Path(\"/content/real_persona_chat_data\")\ndata_dir.mkdir(exist_ok=True)\n\n# GitHubã®rawãƒ•ã‚¡ã‚¤ãƒ«URL\nBASE_URL = \"https://raw.githubusercontent.com/nu-dialogue/real-persona-chat/main/real_persona_chat\"\n\n# 1. Interlocutors (è©±è€…ãƒ‡ãƒ¼ã‚¿) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\nprint(\"\\nè©±è€…ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\ninterlocutors_url = f\"{BASE_URL}/interlocutors.json\"\nresponse = requests.get(interlocutors_url)\ninterlocutors_raw = response.json()\n\n# ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç¢ºèª\nprint(f\"\\nğŸ“Š è©±è€…ãƒ‡ãƒ¼ã‚¿ã®å‹: {type(interlocutors_raw)}\")\nif isinstance(interlocutors_raw, dict):\n    print(f\"ğŸ“Š è¾æ›¸ã®ã‚­ãƒ¼: {list(interlocutors_raw.keys())[:5]}\")  # æœ€åˆã®5ã¤ã®ã‚­ãƒ¼ã‚’è¡¨ç¤º\n    # è¾æ›¸æ§‹é€ ã®å ´åˆã€å€¤ãŒãƒªã‚¹ãƒˆã‹ç¢ºèª\n    first_key = list(interlocutors_raw.keys())[0]\n    print(f\"ğŸ“Š æœ€åˆã®è¦ç´ ã®å‹: {type(interlocutors_raw[first_key])}\")\n    \n    # è¾æ›¸æ§‹é€ ã‚’å¤‰æ›\n    if isinstance(interlocutors_raw[first_key], dict):\n        # {speaker_id: {data}} å½¢å¼ã®å ´åˆ\n        interlocutor_dict = interlocutors_raw\n        print(f\"\\nâœ… è©±è€…ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(interlocutor_dict)} speakers\")\n    else:\n        print(f\"âš ï¸  äºˆæœŸã—ãªã„ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã§ã™\")\n        interlocutor_dict = {}\nelif isinstance(interlocutors_raw, list):\n    print(f\"ğŸ“Š ãƒªã‚¹ãƒˆã®é•·ã•: {len(interlocutors_raw)}\")\n    # ãƒªã‚¹ãƒˆæ§‹é€ ã®å ´åˆã€è¾æ›¸ã«å¤‰æ›\n    interlocutor_dict = {\n        item['interlocutor_id']: item\n        for item in interlocutors_raw\n    }\n    print(f\"\\nâœ… è©±è€…ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(interlocutor_dict)} speakers\")\nelse:\n    print(f\"âš ï¸  äºˆæœŸã—ãªã„ãƒ‡ãƒ¼ã‚¿å‹: {type(interlocutors_raw)}\")\n    interlocutor_dict = {}\n\n# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º\nif interlocutor_dict:\n    first_speaker_id = list(interlocutor_dict.keys())[0]\n    sample_speaker = interlocutor_dict[first_speaker_id]\n    print(f\"\\nğŸ“Š è©±è€…ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆè©±è€…ID: {first_speaker_id}ï¼‰:\")\n    print(f\"  åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ¼: {list(sample_speaker.keys())}\")\n    if 'personality' in sample_speaker:\n        print(f\"  Personality keys: {list(sample_speaker['personality'].keys())[:10]}\")  # æœ€åˆã®10å€‹\n\n# 2. Dialogue ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\nprint(\"\\n\" + \"=\"*70)\nprint(\"å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\nprint(\"ï¼ˆæ³¨ï¼šå…¨14,000ä»¶ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰\")\nprint(\"ãƒ†ã‚¹ãƒˆç”¨ã«æœ€åˆã®1,000ä»¶ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...\")\nprint(\"=\"*70 + \"\\n\")\n\ndialogue_data = []\nnum_dialogues = 1000  # å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼ˆæœ€å¤§14000ï¼‰\nfailed_downloads = 0\n\nfor i in tqdm(range(1, num_dialogues + 1), desc=\"Downloading dialogues\"):\n    dialogue_id = f\"{i:05d}\"  # 00001å½¢å¼\n    dialogue_url = f\"{BASE_URL}/dialogues/{dialogue_id}.json\"\n    \n    try:\n        response = requests.get(dialogue_url)\n        if response.status_code == 200:\n            dialogue = response.json()\n            dialogue_data.append(dialogue)\n        else:\n            failed_downloads += 1\n    except Exception as e:\n        failed_downloads += 1\n        if i <= 10:  # æœ€åˆã®10ä»¶ã®ã‚¨ãƒ©ãƒ¼ã ã‘è¡¨ç¤º\n            print(f\"Failed to download {dialogue_id}: {e}\")\n\nprint(f\"\\nâœ… å¯¾è©±ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(dialogue_data)} dialogues\")\nif failed_downloads > 0:\n    print(f\"âš ï¸  ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {failed_downloads} dialogues\")\n\n# ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç¢ºèª\nprint(\"\\nğŸ“Š å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®1ä»¶ï¼‰:\")\nif dialogue_data:\n    sample = dialogue_data[0]\n    print(f\"  åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ¼: {list(sample.keys())}\")\n    print(f\"  Dialogue ID: {sample.get('dialogue_id', 'N/A')}\")\n    print(f\"  Interlocutors: {sample.get('interlocutors', [])}\")\n    print(f\"  Utterances: {len(sample.get('utterances', []))} utterances\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ãƒ¢ãƒãƒ­ãƒ¼ã‚°ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ\n# å„è©±è€…ã®ç™ºè©±ã®ã¿ã‚’æŠ½å‡ºã—ã¦ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆ\n\ndef create_monologue_dataset(dialogue_data, interlocutor_dict):\n    \"\"\"\n    å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„è©±è€…ã®ãƒ¢ãƒãƒ­ãƒ¼ã‚°ã‚’æŠ½å‡º\n    \n    Args:\n        dialogue_data: List of dialogue dictionaries\n        interlocutor_dict: è©±è€…ID -> è©±è€…æƒ…å ±ã®è¾æ›¸\n    \n    Returns:\n        monologues: List[Dict]\n            - 'speaker_id': str\n            - 'text': str (å…¨ç™ºè©±ã‚’é€£çµ)\n            - 'personality': Dict[str, float] (Big Five scores)\n    \"\"\"\n    monologues = []\n    \n    print(\"\\nãƒ¢ãƒãƒ­ãƒ¼ã‚°æŠ½å‡ºä¸­...\")\n    \n    for dialogue in tqdm(dialogue_data, desc=\"Processing dialogues\"):\n        # å„è©±è€…ã®ç™ºè©±ã‚’åé›†\n        speaker_utterances = {}\n        \n        for utterance in dialogue.get('utterances', []):\n            speaker_id = utterance.get('interlocutor_id')\n            text = utterance.get('text', '')\n            \n            if not speaker_id or not text:\n                continue\n                \n            if speaker_id not in speaker_utterances:\n                speaker_utterances[speaker_id] = []\n            speaker_utterances[speaker_id].append(text)\n        \n        # å„è©±è€…ã®ãƒ¢ãƒãƒ­ãƒ¼ã‚°ã‚’ä½œæˆ\n        for speaker_id, utterances in speaker_utterances.items():\n            # è©±è€…æƒ…å ±ã‚’å–å¾—\n            if speaker_id not in interlocutor_dict:\n                continue  # è©±è€…æƒ…å ±ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—\n            \n            speaker_info = interlocutor_dict[speaker_id]\n            \n            # Big Five ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º\n            personality_data = speaker_info.get('personality', {})\n            \n            # Big Fiveç‰¹æ€§ã®ã‚¹ã‚³ã‚¢ã‚’å–å¾—ï¼ˆ1-7ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰\n            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç¢ºèªã—ã¦é©åˆ‡ãªã‚­ãƒ¼åã‚’ä½¿ç”¨\n            big_five_scores = {\n                'Openness': personality_data.get('big_five_openness', personality_data.get('openness', 4.0)),\n                'Conscientiousness': personality_data.get('big_five_conscientiousness', personality_data.get('conscientiousness', 4.0)),\n                'Extraversion': personality_data.get('big_five_extraversion', personality_data.get('extraversion', 4.0)),\n                'Agreeableness': personality_data.get('big_five_agreeableness', personality_data.get('agreeableness', 4.0)),\n                'Neuroticism': personality_data.get('big_five_neuroticism', personality_data.get('neuroticism', 4.0)),\n            }\n            \n            # ã‚¹ã‚³ã‚¢ãŒå–å¾—ã§ãã¦ã„ã‚‹ã‹ç¢ºèª\n            if all(score == 4.0 for score in big_five_scores.values()):\n                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®ã¾ã¾ã®å ´åˆã¯ä»–ã®ã‚­ãƒ¼åã‚’è©¦ã™\n                for key, value in personality_data.items():\n                    if 'openness' in key.lower():\n                        big_five_scores['Openness'] = value\n                    elif 'conscientiousness' in key.lower():\n                        big_five_scores['Conscientiousness'] = value\n                    elif 'extraversion' in key.lower():\n                        big_five_scores['Extraversion'] = value\n                    elif 'agreeableness' in key.lower():\n                        big_five_scores['Agreeableness'] = value\n                    elif 'neuroticism' in key.lower():\n                        big_five_scores['Neuroticism'] = value\n            \n            monologues.append({\n                'speaker_id': speaker_id,\n                'text': ' '.join(utterances),\n                'personality': big_five_scores\n            })\n    \n    return monologues\n\n# ãƒ¢ãƒãƒ­ãƒ¼ã‚°ãƒ‡ãƒ¼ã‚¿ä½œæˆ\nmonologue_data = create_monologue_dataset(dialogue_data, interlocutor_dict)\n\nprint(f\"\\nâœ… ãƒ¢ãƒãƒ­ãƒ¼ã‚°ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(monologue_data)}\")\nif monologue_data:\n    print(f\"\\nğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ä¾‹:\")\n    print(f\"  è©±è€…ID: {monologue_data[0]['speaker_id']}\")\n    print(f\"  ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(monologue_data[0]['text'])} æ–‡å­—\")\n    print(f\"  Big Five: {monologue_data[0]['personality']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆè©±è€…ãƒ™ãƒ¼ã‚¹ï¼‰\n# è«–æ–‡æº–æ‹ : Train/Val/Test = 8:1:1, è©±è€…ãŒé‡è¤‡ã—ãªã„ã‚ˆã†ã«åˆ†å‰²\n\ndef split_by_speaker(monologue_data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n    \"\"\"\n    è©±è€…ãŒé‡è¤‡ã—ãªã„ã‚ˆã†ã«ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n    \"\"\"\n    # è©±è€…IDã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n    speaker_groups = {}\n    for sample in monologue_data:\n        speaker_id = sample['speaker_id']\n        if speaker_id not in speaker_groups:\n            speaker_groups[speaker_id] = []\n        speaker_groups[speaker_id].append(sample)\n    \n    # è©±è€…ãƒªã‚¹ãƒˆ\n    speakers = list(speaker_groups.keys())\n    np.random.seed(seed)\n    np.random.shuffle(speakers)\n    \n    # åˆ†å‰²\n    n_speakers = len(speakers)\n    n_train = int(n_speakers * train_ratio)\n    n_val = int(n_speakers * val_ratio)\n    \n    train_speakers = speakers[:n_train]\n    val_speakers = speakers[n_train:n_train+n_val]\n    test_speakers = speakers[n_train+n_val:]\n    \n    # ã‚µãƒ³ãƒ—ãƒ«æŠ½å‡º\n    train_data = [s for spk in train_speakers for s in speaker_groups[spk]]\n    val_data = [s for spk in val_speakers for s in speaker_groups[spk]]\n    test_data = [s for spk in test_speakers for s in speaker_groups[spk]]\n    \n    print(f\"\\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²çµæœ:\")\n    print(f\"  Train: {len(train_speakers)} speakers, {len(train_data)} samples\")\n    print(f\"  Val:   {len(val_speakers)} speakers, {len(val_data)} samples\")\n    print(f\"  Test:  {len(test_speakers)} speakers, {len(test_data)} samples\")\n    \n    return train_data, val_data, test_data\n\n# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Ÿè¡Œ\ntrain_data, val_data, test_data = split_by_speaker(monologue_data)\n\nprint(f\"\\nâœ… ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†\n",
    "\n",
    "- LUKEãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "- Big Fiveã‚¹ã‚³ã‚¢ã‚’æ­£è¦åŒ–ï¼ˆ1-7 â†’ 0-1ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LUKEãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ç”¨ã«AutoTokenizerã‚’ä½¿ç”¨ï¼‰\nMODEL_NAME = \"studio-ousia/luke-japanese-base\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Big Fiveç‰¹æ€§ã®é †åº\nBIG_FIVE_TRAITS = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\n\nprint(f\"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†: {MODEL_NAME}\")\nprint(f\"   ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å‹: {type(tokenizer).__name__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# PyTorch Dataset ã‚¯ãƒ©ã‚¹\nclass PersonalityDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=256):  # 512 â†’ 256ã«å¤‰æ›´\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        \n        # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³\n        encoding = self.tokenizer(\n            sample['text'],\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        # Big Five ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–ï¼ˆ1-7 â†’ 0-1ï¼‰\n        personality_scores = torch.tensor([\n            (sample['personality'][trait] - 1) / 6  # 1-7 ã‚’ 0-1 ã«æ­£è¦åŒ–\n            for trait in BIG_FIVE_TRAITS\n        ], dtype=torch.float32)\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'labels': personality_scores\n        }\n\n# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ\nBATCH_SIZE = 8  # 32 â†’ 8ã«å¤‰æ›´ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰\n\ntrain_dataset = PersonalityDataset(train_data, tokenizer)\nval_dataset = PersonalityDataset(val_data, tokenizer)\ntest_dataset = PersonalityDataset(test_data, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\nprint(f\"\\nâœ… Datasetæº–å‚™å®Œäº†\")\nprint(f\"  Max Length: 256 tokens (ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–)\")\nprint(f\"  Batch Size: {BATCH_SIZE} (ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–)\")\nprint(f\"  Train batches: {len(train_loader)}\")\nprint(f\"  Val batches: {len(val_loader)}\")\nprint(f\"  Test batches: {len(test_loader)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰\n",
    "\n",
    "LUKE + 5ã¤ã®å›å¸°ãƒ˜ãƒƒãƒ‰ï¼ˆå„Big Fiveç‰¹æ€§ã«1ã¤ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LUKE Personality Recognition Model\n",
    "class LukePersonalityModel(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME, num_traits=5):\n",
    "        super().__init__()\n",
    "        self.luke = LukeModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.luke.config.hidden_size\n",
    "        \n",
    "        # å„Big Fiveç‰¹æ€§ã«å¯¾ã™ã‚‹å›å¸°ãƒ˜ãƒƒãƒ‰\n",
    "        self.regression_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(256, 1)\n",
    "            )\n",
    "            for _ in range(num_traits)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # LUKE encoding\n",
    "        outputs = self.luke(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # [CLS] token ã® hidden state\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "        \n",
    "        # å„ç‰¹æ€§ã®äºˆæ¸¬\n",
    "        predictions = []\n",
    "        for head in self.regression_heads:\n",
    "            pred = head(pooled_output)  # [batch_size, 1]\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # [batch_size, 5]\n",
    "        predictions = torch.cat(predictions, dim=1)\n",
    "        \n",
    "        # 0-1 ã®ç¯„å›²ã«åˆ¶é™ï¼ˆSigmoidï¼‰\n",
    "        predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–\n",
    "model = LukePersonalityModel().to(device)\n",
    "print(f\"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ç¿’è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nLEARNING_RATE = 1e-5\nNUM_EPOCHS = 20\nWARMUP_STEPS = 150\nEARLY_STOPPING_PATIENCE = 3\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n\n# Learning Rate Scheduler\ntotal_steps = len(train_loader) * NUM_EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=WARMUP_STEPS,\n    num_training_steps=total_steps\n)\n\n# æå¤±é–¢æ•°ï¼ˆMAEï¼‰\ncriterion = nn.L1Loss()  # Mean Absolute Error\n\nprint(f\"\\nâœ… å­¦ç¿’è¨­å®šå®Œäº†\")\nprint(f\"  Learning Rate: {LEARNING_RATE}\")\nprint(f\"  Total Steps: {total_steps}\")\nprint(f\"  Warmup Steps: {WARMUP_STEPS}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©•ä¾¡é–¢æ•°\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return all_predictions, all_labels, avg_loss\n",
    "\n",
    "print(\"è©•ä¾¡é–¢æ•°æº–å‚™å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å­¦ç¿’ãƒ«ãƒ¼ãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å­¦ç¿’å±¥æ­´\nhistory = {\n    'train_loss': [],\n    'val_loss': [],\n    'val_mae': []\n}\n\nbest_val_loss = float('inf')\npatience_counter = 0\n\n# å‹¾é…è“„ç©ï¼ˆå®Ÿè³ªçš„ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§ããã—ã¤ã¤ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ï¼‰\nACCUMULATION_STEPS = 4  # 8 * 4 = å®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚º32ç›¸å½“\n\nprint(\"=\"*70)\nprint(\"å­¦ç¿’é–‹å§‹\")\nprint(f\"å®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH_SIZE * ACCUMULATION_STEPS} (å‹¾é…è“„ç©)\")\nprint(\"=\"*70)\n\n# å­¦ç¿’ãƒ«ãƒ¼ãƒ—\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\n{'='*70}\")\n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n    print(f\"{'='*70}\")\n    \n    # Training\n    model.train()\n    train_loss = 0\n    \n    optimizer.zero_grad()  # æœ€åˆã«ã‚¼ãƒ­åŒ–\n    \n    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        predictions = model(input_ids, attention_mask)\n        loss = criterion(predictions, labels)\n        \n        # å‹¾é…è“„ç©ã®ãŸã‚ã«lossã‚’ã‚¹ã‚±ãƒ¼ãƒ«\n        loss = loss / ACCUMULATION_STEPS\n        loss.backward()\n        \n        # å‹¾é…è“„ç©ï¼šACCUMULATION_STEPSå›ã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°\n        if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n        \n        train_loss += loss.item() * ACCUMULATION_STEPS  # å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™\n        \n        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ10ãƒãƒƒãƒã”ã¨ï¼‰\n        if (batch_idx + 1) % 10 == 0:\n            torch.cuda.empty_cache()\n    \n    # æ®‹ã‚Šã®å‹¾é…ã‚’å‡¦ç†\n    if (len(train_loader) % ACCUMULATION_STEPS) != 0:\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    avg_train_loss = train_loss / len(train_loader)\n    history['train_loss'].append(avg_train_loss)\n    \n    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n    torch.cuda.empty_cache()\n    \n    # Validation\n    val_predictions, val_labels, val_loss = evaluate_model(model, val_loader, device)\n    val_mae = mean_absolute_error(val_labels, val_predictions)\n    \n    history['val_loss'].append(val_loss)\n    history['val_mae'].append(val_mae)\n    \n    print(f\"\\nğŸ“Š Results:\")\n    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n    print(f\"  Val Loss: {val_loss:.4f}\")\n    print(f\"  Val MAE: {val_mae:.4f}\")\n    \n    # GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated(0) / 1e9\n        reserved = torch.cuda.memory_reserved(0) / 1e9\n        print(f\"  GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n    \n    # Early Stopping & Checkpoint\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience_counter = 0\n        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': val_loss,\n        }, f\"{CHECKPOINT_DIR}/best_model.pt\")\n        print(\"  âœ… Best model saved!\")\n    else:\n        patience_counter += 1\n        print(f\"  âš ï¸  Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n        if patience_counter >= EARLY_STOPPING_PATIENCE:\n            print(f\"\\nğŸ›‘ Early stopping at epoch {epoch+1}\")\n            break\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… å­¦ç¿’å®Œäº†\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\nprint(\"=\"*70)\nprint(\"ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\")\nprint(\"=\"*70)\n\ncheckpoint = torch.load(f\"{CHECKPOINT_DIR}/best_model.pt\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\nprint(f\"âœ… Best model loaded! (Epoch {checkpoint['epoch']+1}, Val Loss: {checkpoint['val_loss']:.4f})\")\n\n# ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡\nprint(\"\\n\" + \"=\"*70)\nprint(\"ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡ä¸­...\")\nprint(\"=\"*70)\n\ntest_predictions, test_labels, test_loss = evaluate_model(model, test_loader, device)\nprint(f\"\\nğŸ“Š Test Loss: {test_loss:.4f}\")\nprint(f\"ğŸ“Š Test MAE: {mean_absolute_error(test_labels, test_predictions):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# è©³ç´°è©•ä¾¡é–¢æ•°\ndef compute_metrics(predictions, labels, trait_names=BIG_FIVE_TRAITS):\n    \"\"\"\n    å›å¸°æŒ‡æ¨™ã¨åˆ†é¡æŒ‡æ¨™ã®ä¸¡æ–¹ã‚’è¨ˆç®—\n    \"\"\"\n    results = {}\n    \n    # ä¸­å¤®å€¤ã‚’è¨ˆç®—ï¼ˆåˆ†é¡ç”¨ï¼‰\n    medians = np.median(labels, axis=0)\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"Personality Recognition Results\")\n    print(\"=\"*70)\n    \n    # å›å¸°æŒ‡æ¨™\n    print(\"\\nRegression Metrics:\")\n    print(\"-\"*70)\n    print(f\"{'Trait':<18} {'MAE':>8} {'RMSE':>8} {'Pearson':>10} {'Spearman':>10}\")\n    print(\"-\"*70)\n    \n    regression_metrics = []\n    for i, trait in enumerate(trait_names):\n        pred = predictions[:, i]\n        true = labels[:, i]\n        \n        mae = mean_absolute_error(true, pred)\n        rmse = np.sqrt(mean_squared_error(true, pred))\n        \n        # ç›¸é–¢ä¿‚æ•°ï¼ˆpå€¤ã‚‚è¨ˆç®—ï¼‰\n        pearson_corr, pearson_p = pearsonr(true, pred)\n        spearman_corr, spearman_p = spearmanr(true, pred)\n        \n        regression_metrics.append({\n            'trait': trait,\n            'mae': mae,\n            'rmse': rmse,\n            'pearson': pearson_corr,\n            'spearman': spearman_corr\n        })\n        \n        print(f\"{trait:<18} {mae:>8.4f} {rmse:>8.4f} {pearson_corr:>10.3f} {spearman_corr:>10.3f}\")\n    \n    # å¹³å‡\n    avg_mae = np.mean([m['mae'] for m in regression_metrics])\n    avg_rmse = np.mean([m['rmse'] for m in regression_metrics])\n    avg_pearson = np.mean([m['pearson'] for m in regression_metrics])\n    avg_spearman = np.mean([m['spearman'] for m in regression_metrics])\n    \n    print(\"-\"*70)\n    print(f\"{'Average':<18} {avg_mae:>8.4f} {avg_rmse:>8.4f} {avg_pearson:>10.3f} {avg_spearman:>10.3f}\")\n    \n    # åˆ†é¡æŒ‡æ¨™ï¼ˆä¸­å¤®å€¤ã§2å€¤åŒ–ï¼‰\n    print(\"\\n\\nClassification Metrics (High/Low by median):\")\n    print(\"-\"*70)\n    print(f\"{'Trait':<18} {'Acc':>6} {'Bal Acc':>8} {'Prec':>6} {'Rec':>6} {'F1':>6}\")\n    print(\"-\"*70)\n    \n    classification_metrics = []\n    for i, trait in enumerate(trait_names):\n        pred = predictions[:, i]\n        true = labels[:, i]\n        median = medians[i]\n        \n        # 2å€¤åŒ–\n        pred_binary = (pred > median).astype(int)\n        true_binary = (true > median).astype(int)\n        \n        acc = accuracy_score(true_binary, pred_binary)\n        bal_acc = balanced_accuracy_score(true_binary, pred_binary)\n        prec, rec, f1, _ = precision_recall_fscore_support(\n            true_binary, pred_binary, average='binary', zero_division=0\n        )\n        \n        classification_metrics.append({\n            'trait': trait,\n            'accuracy': acc,\n            'balanced_accuracy': bal_acc,\n            'precision': prec,\n            'recall': rec,\n            'f1': f1\n        })\n        \n        print(f\"{trait:<18} {acc:>6.1%} {bal_acc:>8.1%} {prec:>6.2f} {rec:>6.2f} {f1:>6.2f}\")\n    \n    # å¹³å‡\n    avg_acc = np.mean([m['accuracy'] for m in classification_metrics])\n    avg_bal_acc = np.mean([m['balanced_accuracy'] for m in classification_metrics])\n    avg_prec = np.mean([m['precision'] for m in classification_metrics])\n    avg_rec = np.mean([m['recall'] for m in classification_metrics])\n    avg_f1 = np.mean([m['f1'] for m in classification_metrics])\n    \n    print(\"-\"*70)\n    print(f\"{'Average':<18} {avg_acc:>6.1%} {avg_bal_acc:>8.1%} {avg_prec:>6.2f} {avg_rec:>6.2f} {avg_f1:>6.2f}\")\n    \n    # è«–æ–‡ã¨ã®æ¯”è¼ƒ\n    print(\"\\n\\nComparison with Paper (Fu et al. 2024):\")\n    print(\"-\"*70)\n    print(f\"Paper Balanced Accuracy (Monologue): 60.4%\")\n    print(f\"Our Balanced Accuracy:                {avg_bal_acc:.1%}\")\n    print(\"=\"*70)\n    \n    results['regression'] = regression_metrics\n    results['classification'] = classification_metrics\n    results['averages'] = {\n        'mae': avg_mae,\n        'rmse': avg_rmse,\n        'pearson': avg_pearson,\n        'spearman': avg_spearman,\n        'accuracy': avg_acc,\n        'balanced_accuracy': avg_bal_acc\n    }\n    \n    return results\n\n# è©•ä¾¡å®Ÿè¡Œ\nresults = compute_metrics(test_predictions, test_labels)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–\ndef plot_training_history(history):\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Loss\n    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n    axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss (MAE)')\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    # MAE\n    axes[1].plot(history['val_mae'], label='Val MAE', marker='o', color='green')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('MAE')\n    axes[1].set_title('Validation MAE')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(f'{CHECKPOINT_DIR}/training_history.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ\nplot_training_history(history)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æ•£å¸ƒå›³: äºˆæ¸¬ vs çœŸå€¤\ndef plot_predictions(predictions, labels, trait_names=BIG_FIVE_TRAITS):\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    axes = axes.flatten()\n    \n    for i, trait in enumerate(trait_names):\n        ax = axes[i]\n        \n        pred = predictions[:, i]\n        true = labels[:, i]\n        \n        # æ•£å¸ƒå›³\n        ax.scatter(true, pred, alpha=0.5, s=20)\n        \n        # ç†æƒ³ç·š\n        min_val = min(true.min(), pred.min())\n        max_val = max(true.max(), pred.max())\n        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Ideal')\n        \n        # ç›¸é–¢ä¿‚æ•°\n        pearson, _ = pearsonr(true, pred)\n        mae = mean_absolute_error(true, pred)\n        \n        ax.set_xlabel('True Score')\n        ax.set_ylabel('Predicted Score')\n        ax.set_title(f'{trait}\\nPearson: {pearson:.3f}, MAE: {mae:.3f}')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n    \n    # æœ€å¾Œã®axesã‚’éè¡¨ç¤º\n    axes[-1].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(f'{CHECKPOINT_DIR}/predictions_scatter.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# æ•£å¸ƒå›³ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\nplot_predictions(test_predictions, test_labels)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æ··åŒè¡Œåˆ—ï¼ˆåˆ†é¡ã¨ã—ã¦è©•ä¾¡ï¼‰\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrices(predictions, labels, trait_names=BIG_FIVE_TRAITS):\n    medians = np.median(labels, axis=0)\n    \n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    axes = axes.flatten()\n    \n    for i, trait in enumerate(trait_names):\n        ax = axes[i]\n        \n        pred = predictions[:, i]\n        true = labels[:, i]\n        median = medians[i]\n        \n        # 2å€¤åŒ–\n        pred_binary = (pred > median).astype(int)\n        true_binary = (true > median).astype(int)\n        \n        # æ··åŒè¡Œåˆ—\n        cm = confusion_matrix(true_binary, pred_binary)\n        \n        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n                    xticklabels=['Low', 'High'],\n                    yticklabels=['Low', 'High'])\n        ax.set_xlabel('Predicted')\n        ax.set_ylabel('True')\n        ax.set_title(f'{trait} - Confusion Matrix')\n    \n    # æœ€å¾Œã®axesã‚’éè¡¨ç¤º\n    axes[-1].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(f'{CHECKPOINT_DIR}/confusion_matrices.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# æ··åŒè¡Œåˆ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\nplot_confusion_matrices(test_predictions, test_labels)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ã¾ã¨ã‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ä»¥ä¸‹ã‚’å®Ÿè£…ã—ã¾ã—ãŸï¼š\n",
    "\n",
    "1. âœ… RealPersonaChatãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\n",
    "2. âœ… LUKEå›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰\n",
    "3. âœ… ãƒ¢ãƒãƒ­ãƒ¼ã‚°è¨­å®šã§ã®å­¦ç¿’\n",
    "4. âœ… å›å¸°æŒ‡æ¨™ï¼ˆMAE, RMSE, ç›¸é–¢ï¼‰ã¨åˆ†é¡æŒ‡æ¨™ï¼ˆAcc, F1ï¼‰ã®è©•ä¾¡\n",
    "5. âœ… è«–æ–‡ï¼ˆFu et al. 2024ï¼‰ã¨ã®æ¯”è¼ƒ\n",
    "\n",
    "**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:**\n",
    "- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å®Ÿéš›ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦å®Ÿè¡Œ\n",
    "- å­¦ç¿’ã‚’å®Ÿè¡Œã—ã€çµæœã‚’åˆ†æ\n",
    "- ç™ºè¡¨è³‡æ–™ã®ä½œæˆ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}