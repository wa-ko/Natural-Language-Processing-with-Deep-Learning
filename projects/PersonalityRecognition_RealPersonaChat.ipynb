{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "# Big Five Personality Recognition using LUKE on RealPersonaChat\n\n**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦:**\n- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: RealPersonaChat (14,000å¯¾è©±ã€233è©±è€…) **â† å…¨ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨**\n- ã‚¿ã‚¹ã‚¯: Big Fiveæ€§æ ¼ç‰¹æ€§ã®å›å¸°äºˆæ¸¬\n- ãƒ¢ãƒ‡ãƒ«: LUKE (studio-ousia/luke-japanese-base)\n- è¨­å®š: ãƒ¢ãƒãƒ­ãƒ¼ã‚°ï¼ˆè©±è€…ã®ç™ºè©±ã®ã¿ï¼‰\n\n**è©•ä¾¡æŒ‡æ¨™:**\n- å›å¸°: MAE, RMSE, Pearson, Spearmanç›¸é–¢\n- åˆ†é¡: Accuracy, Balanced Accuracy, Precision, Recall, F1\n\n**ãƒ¡ãƒ¢ãƒªæœ€é©åŒ– (T4 GPUå¯¾å¿œ):**\n- ãƒãƒƒãƒã‚µã‚¤ã‚º: 4\n- Max Length: 256 tokens\n- å‹¾é…è“„ç©: 8ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆå®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚º32ç›¸å½“ï¼‰\n- å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: æœ‰åŠ¹åŒ–\n- Mixed Precision (FP16): æœ‰åŠ¹åŒ–\n\n**å®Ÿè¡Œæ–¹æ³•:**\n\n**åˆå›å®Ÿè¡Œï¼ˆãƒ•ãƒ«å®Ÿè¡Œï¼‰:**\n1. ã‚»ãƒ«1-3: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n2. **ã‚»ãƒ«4-5ã‚’ã‚¹ã‚­ãƒƒãƒ—**ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã¯æ‰‹å‹•å®Ÿè¡Œå°‚ç”¨ï¼‰\n3. ã‚»ãƒ«6-18: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã€å‰å‡¦ç†ã€ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰\n   - **æ³¨æ„**: åˆå›ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¯30-40åˆ†ï¼ˆ14,000å¯¾è©±ï¼‰\n4. ã‚»ãƒ«18: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•ä¿å­˜\n5. ã‚»ãƒ«21: å­¦ç¿’ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œï¼ˆ20-30åˆ†ï¼‰\n\n**ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤ãŒå¿…è¦ãªå ´åˆ:**\n- **ã‚»ãƒ«4ã‚’å€‹åˆ¥ã«æ‰‹å‹•å®Ÿè¡Œ**ï¼ˆRun Allã§ã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ï¼‰\n- å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆä¾‹ï¼š1,000å¯¾è©±ç‰ˆï¼‰ã‚’å‰Šé™¤ã§ãã¾ã™\n- ä½¿ã„æ–¹: ç•ªå·å…¥åŠ›ã§é¸æŠå‰Šé™¤ã€ã¾ãŸã¯ `all` ã§å…¨å‰Šé™¤\n\n**å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã§è½ã¡ãŸå ´åˆã®é«˜é€Ÿå†é–‹:**\n1. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•\n2. ã‚»ãƒ«1-3ã®ã¿å®Ÿè¡Œï¼ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰\n3. ã‚»ãƒ«20ã‚’å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¾©å…ƒã€30ç§’ï¼‰\n4. ã‚»ãƒ«22ã‚’å®Ÿè¡Œï¼ˆå­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼‰\n\nâ†’ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†ï¼ˆ30-40åˆ†ï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—å¯èƒ½ï¼\n\n**æ¨å¥¨ç’°å¢ƒ:**\n- Google Colab with T4 GPU (15GB VRAM)\n- ãƒ©ãƒ³ã‚¿ã‚¤ãƒ å†èµ·å‹•å¾Œã«å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æ¨å¥¨"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpQBruNV2eB1"
   },
   "source": [
    "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1LJenW-O2eB2"
   },
   "outputs": [],
   "source": [
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install -q transformers datasets torch scikit-learn pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIUAvxtp2eB2",
    "outputId": "f6b94ff4-c49a-4937-9e2c-ef3221b5d1a6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW  # transformersã§ã¯ãªãtorch.optimã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\nfrom transformers import (\n    AutoTokenizer,  # LukeTokenizerã®ä»£ã‚ã‚Šã«AutoTokenizerã‚’ä½¿ç”¨\n    LukeModel,\n    get_linear_schedule_with_warmup\n)\nimport json\nimport requests\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    mean_absolute_error,\n    mean_squared_error,\n    accuracy_score,\n    balanced_accuracy_score,\n    precision_recall_fscore_support\n)\nfrom scipy.stats import pearsonr, spearmanr\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\n# ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã®è¨­å®š\nMODEL_NAME = \"studio-ousia/luke-japanese-base\"\nBIG_FIVE_TRAITS = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\nCHECKPOINT_DIR = \"/content/checkpoints\"\n\nprint(f\"\\nâœ… è¨­å®šå®Œäº†\")\nprint(f\"  Model: {MODEL_NAME}\")\nprint(f\"  Traits: {BIG_FIVE_TRAITS}\")\nprint(f\"  Checkpoint Dir: {CHECKPOINT_DIR}\")"
  },
  {
   "cell_type": "code",
   "source": "# âš ï¸ æ‰‹å‹•å®Ÿè¡Œå°‚ç”¨ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†\n# ã“ã®ã‚»ãƒ«ã¯ã€ŒRun Allã€ã§ã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ï¼ˆinput()ã§å…¥åŠ›å¾…ã¡ã®ãŸã‚ï¼‰\n\nfrom pathlib import Path\nimport os\nfrom datetime import datetime\n\ndef manage_cache():\n    \"\"\"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ç®¡ç†ã¨å‰Šé™¤\"\"\"\n    \n    # Google Driveä¸Šã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n    cache_dir = Path(\"/content/drive/MyDrive/real_persona_chat_cache\")\n    \n    if not cache_dir.exists():\n        print(\"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“\")\n        return\n    \n    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—\n    cache_files = list(cache_dir.glob(\"*.pkl\"))\n    \n    if not cache_files:\n        print(\"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“\")\n        return\n    \n    print(\"=\"*70)\n    print(\"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:\")\n    print(\"=\"*70)\n    print(f\"{'No.':<4} {'ãƒ•ã‚¡ã‚¤ãƒ«å':<35} {'ã‚µã‚¤ã‚º':<12} {'æ›´æ–°æ—¥æ™‚'}\")\n    print(\"-\"*70)\n    \n    for idx, file_path in enumerate(cache_files, 1):\n        size_mb = file_path.stat().st_size / 1e6\n        mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)\n        print(f\"{idx:<4} {file_path.name:<35} {size_mb:>8.1f} MB  {mod_time:%Y-%m-%d %H:%M}\")\n    \n    print(\"=\"*70)\n    \n    # å‰Šé™¤ç¢ºèªï¼ˆRun Allã§ã¯ã“ã“ã§æ­¢ã¾ã‚‹ï¼‰\n    try:\n        response = input(\"\\nå‰Šé™¤ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ç•ªå·ã‚’å…¥åŠ›ï¼ˆè¤‡æ•°å¯ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰ã€å…¨å‰Šé™¤ã¯ 'all'ã€ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã¯ Enter: \")\n    except (EOFError, KeyboardInterrupt):\n        print(\"\\nâš ï¸ æ‰‹å‹•å®Ÿè¡ŒãŒå¿…è¦ã§ã™ï¼ˆRun Allã§ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ï¼‰\")\n        return\n    \n    if response.strip() == \"\":\n        print(\"âœ… ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ\")\n        return\n    \n    if response.strip().lower() == \"all\":\n        for file_path in cache_files:\n            file_path.unlink()\n            print(f\"ğŸ—‘ï¸  å‰Šé™¤: {file_path.name}\")\n        print(f\"\\nâœ… {len(cache_files)}ä»¶ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ\")\n    else:\n        try:\n            indices = [int(x.strip()) for x in response.split(',')]\n            deleted_count = 0\n            for idx in indices:\n                if 1 <= idx <= len(cache_files):\n                    file_path = cache_files[idx - 1]\n                    file_path.unlink()\n                    print(f\"ğŸ—‘ï¸  å‰Šé™¤: {file_path.name}\")\n                    deleted_count += 1\n                else:\n                    print(f\"âš ï¸  ç„¡åŠ¹ãªç•ªå·: {idx}\")\n            print(f\"\\nâœ… {deleted_count}ä»¶ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ\")\n        except ValueError:\n            print(\"âš ï¸  ç„¡åŠ¹ãªå…¥åŠ›ã§ã™\")\n\n# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚’å®Ÿè¡Œ\nprint(\"ğŸ“‹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ãƒ„ãƒ¼ãƒ«\")\nprint(\"ğŸ’¡ ã“ã®ã‚»ãƒ«ã¯æ‰‹å‹•ã§å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\")\nprint(\"-\"*70)\nmanage_cache()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ğŸ—‘ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ï¼ˆæ‰‹å‹•å®Ÿè¡Œå°‚ç”¨ï¼‰\n\n**é‡è¦**: ã“ã®ã‚»ãƒ«ã¯ã€ŒRun Allã€ã§ã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ã€‚æ‰‹å‹•ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n\nã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ãŸã„å ´åˆã®ã¿ã€ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "JwKt8CD32eB2",
    "outputId": "b4f1a327-0a8a-4f72-bcc7-008c2a47aff9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "## 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n\nRealPersonaChatãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’GitHubã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿ã¾ã™ã€‚\n- è©±è€…ãƒ‡ãƒ¼ã‚¿: `interlocutors.json` (233å)\n- å¯¾è©±ãƒ‡ãƒ¼ã‚¿: `dialogues/*.json` (14,000ä»¶)\n\n**ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º:**\n- **æœ¬ç•ªç”¨ï¼ˆæ¨å¥¨ï¼‰**: `num_dialogues = 14000` å…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆåˆå›30-40åˆ†ã€2å›ç›®ä»¥é™5ç§’ï¼‰\n- ãƒ†ã‚¹ãƒˆç”¨: `num_dialogues = 1000` éƒ¨åˆ†ãƒ‡ãƒ¼ã‚¿ï¼ˆåˆå›2-3åˆ†ã€2å›ç›®ä»¥é™5ç§’ï¼‰\n\n**Google Driveã‚­ãƒ£ãƒƒã‚·ãƒ¥:**\n- åˆå›: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ + ä¿å­˜ï¼ˆ30-40åˆ†ï¼‰\n- 2å›ç›®ä»¥é™: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆ5ç§’ï¼‰\n\n**æ³¨æ„**: ã‚ˆã‚Šè‰¯ã„çµæœã‚’å¾—ã‚‹ãŸã‚ã€ãƒ•ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆ14,000å¯¾è©±ï¼‰ã®ä½¿ç”¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8BZTbgSL2eB2"
   },
   "source": "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ï¼ˆGitHubã‹ã‚‰ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ + Google Drive ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰\nimport json\nimport requests\nfrom pathlib import Path\nimport pickle\n\nprint(\"RealPersonaChatãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­...\")\n\n# Google Driveä¸Šã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\nCACHE_DIR = Path(\"/content/drive/MyDrive/real_persona_chat_cache\")\nCACHE_DIR.mkdir(exist_ok=True)\n\nINTERLOCUTORS_CACHE = CACHE_DIR / \"interlocutors.pkl\"\nDIALOGUES_CACHE = CACHE_DIR / \"dialogues_1000.pkl\"  # ãƒ†ã‚¹ãƒˆç”¨ï¼ˆ1,000å¯¾è©±ï¼‰\n\n# GitHubã®rawãƒ•ã‚¡ã‚¤ãƒ«URL\nBASE_URL = \"https://raw.githubusercontent.com/nu-dialogue/real-persona-chat/main/real_persona_chat\"\n\n# 1. Interlocutors (è©±è€…ãƒ‡ãƒ¼ã‚¿) ã‚’èª­ã¿è¾¼ã¿\nprint(\"\\n\" + \"=\"*70)\nprint(\"è©±è€…ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\nprint(\"=\"*70)\n\nif INTERLOCUTORS_CACHE.exists():\n    print(\"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆé«˜é€Ÿï¼‰\")\n    with open(INTERLOCUTORS_CACHE, 'rb') as f:\n        interlocutor_dict = pickle.load(f)\n    print(f\"âœ… è©±è€…ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(interlocutor_dict)} speakers\")\nelse:\n    print(\"ğŸ“¥ GitHubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n    interlocutors_url = f\"{BASE_URL}/interlocutors.json\"\n    response = requests.get(interlocutors_url)\n    interlocutors_raw = response.json()\n    \n    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’å¤‰æ›\n    if isinstance(interlocutors_raw, dict):\n        interlocutor_dict = interlocutors_raw\n    elif isinstance(interlocutors_raw, list):\n        interlocutor_dict = {\n            item['interlocutor_id']: item\n            for item in interlocutors_raw\n        }\n    else:\n        print(f\"âš ï¸  äºˆæœŸã—ãªã„ãƒ‡ãƒ¼ã‚¿å‹: {type(interlocutors_raw)}\")\n        interlocutor_dict = {}\n    \n    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜\n    with open(INTERLOCUTORS_CACHE, 'wb') as f:\n        pickle.dump(interlocutor_dict, f)\n    print(f\"âœ… è©±è€…ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {INTERLOCUTORS_CACHE}\")\n    print(f\"âœ… è©±è€…ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(interlocutor_dict)} speakers\")\n\n# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º\nif interlocutor_dict:\n    first_speaker_id = list(interlocutor_dict.keys())[0]\n    sample_speaker = interlocutor_dict[first_speaker_id]\n    print(f\"\\nğŸ“Š è©±è€…ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆè©±è€…ID: {first_speaker_id}ï¼‰:\")\n    print(f\"  åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ¼: {list(sample_speaker.keys())}\")\n    if 'personality' in sample_speaker:\n        print(f\"  Personality keys: {list(sample_speaker['personality'].keys())[:10]}\")\n\n# 2. Dialogue ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\nprint(\"\\n\" + \"=\"*70)\nprint(\"å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\nprint(\"=\"*70)\n\nnum_dialogues = 1000  # ãƒ†ã‚¹ãƒˆç”¨ï¼ˆå‹•ä½œç¢ºèªï¼‰\n\nif DIALOGUES_CACHE.exists():\n    print(f\"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆé«˜é€Ÿï¼‰ - {num_dialogues}ä»¶\")\n    with open(DIALOGUES_CACHE, 'rb') as f:\n        dialogue_data = pickle.load(f)\n    print(f\"âœ… å¯¾è©±ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(dialogue_data)} dialogues\")\nelse:\n    print(f\"ğŸ“¥ GitHubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­... ({num_dialogues}ä»¶)\")\n    print(\"ï¼ˆæ³¨ï¼šåˆå›ã®ã¿æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚æ¬¡å›ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰é«˜é€Ÿèª­ã¿è¾¼ã¿ï¼‰\")\n    \n    dialogue_data = []\n    failed_downloads = 0\n    \n    for i in tqdm(range(1, num_dialogues + 1), desc=\"Downloading dialogues\"):\n        dialogue_id = f\"{i:05d}\"\n        dialogue_url = f\"{BASE_URL}/dialogues/{dialogue_id}.json\"\n        \n        try:\n            response = requests.get(dialogue_url)\n            if response.status_code == 200:\n                dialogue = response.json()\n                dialogue_data.append(dialogue)\n            else:\n                failed_downloads += 1\n        except Exception as e:\n            failed_downloads += 1\n            if i <= 10:\n                print(f\"Failed to download {dialogue_id}: {e}\")\n    \n    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜\n    with open(DIALOGUES_CACHE, 'wb') as f:\n        pickle.dump(dialogue_data, f)\n    print(f\"\\nâœ… å¯¾è©±ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {DIALOGUES_CACHE}\")\n    print(f\"âœ… å¯¾è©±ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(dialogue_data)} dialogues\")\n    if failed_downloads > 0:\n        print(f\"âš ï¸  ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {failed_downloads} dialogues\")\n\n# ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç¢ºèª\nprint(\"\\nğŸ“Š å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®1ä»¶ï¼‰:\")\nif dialogue_data:\n    sample = dialogue_data[0]\n    print(f\"  åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ¼: {list(sample.keys())}\")\n    print(f\"  Dialogue ID: {sample.get('dialogue_id', 'N/A')}\")\n    print(f\"  Interlocutors: {sample.get('interlocutors', [])}\")\n    print(f\"  Utterances: {len(sample.get('utterances', []))} utterances\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ’¡ æ¬¡å›ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰é«˜é€Ÿèª­ã¿è¾¼ã¿ã—ã¾ã™ï¼ˆæ•°ç§’ï¼‰\")\nprint(\"=\"*70)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "P71557N92eB2",
    "outputId": "09412397-e1a7-459c-a34d-082c7d5a2c3f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230,
     "referenced_widgets": [
      "19d48b23355a498d84433523db78564d",
      "a24c1eeff9914e55b645bfd02f8b1882",
      "24a69d6f930d4dba96abc3b4a633b9a7",
      "7a1ccabba91646a4824292d21501495d",
      "294248cc6392435a8f457e8b59007ec1",
      "9ce220cf57e5447d931e4f5f59d53c2f",
      "f107e032aed444dd94371182a042bab2",
      "274827f228ff4a3390e8e51f80be71d0",
      "6d3c1038e6bd4c2d82afb3926b254cd9",
      "1405869b76754b2a8591149402ba197a",
      "aedcbccd31d54613bef83a610ec883f0"
     ]
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "ãƒ¢ãƒãƒ­ãƒ¼ã‚°æŠ½å‡ºä¸­...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Processing dialogues:   0%|          | 0/972 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19d48b23355a498d84433523db78564d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "âœ… ãƒ¢ãƒãƒ­ãƒ¼ã‚°ã‚µãƒ³ãƒ—ãƒ«æ•°: 1944\n",
      "\n",
      "ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ä¾‹:\n",
      "  è©±è€…ID: AA\n",
      "  ãƒ†ã‚­ã‚¹ãƒˆé•·: 248 æ–‡å­—\n",
      "  Big Five: {'Openness': 4.25, 'Conscientiousness': 3.5, 'Extraversion': 2.9, 'Agreeableness': 5.083333333333333, 'Neuroticism': 4.416666666666667}\n"
     ]
    }
   ],
   "source": [
    "# ãƒ¢ãƒãƒ­ãƒ¼ã‚°ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ\n",
    "# å„è©±è€…ã®ç™ºè©±ã®ã¿ã‚’æŠ½å‡ºã—ã¦ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆ\n",
    "\n",
    "def create_monologue_dataset(dialogue_data, interlocutor_dict):\n",
    "    \"\"\"\n",
    "    å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„è©±è€…ã®ãƒ¢ãƒãƒ­ãƒ¼ã‚°ã‚’æŠ½å‡º\n",
    "\n",
    "    Args:\n",
    "        dialogue_data: List of dialogue dictionaries\n",
    "        interlocutor_dict: è©±è€…ID -> è©±è€…æƒ…å ±ã®è¾æ›¸\n",
    "\n",
    "    Returns:\n",
    "        monologues: List[Dict]\n",
    "            - 'speaker_id': str\n",
    "            - 'text': str (å…¨ç™ºè©±ã‚’é€£çµ)\n",
    "            - 'personality': Dict[str, float] (Big Five scores)\n",
    "    \"\"\"\n",
    "    monologues = []\n",
    "\n",
    "    print(\"\\nãƒ¢ãƒãƒ­ãƒ¼ã‚°æŠ½å‡ºä¸­...\")\n",
    "\n",
    "    for dialogue in tqdm(dialogue_data, desc=\"Processing dialogues\"):\n",
    "        # å„è©±è€…ã®ç™ºè©±ã‚’åé›†\n",
    "        speaker_utterances = {}\n",
    "\n",
    "        for utterance in dialogue.get('utterances', []):\n",
    "            speaker_id = utterance.get('interlocutor_id')\n",
    "            text = utterance.get('text', '')\n",
    "\n",
    "            if not speaker_id or not text:\n",
    "                continue\n",
    "\n",
    "            if speaker_id not in speaker_utterances:\n",
    "                speaker_utterances[speaker_id] = []\n",
    "            speaker_utterances[speaker_id].append(text)\n",
    "\n",
    "        # å„è©±è€…ã®ãƒ¢ãƒãƒ­ãƒ¼ã‚°ã‚’ä½œæˆ\n",
    "        for speaker_id, utterances in speaker_utterances.items():\n",
    "            # è©±è€…æƒ…å ±ã‚’å–å¾—\n",
    "            if speaker_id not in interlocutor_dict:\n",
    "                continue  # è©±è€…æƒ…å ±ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "\n",
    "            speaker_info = interlocutor_dict[speaker_id]\n",
    "\n",
    "            # Big Five ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º\n",
    "            personality_data = speaker_info.get('personality', {})\n",
    "\n",
    "            # Big Fiveç‰¹æ€§ã®ã‚¹ã‚³ã‚¢ã‚’å–å¾—ï¼ˆ1-7ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰\n",
    "            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç¢ºèªã—ã¦é©åˆ‡ãªã‚­ãƒ¼åã‚’ä½¿ç”¨\n",
    "            big_five_scores = {\n",
    "                'Openness': personality_data.get('big_five_openness', personality_data.get('openness', 4.0)),\n",
    "                'Conscientiousness': personality_data.get('big_five_conscientiousness', personality_data.get('conscientiousness', 4.0)),\n",
    "                'Extraversion': personality_data.get('big_five_extraversion', personality_data.get('extraversion', 4.0)),\n",
    "                'Agreeableness': personality_data.get('big_five_agreeableness', personality_data.get('agreeableness', 4.0)),\n",
    "                'Neuroticism': personality_data.get('big_five_neuroticism', personality_data.get('neuroticism', 4.0)),\n",
    "            }\n",
    "\n",
    "            # ã‚¹ã‚³ã‚¢ãŒå–å¾—ã§ãã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "            if all(score == 4.0 for score in big_five_scores.values()):\n",
    "                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®ã¾ã¾ã®å ´åˆã¯ä»–ã®ã‚­ãƒ¼åã‚’è©¦ã™\n",
    "                for key, value in personality_data.items():\n",
    "                    if 'openness' in key.lower():\n",
    "                        big_five_scores['Openness'] = value\n",
    "                    elif 'conscientiousness' in key.lower():\n",
    "                        big_five_scores['Conscientiousness'] = value\n",
    "                    elif 'extraversion' in key.lower():\n",
    "                        big_five_scores['Extraversion'] = value\n",
    "                    elif 'agreeableness' in key.lower():\n",
    "                        big_five_scores['Agreeableness'] = value\n",
    "                    elif 'neuroticism' in key.lower():\n",
    "                        big_five_scores['Neuroticism'] = value\n",
    "\n",
    "            monologues.append({\n",
    "                'speaker_id': speaker_id,\n",
    "                'text': ' '.join(utterances),\n",
    "                'personality': big_five_scores\n",
    "            })\n",
    "\n",
    "    return monologues\n",
    "\n",
    "# ãƒ¢ãƒãƒ­ãƒ¼ã‚°ãƒ‡ãƒ¼ã‚¿ä½œæˆ\n",
    "monologue_data = create_monologue_dataset(dialogue_data, interlocutor_dict)\n",
    "\n",
    "print(f\"\\nâœ… ãƒ¢ãƒãƒ­ãƒ¼ã‚°ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(monologue_data)}\")\n",
    "if monologue_data:\n",
    "    print(f\"\\nğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ä¾‹:\")\n",
    "    print(f\"  è©±è€…ID: {monologue_data[0]['speaker_id']}\")\n",
    "    print(f\"  ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(monologue_data[0]['text'])} æ–‡å­—\")\n",
    "    print(f\"  Big Five: {monologue_data[0]['personality']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ka9sYMGW2eB2",
    "outputId": "847a76ce-9f3c-476f-bba5-f08768e71c5d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²çµæœ:\n",
      "  Train: 56 speakers, 1482 samples\n",
      "  Val:   7 speakers, 143 samples\n",
      "  Test:  7 speakers, 319 samples\n",
      "\n",
      "âœ… ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†\n"
     ]
    }
   ],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆè©±è€…ãƒ™ãƒ¼ã‚¹ï¼‰\n",
    "# è«–æ–‡æº–æ‹ : Train/Val/Test = 8:1:1, è©±è€…ãŒé‡è¤‡ã—ãªã„ã‚ˆã†ã«åˆ†å‰²\n",
    "\n",
    "def split_by_speaker(monologue_data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    è©±è€…ãŒé‡è¤‡ã—ãªã„ã‚ˆã†ã«ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "    \"\"\"\n",
    "    # è©±è€…IDã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n",
    "    speaker_groups = {}\n",
    "    for sample in monologue_data:\n",
    "        speaker_id = sample['speaker_id']\n",
    "        if speaker_id not in speaker_groups:\n",
    "            speaker_groups[speaker_id] = []\n",
    "        speaker_groups[speaker_id].append(sample)\n",
    "\n",
    "    # è©±è€…ãƒªã‚¹ãƒˆ\n",
    "    speakers = list(speaker_groups.keys())\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(speakers)\n",
    "\n",
    "    # åˆ†å‰²\n",
    "    n_speakers = len(speakers)\n",
    "    n_train = int(n_speakers * train_ratio)\n",
    "    n_val = int(n_speakers * val_ratio)\n",
    "\n",
    "    train_speakers = speakers[:n_train]\n",
    "    val_speakers = speakers[n_train:n_train+n_val]\n",
    "    test_speakers = speakers[n_train+n_val:]\n",
    "\n",
    "    # ã‚µãƒ³ãƒ—ãƒ«æŠ½å‡º\n",
    "    train_data = [s for spk in train_speakers for s in speaker_groups[spk]]\n",
    "    val_data = [s for spk in val_speakers for s in speaker_groups[spk]]\n",
    "    test_data = [s for spk in test_speakers for s in speaker_groups[spk]]\n",
    "\n",
    "    print(f\"\\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²çµæœ:\")\n",
    "    print(f\"  Train: {len(train_speakers)} speakers, {len(train_data)} samples\")\n",
    "    print(f\"  Val:   {len(val_speakers)} speakers, {len(val_data)} samples\")\n",
    "    print(f\"  Test:  {len(test_speakers)} speakers, {len(test_data)} samples\")\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Ÿè¡Œ\n",
    "train_data, val_data, test_data = split_by_speaker(monologue_data)\n",
    "\n",
    "print(f\"\\nâœ… ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GV566u7_2eB2"
   },
   "source": [
    "## 3. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†\n",
    "\n",
    "- LUKEãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "- Big Fiveã‚¹ã‚³ã‚¢ã‚’æ­£è¦åŒ–ï¼ˆ1-7 â†’ 0-1ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJX4RnZ-2eB2",
    "outputId": "a8674c97-c2fe-4ca5-f9b9-8a03ca63a2ab",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245,
     "referenced_widgets": [
      "75a35ded60e14c808d30e1aa34deaf21",
      "37d8c688def642fd82d42cc2d9a5596a",
      "3cc59d8c4a8f4e5196d7a50e7b2bc7a8",
      "2b1e1e0a40534d8f9e8da461b9cd553c",
      "5de5e2385c274dcd8cd5db3dad22ddbc",
      "cfd48a54d7e84e3097a4addc1b60a26b",
      "1197f1dbde094b7ca5ed306563df6a54",
      "7f66d87c60d74ad68c2b5735fca22a03",
      "d48297d248ce4ef49a970a48befa4160",
      "ce2206154e124bb9be4f1cf62d302b98",
      "a858b2b2667449bc9aa7424ec193e72c",
      "ac2fc283071b403197529c05b3abaf5b",
      "f7b585d985d443038124bd4d1304e0ed",
      "34b18acc56c74d3e889bd9acb90a231e",
      "8269f7e4cb2f4b3280f13576cc8a9067",
      "2d50ea127381455e8d7087d286ed3d20",
      "f66af1fbccb04908a56cd7368511984e",
      "f71b84d7ed6c4c2fbeee3589d6d7bc0b",
      "5e269279a2984893abef447bf7fb83aa",
      "4ddea5a3fc844e0b83a19066bd864497",
      "5d162464fa974a478f1905f85afa8fdb",
      "009698e6dd0449088018ec89e7e860ea",
      "e5c8dba9d5e24866aa24d306eb24c5c7",
      "4ea4e265c1614d03b5607ecd5bee5238",
      "ac7efa2870894d589ef40439be24cefb",
      "70621441faec49269faeb0bce389030c",
      "05c81806aeb04c22b37fadb32eb39f81",
      "6c841fa9eecc45ccb37be28ad1d7d9f7",
      "fefd716796ac4f7aa7d7811d4bd1b143",
      "0a8d9203c073495481ab2563165922b0",
      "6ead8437c0ed4a7e8b58d88441a30c06",
      "7c75e7cee224402eb9264a5d6d2fece8",
      "a51bd7ef950e45c0a2ec74f85a9acba3",
      "1b900fe53a904f40941269536ae34cd9",
      "9102faf79ad4469298373707c36f043e",
      "b2b62472255c4115bfc275d120643268",
      "d10c2d2f4d1746dfb4f6d5432f41aad3",
      "7e76ff6433c14cf2b5791e7368fe1374",
      "3467d3c29e92473794ab7c649ac9dc8c",
      "626973975b8e4ae5a7c58fdadc027fc7",
      "61f3cac7ddab4397be0fe7cd61a6b2a8",
      "56699d9d97c44325884844a005214fef",
      "23a06c0f189e4fa79b21f3f1df444494",
      "a163ee10499b46dd8e74ec8b99b78ca4",
      "2793fe8dc84d48ff933b06d8a3a420c5",
      "d064a00f082946d7a309ef9b78a8a539",
      "973ce485786749b08aea4c957558a605",
      "36dc5a931b4645b29d7630a010d025af",
      "451947b6e980488f8a251cd719e5e274",
      "2e260159c16f483da0362e699d130970",
      "ff7a609e77e34585bd9545b6e01454d4",
      "5293ef9264f1448db087d651a50ef922",
      "4d82e06da71447d5b343c5067ff623f5",
      "346659e76a69418e8eadb1bdf2468847",
      "5384f1c07a46425aa2848ebb2515e1df",
      "0d99fc2b221446939b7f1a7188e9753c",
      "8479d3087bf0402b9a59fec1021007ca",
      "c385f952ee9a4f57b3a9f35a093761e0",
      "5bcf7c04c7864726a17fd259f5b54e3a",
      "a2104007458b4f18ae8c5bef8cd4446f",
      "de9866967c574a068327cbce2215da65",
      "31615ffb4b574603970f840a090e9a38",
      "a129819275474f2fbc41d44922acff2f",
      "de9e6efb43594e9ba99bb5def2165ecc",
      "103ed724b09c4d09a62fb4b892c12692",
      "fa00c020c3f944f0bcb0f602d319be46"
     ]
    }
   },
   "outputs": [],
   "source": "# Tokenizer ãƒ­ãƒ¼ãƒ‰\nprint(\"Tokenizerãƒ­ãƒ¼ãƒ‰ä¸­...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nprint(f\"âœ… Tokenizerãƒ­ãƒ¼ãƒ‰å®Œäº†: {MODEL_NAME}\")\n\n# PyTorch Dataset ã‚¯ãƒ©ã‚¹\nclass PersonalityDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=256):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        \n        # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³\n        encoding = self.tokenizer(\n            sample['text'],\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        # Big Five ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–ï¼ˆ1-7 â†’ 0-1ï¼‰\n        personality_scores = torch.tensor([\n            (sample['personality'][trait] - 1) / 6  # 1-7 ã‚’ 0-1 ã«æ­£è¦åŒ–\n            for trait in BIG_FIVE_TRAITS\n        ], dtype=torch.float32)\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'labels': personality_scores\n        }\n\n# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ\nBATCH_SIZE = 4  # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–\n\ntrain_dataset = PersonalityDataset(train_data, tokenizer)\nval_dataset = PersonalityDataset(val_data, tokenizer)\ntest_dataset = PersonalityDataset(test_data, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\nprint(f\"\\nâœ… Datasetæº–å‚™å®Œäº†\")\nprint(f\"  Max Length: 256 tokens (ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–)\")\nprint(f\"  Batch Size: {BATCH_SIZE} (æ¥µå°ãƒ¡ãƒ¢ãƒªãƒ¢ãƒ¼ãƒ‰)\")\nprint(f\"  Train batches: {len(train_loader)}\")\nprint(f\"  Val batches: {len(val_loader)}\")\nprint(f\"  Test batches: {len(test_loader)}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "9K06k2jZ2eB3",
    "outputId": "c6963d1e-a16b-4257-a0a6-b4818bb81a6f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "## 4. ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰\n\nLUKE + 5ã¤ã®å›å¸°ãƒ˜ãƒƒãƒ‰ï¼ˆå„Big Fiveç‰¹æ€§ã«1ã¤ï¼‰\n\n**æ³¨æ„**: ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰å‰ã«GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9vOr3Le32eB3"
   },
   "source": "# GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰å‰ï¼‰\nimport gc\ntorch.cuda.empty_cache()\ngc.collect()\nprint(\"âœ… GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ\")\n\n# LUKE Personality Recognition Model\nclass LukePersonalityModel(nn.Module):\n    def __init__(self, model_name=MODEL_NAME, num_traits=5):\n        super().__init__()\n        self.luke = LukeModel.from_pretrained(model_name)\n        \n        # å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æœ‰åŠ¹åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰\n        self.luke.gradient_checkpointing_enable()\n        \n        self.hidden_size = self.luke.config.hidden_size\n        \n        # å„Big Fiveç‰¹æ€§ã«å¯¾ã™ã‚‹å›å¸°ãƒ˜ãƒƒãƒ‰\n        self.regression_heads = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(self.hidden_size, 256),\n                nn.ReLU(),\n                nn.Dropout(0.1),\n                nn.Linear(256, 1)\n            )\n            for _ in range(num_traits)\n        ])\n    \n    def forward(self, input_ids, attention_mask):\n        # LUKE encoding\n        outputs = self.luke(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        # [CLS] token ã® hidden state\n        pooled_output = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n        \n        # å„ç‰¹æ€§ã®äºˆæ¸¬\n        predictions = []\n        for head in self.regression_heads:\n            pred = head(pooled_output)  # [batch_size, 1]\n            predictions.append(pred)\n        \n        # [batch_size, 5]\n        predictions = torch.cat(predictions, dim=1)\n        \n        # 0-1 ã®ç¯„å›²ã«åˆ¶é™ï¼ˆSigmoidï¼‰\n        predictions = torch.sigmoid(predictions)\n        \n        return predictions\n\n# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–\nprint(\"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...\")\nmodel = LukePersonalityModel().to(device)\n\nprint(f\"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\nprint(f\"âœ… å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: æœ‰åŠ¹åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰\")\n\n# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç¢ºèª\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated(0) / 1e9\n    reserved = torch.cuda.memory_reserved(0) / 1e9\n    print(f\"âœ… GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31mAYDid2eB3"
   },
   "source": [
    "## 5. å­¦ç¿’è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUmOPdck2eB3",
    "outputId": "78ad2972-ae59-4409-a8ee-eaa96dc22130",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nLEARNING_RATE = 1e-5\nNUM_EPOCHS = 20\nWARMUP_STEPS = 150\nEARLY_STOPPING_PATIENCE = 5  # 3 â†’ 5ã«å¤‰æ›´ï¼ˆã‚ˆã‚Šé•·ãå­¦ç¿’ï¼‰\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n\n# Learning Rate Scheduler\ntotal_steps = len(train_loader) * NUM_EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=WARMUP_STEPS,\n    num_training_steps=total_steps\n)\n\n# æå¤±é–¢æ•°ï¼ˆMAEï¼‰\ncriterion = nn.L1Loss()  # Mean Absolute Error\n\nprint(f\"\\nâœ… å­¦ç¿’è¨­å®šå®Œäº†\")\nprint(f\"  Learning Rate: {LEARNING_RATE}\")\nprint(f\"  Total Steps: {total_steps}\")\nprint(f\"  Warmup Steps: {WARMUP_STEPS}\")\nprint(f\"  Early Stopping Patience: {EARLY_STOPPING_PATIENCE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EO2lf4K72eB3",
    "outputId": "ab90abbb-cbdd-40da-b23a-5e442aea60a2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "è©•ä¾¡é–¢æ•°æº–å‚™å®Œäº†\n"
     ]
    }
   ],
   "source": [
    "# è©•ä¾¡é–¢æ•°\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    return all_predictions, all_labels, avg_loss\n",
    "\n",
    "print(\"è©•ä¾¡é–¢æ•°æº–å‚™å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 6. å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ï¼ˆæ™‚é–“ç¯€ç´„ï¼‰\n\n**é‡è¦**: å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã§è½ã¡ãŸå ´åˆã€æ¬¡å›ã¯ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã ã‘ã§OKï¼\n- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—\n- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç›´æ¥ãƒ­ãƒ¼ãƒ‰ã—ã¦å­¦ç¿’é–‹å§‹",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã‚’Google Driveã«ä¿å­˜\n# ã“ã‚Œã«ã‚ˆã‚Šã€å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã§è½ã¡ã¦ã‚‚æ¬¡å›ã¯é«˜é€Ÿã«å†é–‹ã§ãã¾ã™\n\nimport pickle\n\nPREPROCESSED_CACHE = CACHE_DIR / \"preprocessed_training_state.pkl\"\n\nprint(\"=\"*70)\nprint(\"å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­...\")\nprint(\"=\"*70)\n\n# ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿\ntraining_state = {\n    'train_data': train_data,\n    'val_data': val_data,\n    'test_data': test_data,\n    'model_state': model.state_dict(),\n    'config': {\n        'MODEL_NAME': MODEL_NAME,\n        'BIG_FIVE_TRAITS': BIG_FIVE_TRAITS,\n        'BATCH_SIZE': BATCH_SIZE,\n        'CHECKPOINT_DIR': CHECKPOINT_DIR,\n        'LEARNING_RATE': LEARNING_RATE,\n        'NUM_EPOCHS': NUM_EPOCHS,\n        'WARMUP_STEPS': WARMUP_STEPS,\n        'EARLY_STOPPING_PATIENCE': EARLY_STOPPING_PATIENCE\n    }\n}\n\nwith open(PREPROCESSED_CACHE, 'wb') as f:\n    pickle.dump(training_state, f)\n\nprint(f\"âœ… ä¿å­˜å®Œäº†: {PREPROCESSED_CACHE}\")\nprint(f\"âœ… ã‚µã‚¤ã‚º: {PREPROCESSED_CACHE.stat().st_size / 1e6:.1f} MB\")\nprint(\"\\nğŸ’¡ æ¬¡å›å®Ÿè¡Œæ™‚:\")\nprint(\"  1. ã‚»ãƒ«1-3: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®ã¿å®Ÿè¡Œ\")\nprint(\"  2. æ¬¡ã®ã‚»ãƒ«: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¾©å…ƒ\")\nprint(\"  3. å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’ç›´æ¥å®Ÿè¡Œ\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ğŸš€ é«˜é€Ÿå†é–‹ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¾©å…ƒï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n\n**å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã§è½ã¡ãŸå ´åˆã®å†é–‹æ‰‹é †:**\n1. ã‚»ãƒ«1-3ã‚’å®Ÿè¡Œï¼ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰\n2. **ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œ**ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¾©å…ƒï¼‰\n3. å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œ\n\nã“ã‚Œã§ã€ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†ï¼ˆ5-10åˆ†ï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã§ãã¾ã™ï¼",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒï¼ˆé«˜é€Ÿå†é–‹ï¼‰\nimport pickle\nimport gc\nfrom pathlib import Path\n\n# Google Driveä¸Šã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¹\nCACHE_DIR = Path(\"/content/drive/MyDrive/real_persona_chat_cache\")\nPREPROCESSED_CACHE = CACHE_DIR / \"preprocessed_training_state.pkl\"\n\nif PREPROCESSED_CACHE.exists():\n    print(\"=\"*70)\n    print(\"ğŸš€ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¾©å…ƒä¸­...\")\n    print(\"=\"*70)\n    \n    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ­ãƒ¼ãƒ‰\n    with open(PREPROCESSED_CACHE, 'rb') as f:\n        training_state = pickle.load(f)\n    \n    # ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ\n    train_data = training_state['train_data']\n    val_data = training_state['val_data']\n    test_data = training_state['test_data']\n    config = training_state['config']\n    \n    # è¨­å®šã‚’å¾©å…ƒ\n    MODEL_NAME = config['MODEL_NAME']\n    BIG_FIVE_TRAITS = config['BIG_FIVE_TRAITS']\n    BATCH_SIZE = config['BATCH_SIZE']\n    CHECKPOINT_DIR = config['CHECKPOINT_DIR']\n    LEARNING_RATE = config['LEARNING_RATE']\n    NUM_EPOCHS = config['NUM_EPOCHS']\n    WARMUP_STEPS = config['WARMUP_STEPS']\n    EARLY_STOPPING_PATIENCE = config['EARLY_STOPPING_PATIENCE']\n    \n    print(f\"âœ… ãƒ‡ãƒ¼ã‚¿å¾©å…ƒå®Œäº†:\")\n    print(f\"  Train samples: {len(train_data)}\")\n    print(f\"  Val samples: {len(val_data)}\")\n    print(f\"  Test samples: {len(test_data)}\")\n    \n    # Tokenizerãƒ­ãƒ¼ãƒ‰\n    print(\"\\nğŸ“¥ Tokenizerãƒ­ãƒ¼ãƒ‰ä¸­...\")\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    \n    # Dataset & DataLoaderä½œæˆ\n    print(\"ğŸ“¦ DataLoaderä½œæˆä¸­...\")\n    train_dataset = PersonalityDataset(train_data, tokenizer)\n    val_dataset = PersonalityDataset(val_data, tokenizer)\n    test_dataset = PersonalityDataset(test_data, tokenizer)\n    \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n    \n    # GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ\n    print(\"ğŸ¤– ãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...\")\n    model = LukePersonalityModel().to(device)\n    model.load_state_dict(training_state['model_state'])\n    model.luke.gradient_checkpointing_enable()\n    \n    # Optimizer & Scheduler\n    print(\"âš™ï¸  Optimizer & Schedulerè¨­å®šä¸­...\")\n    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n    total_steps = len(train_loader) * NUM_EPOCHS\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=WARMUP_STEPS,\n        num_training_steps=total_steps\n    )\n    criterion = nn.L1Loss()\n    \n    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n    Path(CHECKPOINT_DIR).mkdir(exist_ok=True)\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"âœ… å¾©å…ƒå®Œäº†ï¼å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã§ãã¾ã™\")\n    print(\"=\"*70)\n    print(f\"  Model: {MODEL_NAME}\")\n    print(f\"  Batch Size: {BATCH_SIZE}\")\n    print(f\"  Train batches: {len(train_loader)}\")\n    print(f\"  Val batches: {len(val_loader)}\")\n    print(f\"  Test batches: {len(test_loader)}\")\n    \n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated(0) / 1e9\n        reserved = torch.cuda.memory_reserved(0) / 1e9\n        print(f\"  GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n    \n    print(\"\\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œ\")\n    print(\"=\"*70)\n    \nelse:\n    print(\"âš ï¸  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n    print(\"ğŸ’¡ åˆå›å®Ÿè¡Œæ™‚ã¯ã€ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‹ã‚‰é †ç•ªã«å®Ÿè¡Œã—ã¦ãã ã•ã„\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å­¦ç¿’ãƒ«ãƒ¼ãƒ—\n\nå­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FdWKfyCN2eB3"
   },
   "source": "# å­¦ç¿’å±¥æ­´\nhistory = {\n    'train_loss': [],\n    'val_loss': [],\n    'val_mae': []\n}\n\nbest_val_loss = float('inf')\npatience_counter = 0\n\n# å‹¾é…è“„ç©ï¼ˆå®Ÿè³ªçš„ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§ããã—ã¤ã¤ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ï¼‰\nACCUMULATION_STEPS = 8  # 4 * 8 = å®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚º32ç›¸å½“\n\n# Mixed Precision (FP16) ã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒ¢ãƒªå‰Šæ¸›\nfrom torch.cuda.amp import autocast, GradScaler\nscaler = GradScaler()\n\nprint(\"=\"*70)\nprint(\"å­¦ç¿’é–‹å§‹\")\nprint(f\"å®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH_SIZE * ACCUMULATION_STEPS} (å‹¾é…è“„ç©)\")\nprint(f\"Mixed Precision: æœ‰åŠ¹åŒ– (FP16)\")\nprint(\"=\"*70)\n\n# å­¦ç¿’ãƒ«ãƒ¼ãƒ—\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\n{'='*70}\")\n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n    print(f\"{'='*70}\")\n    \n    # Training\n    model.train()\n    train_loss = 0\n    \n    optimizer.zero_grad()  # æœ€åˆã«ã‚¼ãƒ­åŒ–\n    \n    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        # Mixed Precision (FP16) ã§ forward pass\n        with autocast():\n            predictions = model(input_ids, attention_mask)\n            loss = criterion(predictions, labels)\n            loss = loss / ACCUMULATION_STEPS  # å‹¾é…è“„ç©ã®ãŸã‚ã«ã‚¹ã‚±ãƒ¼ãƒ«\n        \n        # Backward pass with scaled gradients\n        scaler.scale(loss).backward()\n        \n        # å‹¾é…è“„ç©ï¼šACCUMULATION_STEPSå›ã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°\n        if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            optimizer.zero_grad()\n        \n        train_loss += loss.item() * ACCUMULATION_STEPS  # å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™\n        \n        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ10ãƒãƒƒãƒã”ã¨ï¼‰\n        if (batch_idx + 1) % 10 == 0:\n            torch.cuda.empty_cache()\n    \n    # æ®‹ã‚Šã®å‹¾é…ã‚’å‡¦ç†\n    if (len(train_loader) % ACCUMULATION_STEPS) != 0:\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n    \n    avg_train_loss = train_loss / len(train_loader)\n    history['train_loss'].append(avg_train_loss)\n    \n    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n    torch.cuda.empty_cache()\n    \n    # Validation\n    val_predictions, val_labels, val_loss = evaluate_model(model, val_loader, device)\n    val_mae = mean_absolute_error(val_labels, val_predictions)\n    \n    history['val_loss'].append(val_loss)\n    history['val_mae'].append(val_mae)\n    \n    print(f\"\\nğŸ“Š Results:\")\n    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n    print(f\"  Val Loss: {val_loss:.4f}\")\n    print(f\"  Val MAE: {val_mae:.4f}\")\n    \n    # GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated(0) / 1e9\n        reserved = torch.cuda.memory_reserved(0) / 1e9\n        print(f\"  GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n    \n    # Early Stopping & Checkpoint\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience_counter = 0\n        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': val_loss,\n        }, f\"{CHECKPOINT_DIR}/best_model.pt\")\n        print(\"  âœ… Best model saved!\")\n    else:\n        patience_counter += 1\n        print(f\"  âš ï¸  Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n        if patience_counter >= EARLY_STOPPING_PATIENCE:\n            print(f\"\\nğŸ›‘ Early stopping at epoch {epoch+1}\")\n            break\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… å­¦ç¿’å®Œäº†\")\nprint(\"=\"*70)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-q5Bk_v2eB3"
   },
   "source": [
    "## 8. ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogvVgxMJ2eB3"
   },
   "outputs": [],
   "source": [
    "# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
    "print(\"=\"*70)\n",
    "print(\"ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "checkpoint = torch.load(f\"{CHECKPOINT_DIR}/best_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"âœ… Best model loaded! (Epoch {checkpoint['epoch']+1}, Val Loss: {checkpoint['val_loss']:.4f})\")\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡ä¸­...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_predictions, test_labels, test_loss = evaluate_model(model, test_loader, device)\n",
    "print(f\"\\nğŸ“Š Test Loss: {test_loss:.4f}\")\n",
    "print(f\"ğŸ“Š Test MAE: {mean_absolute_error(test_labels, test_predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBy0NRIg2eB3"
   },
   "outputs": [],
   "source": [
    "# è©³ç´°è©•ä¾¡é–¢æ•°\n",
    "def compute_metrics(predictions, labels, trait_names=BIG_FIVE_TRAITS):\n",
    "    \"\"\"\n",
    "    å›å¸°æŒ‡æ¨™ã¨åˆ†é¡æŒ‡æ¨™ã®ä¸¡æ–¹ã‚’è¨ˆç®—\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # ä¸­å¤®å€¤ã‚’è¨ˆç®—ï¼ˆåˆ†é¡ç”¨ï¼‰\n",
    "    medians = np.median(labels, axis=0)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Personality Recognition Results\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # å›å¸°æŒ‡æ¨™\n",
    "    print(\"\\nRegression Metrics:\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Trait':<18} {'MAE':>8} {'RMSE':>8} {'Pearson':>10} {'Spearman':>10}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    regression_metrics = []\n",
    "    for i, trait in enumerate(trait_names):\n",
    "        pred = predictions[:, i]\n",
    "        true = labels[:, i]\n",
    "\n",
    "        mae = mean_absolute_error(true, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(true, pred))\n",
    "\n",
    "        # ç›¸é–¢ä¿‚æ•°ï¼ˆpå€¤ã‚‚è¨ˆç®—ï¼‰\n",
    "        pearson_corr, pearson_p = pearsonr(true, pred)\n",
    "        spearman_corr, spearman_p = spearmanr(true, pred)\n",
    "\n",
    "        regression_metrics.append({\n",
    "            'trait': trait,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'pearson': pearson_corr,\n",
    "            'spearman': spearman_corr\n",
    "        })\n",
    "\n",
    "        print(f\"{trait:<18} {mae:>8.4f} {rmse:>8.4f} {pearson_corr:>10.3f} {spearman_corr:>10.3f}\")\n",
    "\n",
    "    # å¹³å‡\n",
    "    avg_mae = np.mean([m['mae'] for m in regression_metrics])\n",
    "    avg_rmse = np.mean([m['rmse'] for m in regression_metrics])\n",
    "    avg_pearson = np.mean([m['pearson'] for m in regression_metrics])\n",
    "    avg_spearman = np.mean([m['spearman'] for m in regression_metrics])\n",
    "\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Average':<18} {avg_mae:>8.4f} {avg_rmse:>8.4f} {avg_pearson:>10.3f} {avg_spearman:>10.3f}\")\n",
    "\n",
    "    # åˆ†é¡æŒ‡æ¨™ï¼ˆä¸­å¤®å€¤ã§2å€¤åŒ–ï¼‰\n",
    "    print(\"\\n\\nClassification Metrics (High/Low by median):\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Trait':<18} {'Acc':>6} {'Bal Acc':>8} {'Prec':>6} {'Rec':>6} {'F1':>6}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    classification_metrics = []\n",
    "    for i, trait in enumerate(trait_names):\n",
    "        pred = predictions[:, i]\n",
    "        true = labels[:, i]\n",
    "        median = medians[i]\n",
    "\n",
    "        # 2å€¤åŒ–\n",
    "        pred_binary = (pred > median).astype(int)\n",
    "        true_binary = (true > median).astype(int)\n",
    "\n",
    "        acc = accuracy_score(true_binary, pred_binary)\n",
    "        bal_acc = balanced_accuracy_score(true_binary, pred_binary)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "            true_binary, pred_binary, average='binary', zero_division=0\n",
    "        )\n",
    "\n",
    "        classification_metrics.append({\n",
    "            'trait': trait,\n",
    "            'accuracy': acc,\n",
    "            'balanced_accuracy': bal_acc,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'f1': f1\n",
    "        })\n",
    "\n",
    "        print(f\"{trait:<18} {acc:>6.1%} {bal_acc:>8.1%} {prec:>6.2f} {rec:>6.2f} {f1:>6.2f}\")\n",
    "\n",
    "    # å¹³å‡\n",
    "    avg_acc = np.mean([m['accuracy'] for m in classification_metrics])\n",
    "    avg_bal_acc = np.mean([m['balanced_accuracy'] for m in classification_metrics])\n",
    "    avg_prec = np.mean([m['precision'] for m in classification_metrics])\n",
    "    avg_rec = np.mean([m['recall'] for m in classification_metrics])\n",
    "    avg_f1 = np.mean([m['f1'] for m in classification_metrics])\n",
    "\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Average':<18} {avg_acc:>6.1%} {avg_bal_acc:>8.1%} {avg_prec:>6.2f} {avg_rec:>6.2f} {avg_f1:>6.2f}\")\n",
    "\n",
    "    # è«–æ–‡ã¨ã®æ¯”è¼ƒ\n",
    "    print(\"\\n\\nComparison with Paper (Fu et al. 2024):\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Paper Balanced Accuracy (Monologue): 60.4%\")\n",
    "    print(f\"Our Balanced Accuracy:                {avg_bal_acc:.1%}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    results['regression'] = regression_metrics\n",
    "    results['classification'] = classification_metrics\n",
    "    results['averages'] = {\n",
    "        'mae': avg_mae,\n",
    "        'rmse': avg_rmse,\n",
    "        'pearson': avg_pearson,\n",
    "        'spearman': avg_spearman,\n",
    "        'accuracy': avg_acc,\n",
    "        'balanced_accuracy': avg_bal_acc\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# è©•ä¾¡å®Ÿè¡Œ\n",
    "results = compute_metrics(test_predictions, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k1m9orP2eB3"
   },
   "source": [
    "## 9. å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFBqHgx_2eB3"
   },
   "outputs": [],
   "source": [
    "# å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–\n",
    "def plot_training_history(history):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss (MAE)')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # MAE\n",
    "    axes[1].plot(history['val_mae'], label='Val MAE', marker='o', color='green')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('MAE')\n",
    "    axes[1].set_title('Validation MAE')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{CHECKPOINT_DIR}/training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyUhZgMa2eB3"
   },
   "outputs": [],
   "source": [
    "# æ•£å¸ƒå›³: äºˆæ¸¬ vs çœŸå€¤\n",
    "def plot_predictions(predictions, labels, trait_names=BIG_FIVE_TRAITS):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, trait in enumerate(trait_names):\n",
    "        ax = axes[i]\n",
    "\n",
    "        pred = predictions[:, i]\n",
    "        true = labels[:, i]\n",
    "\n",
    "        # æ•£å¸ƒå›³\n",
    "        ax.scatter(true, pred, alpha=0.5, s=20)\n",
    "\n",
    "        # ç†æƒ³ç·š\n",
    "        min_val = min(true.min(), pred.min())\n",
    "        max_val = max(true.max(), pred.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Ideal')\n",
    "\n",
    "        # ç›¸é–¢ä¿‚æ•°\n",
    "        pearson, _ = pearsonr(true, pred)\n",
    "        mae = mean_absolute_error(true, pred)\n",
    "\n",
    "        ax.set_xlabel('True Score')\n",
    "        ax.set_ylabel('Predicted Score')\n",
    "        ax.set_title(f'{trait}\\nPearson: {pearson:.3f}, MAE: {mae:.3f}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # æœ€å¾Œã®axesã‚’éè¡¨ç¤º\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{CHECKPOINT_DIR}/predictions_scatter.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# æ•£å¸ƒå›³ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "plot_predictions(test_predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zj5mQ0Ij2eB3"
   },
   "outputs": [],
   "source": [
    "# æ··åŒè¡Œåˆ—ï¼ˆåˆ†é¡ã¨ã—ã¦è©•ä¾¡ï¼‰\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrices(predictions, labels, trait_names=BIG_FIVE_TRAITS):\n",
    "    medians = np.median(labels, axis=0)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, trait in enumerate(trait_names):\n",
    "        ax = axes[i]\n",
    "\n",
    "        pred = predictions[:, i]\n",
    "        true = labels[:, i]\n",
    "        median = medians[i]\n",
    "\n",
    "        # 2å€¤åŒ–\n",
    "        pred_binary = (pred > median).astype(int)\n",
    "        true_binary = (true > median).astype(int)\n",
    "\n",
    "        # æ··åŒè¡Œåˆ—\n",
    "        cm = confusion_matrix(true_binary, pred_binary)\n",
    "\n",
    "        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                    xticklabels=['Low', 'High'],\n",
    "                    yticklabels=['Low', 'High'])\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('True')\n",
    "        ax.set_title(f'{trait} - Confusion Matrix')\n",
    "\n",
    "    # æœ€å¾Œã®axesã‚’éè¡¨ç¤º\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{CHECKPOINT_DIR}/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# æ··åŒè¡Œåˆ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "plot_confusion_matrices(test_predictions, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv3ZhqGH2eB3"
   },
   "source": [
    "## 10. ã¾ã¨ã‚\n\nã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ä»¥ä¸‹ã‚’å®Ÿè£…ã—ã¾ã—ãŸï¼š\n\n1. âœ… RealPersonaChatãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\n2. âœ… LUKEå›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰\n3. âœ… ãƒ¢ãƒãƒ­ãƒ¼ã‚°è¨­å®šã§ã®å­¦ç¿’\n4. âœ… å›å¸°æŒ‡æ¨™ï¼ˆMAE, RMSE, ç›¸é–¢ï¼‰ã¨åˆ†é¡æŒ‡æ¨™ï¼ˆAcc, F1ï¼‰ã®è©•ä¾¡\n5. âœ… è«–æ–‡ï¼ˆFu et al. 2024ï¼‰ã¨ã®æ¯”è¼ƒ\n\n**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:**\n- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å®Ÿéš›ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦å®Ÿè¡Œ\n- å­¦ç¿’ã‚’å®Ÿè¡Œã—ã€çµæœã‚’åˆ†æ\n- ç™ºè¡¨è³‡æ–™ã®ä½œæˆ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}