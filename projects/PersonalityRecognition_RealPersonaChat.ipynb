{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wa-ko/Natural-Language-Processing-with-Deep-Learning/blob/main/projects/PersonalityRecognition_RealPersonaChat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Big Five Personality Recognition using LUKE on RealPersonaChat\n",
        "\n",
        "**Project Overview:**\n",
        "- Dataset: RealPersonaChat (14,000 dialogues, 233 speakers)\n",
        "- Task: Big Five personality trait regression prediction\n",
        "- Model: LUKE (studio-ousia/luke-japanese-base)\n",
        "- Setting: Monologue (speaker's utterances only)\n",
        "\n",
        "**Evaluation Metrics:**\n",
        "- Regression: MAE, RMSE, Pearson, Spearman correlation\n",
        "- Classification: Accuracy, Balanced Accuracy, Precision, Recall, F1\n",
        "\n",
        "**Memory Optimization (T4 GPU):**\n",
        "- Batch size: 4\n",
        "- Max Length: 256 tokens\n",
        "- Gradient accumulation: 8 steps (effective batch size 32)\n",
        "- Gradient checkpointing: Enabled\n",
        "- Mixed Precision (FP16): Enabled\n",
        "\n",
        "**Execution Instructions:**\n",
        "\n",
        "Execute all cells in order from top to bottom.\n",
        "\n",
        "**Data Loading:**\n",
        "- Speaker data: Download from GitHub (233 speakers)\n",
        "- Dialogue data: Download from GitHub (14,000 dialogues, takes 30-40 minutes)\n",
        "\n",
        "**Recommended Environment:**\n",
        "- Google Colab with T4 GPU (15GB VRAM)"
      ],
      "metadata": {
        "id": "WHsyRzPfotDA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpQBruNV2eB1"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIUAvxtp2eB2"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    LukeModel,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "import json\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    accuracy_score,\n",
        "    balanced_accuracy_score,\n",
        "    precision_recall_fscore_support\n",
        ")\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure device (GPU/CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# Model and data configuration\n",
        "MODEL_NAME = \"studio-ousia/luke-japanese-base\"\n",
        "BIG_FIVE_TRAITS = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\n",
        "CHECKPOINT_DIR = \"/content/checkpoints\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka9sYMGW2eB2",
        "outputId": "847a76ce-9f3c-476f-bba5-f08768e71c5d"
      },
      "source": [
        "## 2. Data Loading\n",
        "\n",
        "Load RealPersonaChat dataset directly from GitHub.\n",
        "- Speaker data: `interlocutors.json` (233 speakers)\n",
        "- Dialogue data: `dialogues/*.json` (14,000 dialogues)\n",
        "\n",
        "**Data Size:**\n",
        "- **Production (Recommended)**: `num_dialogues = 14000` - Full dataset\n",
        "- Test: `num_dialogues = 1000` - Partial dataset for quick testing\n",
        "\n",
        "**Note**:\n",
        "- Initial data loading takes 30-40 minutes for full dataset (14,000 dialogues)\n",
        "- Using full data is strongly recommended for better results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJX4RnZ-2eB2"
      },
      "outputs": [],
      "source": [
        "# Load dataset directly from GitHub\n",
        "import json\n",
        "import requests\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "BASE_URL = \"https://raw.githubusercontent.com/nu-dialogue/real-persona-chat/main/real_persona_chat\"\n",
        "\n",
        "# Load speaker data (interlocutors)\n",
        "print(\"=\"*70)\n",
        "print(\"Loading speaker data...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "interlocutors_url = f\"{BASE_URL}/interlocutors.json\"\n",
        "response = requests.get(interlocutors_url)\n",
        "interlocutors_raw = response.json()\n",
        "\n",
        "# Convert data structure to dictionary\n",
        "if isinstance(interlocutors_raw, dict):\n",
        "    interlocutor_dict = interlocutors_raw\n",
        "elif isinstance(interlocutors_raw, list):\n",
        "    interlocutor_dict = {\n",
        "        item['interlocutor_id']: item\n",
        "        for item in interlocutors_raw\n",
        "    }\n",
        "else:\n",
        "    print(f\"Unexpected data type: {type(interlocutors_raw)}\")\n",
        "    interlocutor_dict = {}\n",
        "\n",
        "print(f\"Loaded {len(interlocutor_dict)} speakers\")\n",
        "\n",
        "# Display sample\n",
        "if interlocutor_dict:\n",
        "    first_speaker_id = list(interlocutor_dict.keys())[0]\n",
        "    sample_speaker = interlocutor_dict[first_speaker_id]\n",
        "    print(f\"\\nSample speaker (ID: {first_speaker_id}):\")\n",
        "    print(f\"  Keys: {list(sample_speaker.keys())}\")\n",
        "    if 'personality' in sample_speaker:\n",
        "        print(f\"  Personality keys: {list(sample_speaker['personality'].keys())[:10]}\")\n",
        "\n",
        "# Load dialogue data\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Loading dialogue data...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "num_dialogues = 1000  # Set to 14000 for full dataset\n",
        "\n",
        "print(f\"Downloading {num_dialogues} dialogues from GitHub...\")\n",
        "print(\"(This may take 30-40 minutes for full dataset)\")\n",
        "\n",
        "dialogue_data = []\n",
        "failed_downloads = 0\n",
        "\n",
        "for i in tqdm(range(1, num_dialogues + 1), desc=\"Downloading dialogues\"):\n",
        "    dialogue_id = f\"{i:05d}\"\n",
        "    dialogue_url = f\"{BASE_URL}/dialogues/{dialogue_id}.json\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(dialogue_url)\n",
        "        if response.status_code == 200:\n",
        "            dialogue = response.json()\n",
        "            dialogue_data.append(dialogue)\n",
        "        else:\n",
        "            failed_downloads += 1\n",
        "    except Exception as e:\n",
        "        failed_downloads += 1\n",
        "        if i <= 10:  # Only show first 10 errors\n",
        "            print(f\"Failed to download {dialogue_id}: {e}\")\n",
        "\n",
        "if failed_downloads > 0:\n",
        "    print(f\"\\nDownload failed: {failed_downloads} dialogues\")\n",
        "\n",
        "print(f\"Loaded {len(dialogue_data)} dialogues\")\n",
        "\n",
        "# Display sample\n",
        "if dialogue_data:\n",
        "    sample = dialogue_data[0]\n",
        "    print(f\"\\nSample dialogue:\")\n",
        "    print(f\"  Keys: {list(sample.keys())}\")\n",
        "    print(f\"  Dialogue ID: {sample.get('dialogue_id', 'N/A')}\")\n",
        "    print(f\"  Interlocutors: {sample.get('interlocutors', [])}\")\n",
        "    print(f\"  Utterances: {len(sample.get('utterances', []))}\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_monologue_dataset(dialogue_data, interlocutor_dict):\n",
        "    \"\"\"\n",
        "    Extract monologues for each speaker from dialogue data\n",
        "\n",
        "    Args:\n",
        "        dialogue_data: List of dialogue dictionaries\n",
        "        interlocutor_dict: Dictionary of speaker ID -> speaker information\n",
        "\n",
        "    Returns:\n",
        "        monologues: List[Dict]\n",
        "            - 'speaker_id': str\n",
        "            - 'text': str (concatenated utterances)\n",
        "            - 'personality': Dict[str, float] (Big Five scores)\n",
        "    \"\"\"\n",
        "    monologues = []\n",
        "\n",
        "    # Process each dialogue\n",
        "    for dialogue in tqdm(dialogue_data, desc=\"Processing dialogues\"):\n",
        "        speaker_utterances = {}\n",
        "\n",
        "        # Collect utterances by speaker\n",
        "        for utterance in dialogue.get('utterances', []):\n",
        "            speaker_id = utterance.get('interlocutor_id')\n",
        "            text = utterance.get('text', '')\n",
        "\n",
        "            if not speaker_id or not text:\n",
        "                continue\n",
        "\n",
        "            if speaker_id not in speaker_utterances:\n",
        "                speaker_utterances[speaker_id] = []\n",
        "            speaker_utterances[speaker_id].append(text)\n",
        "\n",
        "        # Create monologue for each speaker\n",
        "        for speaker_id, utterances in speaker_utterances.items():\n",
        "            # Skip if speaker info not available\n",
        "            if speaker_id not in interlocutor_dict:\n",
        "                continue\n",
        "\n",
        "            speaker_info = interlocutor_dict[speaker_id]\n",
        "            personality_data = speaker_info.get('personality', {})\n",
        "\n",
        "            # Extract Big Five scores (1-7 scale)\n",
        "            big_five_scores = {\n",
        "                'Openness': personality_data.get('big_five_openness', personality_data.get('openness', 4.0)),\n",
        "                'Conscientiousness': personality_data.get('big_five_conscientiousness', personality_data.get('conscientiousness', 4.0)),\n",
        "                'Extraversion': personality_data.get('big_five_extraversion', personality_data.get('extraversion', 4.0)),\n",
        "                'Agreeableness': personality_data.get('big_five_agreeableness', personality_data.get('agreeableness', 4.0)),\n",
        "                'Neuroticism': personality_data.get('big_five_neuroticism', personality_data.get('neuroticism', 4.0)),\n",
        "            }\n",
        "\n",
        "            # Check if scores are properly obtained (not all default values)\n",
        "            if all(score == 4.0 for score in big_five_scores.values()):\n",
        "                # Try alternative key names\n",
        "                for key, value in personality_data.items():\n",
        "                    if 'openness' in key.lower():\n",
        "                        big_five_scores['Openness'] = value\n",
        "                    elif 'conscientiousness' in key.lower():\n",
        "                        big_five_scores['Conscientiousness'] = value\n",
        "                    elif 'extraversion' in key.lower():\n",
        "                        big_five_scores['Extraversion'] = value\n",
        "                    elif 'agreeableness' in key.lower():\n",
        "                        big_five_scores['Agreeableness'] = value\n",
        "                    elif 'neuroticism' in key.lower():\n",
        "                        big_five_scores['Neuroticism'] = value\n",
        "\n",
        "            # Add monologue sample\n",
        "            monologues.append({\n",
        "                'speaker_id': speaker_id,\n",
        "                'text': ' '.join(utterances),\n",
        "                'personality': big_five_scores\n",
        "            })\n",
        "\n",
        "    return monologues\n",
        "\n",
        "# Create monologue dataset\n",
        "monologue_data = create_monologue_dataset(dialogue_data, interlocutor_dict)\n",
        "\n",
        "print(f\"\\nMonologue samples: {len(monologue_data)}\")\n",
        "if monologue_data:\n",
        "    print(f\"Sample: Speaker {monologue_data[0]['speaker_id']}, Text length: {len(monologue_data[0]['text'])} chars\")"
      ],
      "metadata": {
        "id": "hUOMkTaFobKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_by_speaker(monologue_data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
        "    \"\"\"\n",
        "    Split data ensuring no speaker overlap between train/val/test sets\n",
        "    Following the paper's approach (Train/Val/Test = 8:1:1)\n",
        "    \"\"\"\n",
        "    # Group samples by speaker\n",
        "    speaker_groups = {}\n",
        "    for sample in monologue_data:\n",
        "        speaker_id = sample['speaker_id']\n",
        "        if speaker_id not in speaker_groups:\n",
        "            speaker_groups[speaker_id] = []\n",
        "        speaker_groups[speaker_id].append(sample)\n",
        "\n",
        "    # Shuffle speakers\n",
        "    speakers = list(speaker_groups.keys())\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(speakers)\n",
        "\n",
        "    # Split speakers into train/val/test\n",
        "    n_speakers = len(speakers)\n",
        "    n_train = int(n_speakers * train_ratio)\n",
        "    n_val = int(n_speakers * val_ratio)\n",
        "\n",
        "    train_speakers = speakers[:n_train]\n",
        "    val_speakers = speakers[n_train:n_train+n_val]\n",
        "    test_speakers = speakers[n_train+n_val:]\n",
        "\n",
        "    # Extract samples for each split\n",
        "    train_data = [s for spk in train_speakers for s in speaker_groups[spk]]\n",
        "    val_data = [s for spk in val_speakers for s in speaker_groups[spk]]\n",
        "    test_data = [s for spk in test_speakers for s in speaker_groups[spk]]\n",
        "\n",
        "    print(f\"\\nData split:\")\n",
        "    print(f\"  Train: {len(train_speakers)} speakers, {len(train_data)} samples\")\n",
        "    print(f\"  Val:   {len(val_speakers)} speakers, {len(val_data)} samples\")\n",
        "    print(f\"  Test:  {len(test_speakers)} speakers, {len(test_data)} samples\")\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "# Split data\n",
        "train_data, val_data, test_data = split_by_speaker(monologue_data)"
      ],
      "metadata": {
        "id": "Hzsix4NfobKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "- Encode text with LUKE tokenizer\n",
        "- Normalize Big Five scores (1-7 → 0-1)"
      ],
      "metadata": {
        "id": "X1zPZgcHobKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "class PersonalityDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for personality prediction\n",
        "    Tokenizes text and normalizes Big Five scores\n",
        "    \"\"\"\n",
        "    def __init__(self, data, tokenizer, max_length=256):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        # Tokenize text\n",
        "        encoding = self.tokenizer(\n",
        "            sample['text'],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Normalize Big Five scores (1-7 → 0-1)\n",
        "        personality_scores = torch.tensor([\n",
        "            (sample['personality'][trait] - 1) / 6\n",
        "            for trait in BIG_FIVE_TRAITS\n",
        "        ], dtype=torch.float32)\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': personality_scores\n",
        "        }\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "BATCH_SIZE = 4  # Small batch size for memory efficiency\n",
        "\n",
        "train_dataset = PersonalityDataset(train_data, tokenizer)\n",
        "val_dataset = PersonalityDataset(val_data, tokenizer)\n",
        "test_dataset = PersonalityDataset(test_data, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(f\"\\nDataset ready: {len(train_loader)} train batches, {len(val_loader)} val batches, {len(test_loader)} test batches\")"
      ],
      "metadata": {
        "id": "RcIYpHtUobKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K06k2jZ2eB3",
        "outputId": "c6963d1e-a16b-4257-a0a6-b4818bb81a6f"
      },
      "source": [
        "## 4. Model Construction\n",
        "\n",
        "LUKE + 5 regression heads (one for each Big Five trait)\n",
        "\n",
        "**Note**: Clear GPU memory before model construction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vOr3Le32eB3"
      },
      "source": [
        "# Clear GPU memory before model initialization\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "class LukePersonalityModel(nn.Module):\n",
        "    \"\"\"\n",
        "    LUKE-based personality prediction model\n",
        "    Architecture: LUKE encoder + 5 regression heads (one per trait)\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name=MODEL_NAME, num_traits=5):\n",
        "        super().__init__()\n",
        "        # Load pre-trained LUKE model\n",
        "        self.luke = LukeModel.from_pretrained(model_name)\n",
        "\n",
        "        # Enable gradient checkpointing for memory efficiency\n",
        "        self.luke.gradient_checkpointing_enable()\n",
        "\n",
        "        self.hidden_size = self.luke.config.hidden_size\n",
        "\n",
        "        # Create regression head for each Big Five trait\n",
        "        self.regression_heads = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(self.hidden_size, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(256, 1)\n",
        "            )\n",
        "            for _ in range(num_traits)\n",
        "        ])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Encode with LUKE\n",
        "        outputs = self.luke(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Extract [CLS] token representation\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Predict each trait\n",
        "        predictions = []\n",
        "        for head in self.regression_heads:\n",
        "            pred = head(pooled_output)\n",
        "            predictions.append(pred)\n",
        "\n",
        "        # Concatenate predictions and apply sigmoid (0-1 range)\n",
        "        predictions = torch.cat(predictions, dim=1)\n",
        "        predictions = torch.sigmoid(predictions)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Initialize model\n",
        "model = LukePersonalityModel().to(device)\n",
        "\n",
        "print(f\"Model initialized: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M parameters\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f}GB allocated\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31mAYDid2eB3"
      },
      "source": [
        "## 5. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training hyperparameters\n",
        "LEARNING_RATE = 1e-5\n",
        "NUM_EPOCHS = 20\n",
        "WARMUP_STEPS = 150\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Learning rate scheduler with warmup\n",
        "total_steps = len(train_loader) * NUM_EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=WARMUP_STEPS,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Loss function (Mean Absolute Error)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "print(f\"Training config: LR={LEARNING_RATE}, Epochs={NUM_EPOCHS}, Steps={total_steps}\")"
      ],
      "metadata": {
        "id": "KAlCQh1ZobKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EO2lf4K72eB3"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluate model on given dataloader\n",
        "    Returns predictions, labels, and average loss\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = model(input_ids, attention_mask)\n",
        "            loss = criterion(predictions, labels)\n",
        "\n",
        "            # Collect results\n",
        "            total_loss += loss.item()\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    # Stack all batches\n",
        "    all_predictions = np.vstack(all_predictions)\n",
        "    all_labels = np.vstack(all_labels)\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "    return all_predictions, all_labels, avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with gradient accumulation and mixed precision\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'val_mae': []\n",
        "}\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "# Gradient accumulation for larger effective batch size (4 * 8 = 32)\n",
        "ACCUMULATION_STEPS = 8\n",
        "\n",
        "# Mixed precision training for memory efficiency\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Create checkpoint directory\n",
        "Path(CHECKPOINT_DIR).mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Training started: {NUM_EPOCHS} epochs, effective batch size {BATCH_SIZE * ACCUMULATION_STEPS}\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with autocast():\n",
        "            predictions = model(input_ids, attention_mask)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss = loss / ACCUMULATION_STEPS  # Scale for gradient accumulation\n",
        "\n",
        "        # Backward pass\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Update weights every ACCUMULATION_STEPS\n",
        "        if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item() * ACCUMULATION_STEPS\n",
        "\n",
        "        # Periodic memory cleanup\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Handle remaining gradients\n",
        "    if (len(train_loader) % ACCUMULATION_STEPS) != 0:\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "    # Clear memory before validation\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Validation phase\n",
        "    val_predictions, val_labels, val_loss = evaluate_model(model, val_loader, device)\n",
        "    val_mae = mean_absolute_error(val_labels, val_predictions)\n",
        "\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_mae'].append(val_mae)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val MAE: {val_mae:.4f}\")\n",
        "\n",
        "    # Early stopping and checkpoint\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "        }, f\"{CHECKPOINT_DIR}/best_model.pt\")\n",
        "        print(\"Best model saved\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
        "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "print(\"\\nTraining complete\")"
      ],
      "metadata": {
        "id": "xTAgBnyRobKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-q5Bk_v2eB3"
      },
      "source": [
        "## 6. Training Loop\n",
        "\n",
        "Start training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(predictions, labels, trait_names=BIG_FIVE_TRAITS):\n",
        "    \"\"\"\n",
        "    Compute comprehensive metrics for personality prediction\n",
        "    Includes both regression metrics and classification metrics (binarized by median)\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    medians = np.median(labels, axis=0)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Personality Recognition Results\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Regression metrics\n",
        "    print(\"\\nRegression Metrics:\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"{'Trait':<18} {'MAE':>8} {'RMSE':>8} {'Pearson':>10} {'Spearman':>10}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    regression_metrics = []\n",
        "    for i, trait in enumerate(trait_names):\n",
        "        pred = predictions[:, i]\n",
        "        true = labels[:, i]\n",
        "\n",
        "        # Compute regression metrics\n",
        "        mae = mean_absolute_error(true, pred)\n",
        "        rmse = np.sqrt(mean_squared_error(true, pred))\n",
        "        pearson_corr, _ = pearsonr(true, pred)\n",
        "        spearman_corr, _ = spearmanr(true, pred)\n",
        "\n",
        "        regression_metrics.append({\n",
        "            'trait': trait,\n",
        "            'mae': mae,\n",
        "            'rmse': rmse,\n",
        "            'pearson': pearson_corr,\n",
        "            'spearman': spearman_corr\n",
        "        })\n",
        "\n",
        "        print(f\"{trait:<18} {mae:>8.4f} {rmse:>8.4f} {pearson_corr:>10.3f} {spearman_corr:>10.3f}\")\n",
        "\n",
        "    # Compute averages\n",
        "    avg_mae = np.mean([m['mae'] for m in regression_metrics])\n",
        "    avg_rmse = np.mean([m['rmse'] for m in regression_metrics])\n",
        "    avg_pearson = np.mean([m['pearson'] for m in regression_metrics])\n",
        "    avg_spearman = np.mean([m['spearman'] for m in regression_metrics])\n",
        "\n",
        "    print(\"-\"*70)\n",
        "    print(f\"{'Average':<18} {avg_mae:>8.4f} {avg_rmse:>8.4f} {avg_pearson:>10.3f} {avg_spearman:>10.3f}\")\n",
        "\n",
        "    # Classification metrics (binarize by median)\n",
        "    print(\"\\n\\nClassification Metrics (High/Low by median):\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"{'Trait':<18} {'Acc':>6} {'Bal Acc':>8} {'Prec':>6} {'Rec':>6} {'F1':>6}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    classification_metrics = []\n",
        "    for i, trait in enumerate(trait_names):\n",
        "        pred = predictions[:, i]\n",
        "        true = labels[:, i]\n",
        "        median = medians[i]\n",
        "\n",
        "        # Binarize predictions and labels\n",
        "        pred_binary = (pred > median).astype(int)\n",
        "        true_binary = (true > median).astype(int)\n",
        "\n",
        "        # Compute classification metrics\n",
        "        acc = accuracy_score(true_binary, pred_binary)\n",
        "        bal_acc = balanced_accuracy_score(true_binary, pred_binary)\n",
        "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "            true_binary, pred_binary, average='binary', zero_division=0\n",
        "        )\n",
        "\n",
        "        classification_metrics.append({\n",
        "            'trait': trait,\n",
        "            'accuracy': acc,\n",
        "            'balanced_accuracy': bal_acc,\n",
        "            'precision': prec,\n",
        "            'recall': rec,\n",
        "            'f1': f1\n",
        "        })\n",
        "\n",
        "        print(f\"{trait:<18} {acc:>6.1%} {bal_acc:>8.1%} {prec:>6.2f} {rec:>6.2f} {f1:>6.2f}\")\n",
        "\n",
        "    # Compute classification averages\n",
        "    avg_acc = np.mean([m['accuracy'] for m in classification_metrics])\n",
        "    avg_bal_acc = np.mean([m['balanced_accuracy'] for m in classification_metrics])\n",
        "    avg_prec = np.mean([m['precision'] for m in classification_metrics])\n",
        "    avg_rec = np.mean([m['recall'] for m in classification_metrics])\n",
        "    avg_f1 = np.mean([m['f1'] for m in classification_metrics])\n",
        "\n",
        "    print(\"-\"*70)\n",
        "    print(f\"{'Average':<18} {avg_acc:>6.1%} {avg_bal_acc:>8.1%} {avg_prec:>6.2f} {avg_rec:>6.2f} {avg_f1:>6.2f}\")\n",
        "\n",
        "    # Compare with paper\n",
        "    print(\"\\n\\nComparison with Paper (Fu et al. 2024):\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"Paper Balanced Accuracy (Monologue): 60.4%\")\n",
        "    print(f\"Our Balanced Accuracy:                {avg_bal_acc:.1%}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Package results\n",
        "    results['regression'] = regression_metrics\n",
        "    results['classification'] = classification_metrics\n",
        "    results['averages'] = {\n",
        "        'mae': avg_mae,\n",
        "        'rmse': avg_rmse,\n",
        "        'pearson': avg_pearson,\n",
        "        'spearman': avg_spearman,\n",
        "        'accuracy': avg_acc,\n",
        "        'balanced_accuracy': avg_bal_acc\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Compute and display metrics\n",
        "results = compute_metrics(test_predictions, test_labels)"
      ],
      "metadata": {
        "id": "2usQmjHdobKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Test Set Evaluation"
      ],
      "metadata": {
        "id": "futvKlTxobKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model and evaluate on test set\n",
        "checkpoint = torch.load(f\"{CHECKPOINT_DIR}/best_model.pt\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "print(f\"Best model loaded: Epoch {checkpoint['epoch']+1}, Val Loss {checkpoint['val_loss']:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_predictions, test_labels, test_loss = evaluate_model(model, test_loader, device)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test MAE: {mean_absolute_error(test_labels, test_predictions):.4f}\")"
      ],
      "metadata": {
        "id": "JAfE8HyFobKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Visualization"
      ],
      "metadata": {
        "id": "vnypAKr5obKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(predictions, labels, trait_names=BIG_FIVE_TRAITS):\n",
        "    \"\"\"Plot prediction vs true value scatter plots for each trait\"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, trait in enumerate(trait_names):\n",
        "        ax = axes[i]\n",
        "\n",
        "        pred = predictions[:, i]\n",
        "        true = labels[:, i]\n",
        "\n",
        "        # Scatter plot\n",
        "        ax.scatter(true, pred, alpha=0.5, s=20)\n",
        "\n",
        "        # Ideal line (perfect prediction)\n",
        "        min_val = min(true.min(), pred.min())\n",
        "        max_val = max(true.max(), pred.max())\n",
        "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Ideal')\n",
        "\n",
        "        # Compute metrics for title\n",
        "        pearson, _ = pearsonr(true, pred)\n",
        "        mae = mean_absolute_error(true, pred)\n",
        "\n",
        "        ax.set_xlabel('True Score')\n",
        "        ax.set_ylabel('Predicted Score')\n",
        "        ax.set_title(f'{trait}\\nPearson: {pearson:.3f}, MAE: {mae:.3f}')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Hide unused subplot\n",
        "    axes[-1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{CHECKPOINT_DIR}/predictions_scatter.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Plot predictions\n",
        "plot_predictions(test_predictions, test_labels)"
      ],
      "metadata": {
        "id": "pGtm00p5obKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyUhZgMa2eB3"
      },
      "outputs": [],
      "source": [
        "## 5. Training Loop\n",
        "\n",
        "Start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj5mQ0Ij2eB3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrices(predictions, labels, trait_names=BIG_FIVE_TRAITS):\n",
        "    \"\"\"Plot confusion matrices for binarized predictions (High/Low by median)\"\"\"\n",
        "    medians = np.median(labels, axis=0)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, trait in enumerate(trait_names):\n",
        "        ax = axes[i]\n",
        "\n",
        "        pred = predictions[:, i]\n",
        "        true = labels[:, i]\n",
        "        median = medians[i]\n",
        "\n",
        "        # Binarize by median\n",
        "        pred_binary = (pred > median).astype(int)\n",
        "        true_binary = (true > median).astype(int)\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        cm = confusion_matrix(true_binary, pred_binary)\n",
        "\n",
        "        # Plot heatmap\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                    xticklabels=['Low', 'High'],\n",
        "                    yticklabels=['Low', 'High'])\n",
        "        ax.set_xlabel('Predicted')\n",
        "        ax.set_ylabel('True')\n",
        "        ax.set_title(f'{trait} - Confusion Matrix')\n",
        "\n",
        "    # Hide unused subplot\n",
        "    axes[-1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{CHECKPOINT_DIR}/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrices\n",
        "plot_confusion_matrices(test_predictions, test_labels)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}