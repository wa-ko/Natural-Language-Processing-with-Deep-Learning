{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "# Big Five Personality Recognition using LUKE on RealPersonaChat\n\n**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦:**\n- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: RealPersonaChat (14,000å¯¾è©±ã€233è©±è€…)\n- ã‚¿ã‚¹ã‚¯: Big Fiveæ€§æ ¼ç‰¹æ€§ã®å›å¸°äºˆæ¸¬\n- ãƒ¢ãƒ‡ãƒ«: LUKE (studio-ousia/luke-japanese-base)\n- è¨­å®š: ãƒ¢ãƒãƒ­ãƒ¼ã‚°ï¼ˆè©±è€…ã®ç™ºè©±ã®ã¿ï¼‰\n\n**è©•ä¾¡æŒ‡æ¨™:**\n- å›å¸°: MAE, RMSE, Pearson, Spearmanç›¸é–¢\n- åˆ†é¡: Accuracy, Balanced Accuracy, Precision, Recall, F1\n\n**ãƒ¡ãƒ¢ãƒªæœ€é©åŒ– (T4 GPUå¯¾å¿œ):**\n- ãƒãƒƒãƒã‚µã‚¤ã‚º: 4\n- Max Length: 256 tokens\n- å‹¾é…è“„ç©: 8ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆå®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚º32ç›¸å½“ï¼‰\n- å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: æœ‰åŠ¹åŒ–\n- Mixed Precision (FP16): æœ‰åŠ¹åŒ–\n\n**æ¨å¥¨ç’°å¢ƒ:**\n- Google Colab with T4 GPU (15GB VRAM)\n- ãƒ©ãƒ³ã‚¿ã‚¤ãƒ å†èµ·å‹•å¾Œã«å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æ¨å¥¨"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRfHNeAe2eB1"
   },
   "source": [
    "# Big Five Personality Recognition using LUKE on RealPersonaChat\n",
    "\n",
    "**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦:**\n",
    "- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: RealPersonaChat (14,000å¯¾è©±ã€233è©±è€…)\n",
    "- ã‚¿ã‚¹ã‚¯: Big Fiveæ€§æ ¼ç‰¹æ€§ã®å›å¸°äºˆæ¸¬\n",
    "- ãƒ¢ãƒ‡ãƒ«: LUKE (studio-ousia/luke-japanese-base)\n",
    "- è¨­å®š: ãƒ¢ãƒãƒ­ãƒ¼ã‚°ï¼ˆè©±è€…ã®ç™ºè©±ã®ã¿ï¼‰\n",
    "\n",
    "**è©•ä¾¡æŒ‡æ¨™:**\n",
    "- å›å¸°: MAE, RMSE, Pearson, Spearmanç›¸é–¢\n",
    "- åˆ†é¡: Accuracy, Balanced Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpQBruNV2eB1"
   },
   "source": [
    "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1LJenW-O2eB2"
   },
   "outputs": [],
   "source": [
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install -q transformers datasets torch scikit-learn pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AIUAvxtp2eB2",
    "outputId": "f6b94ff4-c49a-4937-9e2c-ef3221b5d1a6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n",
      "GPU: Tesla T4\n",
      "VRAM: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW  # transformersã§ã¯ãªãtorch.optimã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from transformers import (\n",
    "    AutoTokenizer,  # LukeTokenizerã®ä»£ã‚ã‚Šã«AutoTokenizerã‚’ä½¿ç”¨\n",
    "    LukeModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "JwKt8CD32eB2",
    "outputId": "b4f1a327-0a8a-4f72-bcc7-008c2a47aff9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "## 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n\nRealPersonaChatãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’GitHubã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿ã¾ã™ã€‚\n- è©±è€…ãƒ‡ãƒ¼ã‚¿: `interlocutors.json` (233å)\n- å¯¾è©±ãƒ‡ãƒ¼ã‚¿: `dialogues/*.json` (æœ€å¤§14,000ä»¶)\n\n**æ³¨æ„**: å…¨14,000ä»¶ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚\n- ãƒ†ã‚¹ãƒˆç”¨: `num_dialogues = 1000` (ç´„2-3åˆ†)\n- æœ¬ç•ªç”¨: `num_dialogues = 14000` (ç´„30-40åˆ†)\n\n**Google Driveã‚­ãƒ£ãƒƒã‚·ãƒ¥:**\n- åˆå›: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ + ä¿å­˜ï¼ˆ2-3åˆ†ï¼‰\n- 2å›ç›®ä»¥é™: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆ5ç§’ï¼‰"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BZTbgSL2eB2"
   },
   "source": "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ï¼ˆGitHubã‹ã‚‰ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ + Google Drive ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰\nimport json\nimport requests\nfrom pathlib import Path\nimport pickle\n\nprint(\"RealPersonaChatãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­...\")\n\n# Google Driveä¸Šã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\nCACHE_DIR = Path(\"/content/drive/MyDrive/real_persona_chat_cache\")\nCACHE_DIR.mkdir(exist_ok=True)\n\nINTERLOCUTORS_CACHE = CACHE_DIR / \"interlocutors.pkl\"\nDIALOGUES_CACHE = CACHE_DIR / \"dialogues_1000.pkl\"\n\n# GitHubã®rawãƒ•ã‚¡ã‚¤ãƒ«URL\nBASE_URL = \"https://raw.githubusercontent.com/nu-dialogue/real-persona-chat/main/real_persona_chat\"\n\n# 1. Interlocutors (è©±è€…ãƒ‡ãƒ¼ã‚¿) ã‚’èª­ã¿è¾¼ã¿\nprint(\"\\n\" + \"=\"*70)\nprint(\"è©±è€…ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\nprint(\"=\"*70)\n\nif INTERLOCUTORS_CACHE.exists():\n    print(\"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆé«˜é€Ÿï¼‰\")\n    with open(INTERLOCUTORS_CACHE, 'rb') as f:\n        interlocutor_dict = pickle.load(f)\n    print(f\"âœ… è©±è€…ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(interlocutor_dict)} speakers\")\nelse:\n    print(\"ğŸ“¥ GitHubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n    interlocutors_url = f\"{BASE_URL}/interlocutors.json\"\n    response = requests.get(interlocutors_url)\n    interlocutors_raw = response.json()\n    \n    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’å¤‰æ›\n    if isinstance(interlocutors_raw, dict):\n        interlocutor_dict = interlocutors_raw\n    elif isinstance(interlocutors_raw, list):\n        interlocutor_dict = {\n            item['interlocutor_id']: item\n            for item in interlocutors_raw\n        }\n    else:\n        print(f\"âš ï¸  äºˆæœŸã—ãªã„ãƒ‡ãƒ¼ã‚¿å‹: {type(interlocutors_raw)}\")\n        interlocutor_dict = {}\n    \n    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜\n    with open(INTERLOCUTORS_CACHE, 'wb') as f:\n        pickle.dump(interlocutor_dict, f)\n    print(f\"âœ… è©±è€…ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {INTERLOCUTORS_CACHE}\")\n    print(f\"âœ… è©±è€…ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(interlocutor_dict)} speakers\")\n\n# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º\nif interlocutor_dict:\n    first_speaker_id = list(interlocutor_dict.keys())[0]\n    sample_speaker = interlocutor_dict[first_speaker_id]\n    print(f\"\\nğŸ“Š è©±è€…ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆè©±è€…ID: {first_speaker_id}ï¼‰:\")\n    print(f\"  åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ¼: {list(sample_speaker.keys())}\")\n    if 'personality' in sample_speaker:\n        print(f\"  Personality keys: {list(sample_speaker['personality'].keys())[:10]}\")\n\n# 2. Dialogue ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\nprint(\"\\n\" + \"=\"*70)\nprint(\"å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\nprint(\"=\"*70)\n\nnum_dialogues = 1000  # å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼ˆæœ€å¤§14000ï¼‰\n\nif DIALOGUES_CACHE.exists():\n    print(f\"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆé«˜é€Ÿï¼‰ - {num_dialogues}ä»¶\")\n    with open(DIALOGUES_CACHE, 'rb') as f:\n        dialogue_data = pickle.load(f)\n    print(f\"âœ… å¯¾è©±ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(dialogue_data)} dialogues\")\nelse:\n    print(f\"ğŸ“¥ GitHubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­... ({num_dialogues}ä»¶)\")\n    print(\"ï¼ˆæ³¨ï¼šåˆå›ã®ã¿æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚æ¬¡å›ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰é«˜é€Ÿèª­ã¿è¾¼ã¿ï¼‰\")\n    \n    dialogue_data = []\n    failed_downloads = 0\n    \n    for i in tqdm(range(1, num_dialogues + 1), desc=\"Downloading dialogues\"):\n        dialogue_id = f\"{i:05d}\"\n        dialogue_url = f\"{BASE_URL}/dialogues/{dialogue_id}.json\"\n        \n        try:\n            response = requests.get(dialogue_url)\n            if response.status_code == 200:\n                dialogue = response.json()\n                dialogue_data.append(dialogue)\n            else:\n                failed_downloads += 1\n        except Exception as e:\n            failed_downloads += 1\n            if i <= 10:\n                print(f\"Failed to download {dialogue_id}: {e}\")\n    \n    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜\n    with open(DIALOGUES_CACHE, 'wb') as f:\n        pickle.dump(dialogue_data, f)\n    print(f\"\\nâœ… å¯¾è©±ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {DIALOGUES_CACHE}\")\n    print(f\"âœ… å¯¾è©±ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(dialogue_data)} dialogues\")\n    if failed_downloads > 0:\n        print(f\"âš ï¸  ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {failed_downloads} dialogues\")\n\n# ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç¢ºèª\nprint(\"\\nğŸ“Š å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®1ä»¶ï¼‰:\")\nif dialogue_data:\n    sample = dialogue_data[0]\n    print(f\"  åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ¼: {list(sample.keys())}\")\n    print(f\"  Dialogue ID: {sample.get('dialogue_id', 'N/A')}\")\n    print(f\"  Interlocutors: {sample.get('interlocutors', [])}\")\n    print(f\"  Utterances: {len(sample.get('utterances', []))} utterances\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ’¡ æ¬¡å›ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰é«˜é€Ÿèª­ã¿è¾¼ã¿ã—ã¾ã™ï¼ˆæ•°ç§’ï¼‰\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h_OvgO762eB2",
    "outputId": "31a422a8-5023-4a59-ebd9-fe722d76571d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587,
     "referenced_widgets": [
      "1107a7828b424109a8c5c7ee2e77faef",
      "2296fed9169d411aabd62fcffab4466c",
      "93eac4bb42ff4ee482d028e6fa0f417f",
      "e947ae36b3f14175aa0952bd2f039f64",
      "e0ae76aaf47c4ab7ace78ed60524034f",
      "ac09ab32cbe84a6bbaad09bcf8d8ad23",
      "c0eaade6838d45a88dee538850151612",
      "3e9603e25b384a0bbbed30f50e394f61",
      "406a47b6a089446abadd4b6b0145287b",
      "b8658729343443d09f13a925a3301f6f",
      "b67e8146de724b9e976f68b5f5fb8a41"
     ]
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GitHubã‹ã‚‰RealPersonaChatãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­...\n",
      "\n",
      "è©±è€…ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\n",
      "\n",
      "ğŸ“Š è©±è€…ãƒ‡ãƒ¼ã‚¿ã®å‹: <class 'dict'>\n",
      "ğŸ“Š è¾æ›¸ã®ã‚­ãƒ¼: ['AH', 'HP', 'CX', 'GS', 'CF']\n",
      "ğŸ“Š æœ€åˆã®è¦ç´ ã®å‹: <class 'dict'>\n",
      "\n",
      "âœ… è©±è€…ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: 233 speakers\n",
      "\n",
      "ğŸ“Š è©±è€…ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆè©±è€…ID: AHï¼‰:\n",
      "  åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ¼: ['interlocutor_id', 'persona', 'personality', 'demographic_information', 'text_chat_experience']\n",
      "  Personality keys: ['BigFive_Openness', 'BigFive_Conscientiousness', 'BigFive_Extraversion', 'BigFive_Agreeableness', 'BigFive_Neuroticism', 'KiSS18_BasicSkill', 'KiSS18_AdvancedSkill', 'KiSS18_EmotionalManagementSkill', 'KiSS18_OffenceManagementSkill', 'KiSS18_StressManagementSkill']\n",
      "\n",
      "======================================================================\n",
      "å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\n",
      "ï¼ˆæ³¨ï¼šå…¨14,000ä»¶ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰\n",
      "ãƒ†ã‚¹ãƒˆç”¨ã«æœ€åˆã®1,000ä»¶ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading dialogues:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1107a7828b424109a8c5c7ee2e77faef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "âœ… å¯¾è©±ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: 972 dialogues\n",
      "âš ï¸  ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: 28 dialogues\n",
      "\n",
      "ğŸ“Š å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®1ä»¶ï¼‰:\n",
      "  åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ¼: ['dialogue_id', 'interlocutors', 'utterances', 'evaluations']\n",
      "  Dialogue ID: 1\n",
      "  Interlocutors: ['AA', 'AB']\n",
      "  Utterances: 30 utterances\n"
     ]
    }
   ],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ï¼ˆGitHubã‹ã‚‰ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"GitHubã‹ã‚‰RealPersonaChatãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­...\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "data_dir = Path(\"/content/real_persona_chat_data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# GitHubã®rawãƒ•ã‚¡ã‚¤ãƒ«URL\n",
    "BASE_URL = \"https://raw.githubusercontent.com/nu-dialogue/real-persona-chat/main/real_persona_chat\"\n",
    "\n",
    "# 1. Interlocutors (è©±è€…ãƒ‡ãƒ¼ã‚¿) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "print(\"\\nè©±è€…ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "interlocutors_url = f\"{BASE_URL}/interlocutors.json\"\n",
    "response = requests.get(interlocutors_url)\n",
    "interlocutors_raw = response.json()\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç¢ºèª\n",
    "print(f\"\\nğŸ“Š è©±è€…ãƒ‡ãƒ¼ã‚¿ã®å‹: {type(interlocutors_raw)}\")\n",
    "if isinstance(interlocutors_raw, dict):\n",
    "    print(f\"ğŸ“Š è¾æ›¸ã®ã‚­ãƒ¼: {list(interlocutors_raw.keys())[:5]}\")  # æœ€åˆã®5ã¤ã®ã‚­ãƒ¼ã‚’è¡¨ç¤º\n",
    "    # è¾æ›¸æ§‹é€ ã®å ´åˆã€å€¤ãŒãƒªã‚¹ãƒˆã‹ç¢ºèª\n",
    "    first_key = list(interlocutors_raw.keys())[0]\n",
    "    print(f\"ğŸ“Š æœ€åˆã®è¦ç´ ã®å‹: {type(interlocutors_raw[first_key])}\")\n",
    "\n",
    "    # è¾æ›¸æ§‹é€ ã‚’å¤‰æ›\n",
    "    if isinstance(interlocutors_raw[first_key], dict):\n",
    "        # {speaker_id: {data}} å½¢å¼ã®å ´åˆ\n",
    "        interlocutor_dict = interlocutors_raw\n",
    "        print(f\"\\nâœ… è©±è€…ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(interlocutor_dict)} speakers\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  äºˆæœŸã—ãªã„ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã§ã™\")\n",
    "        interlocutor_dict = {}\n",
    "elif isinstance(interlocutors_raw, list):\n",
    "    print(f\"ğŸ“Š ãƒªã‚¹ãƒˆã®é•·ã•: {len(interlocutors_raw)}\")\n",
    "    # ãƒªã‚¹ãƒˆæ§‹é€ ã®å ´åˆã€è¾æ›¸ã«å¤‰æ›\n",
    "    interlocutor_dict = {\n",
    "        item['interlocutor_id']: item\n",
    "        for item in interlocutors_raw\n",
    "    }\n",
    "    print(f\"\\nâœ… è©±è€…ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(interlocutor_dict)} speakers\")\n",
    "else:\n",
    "    print(f\"âš ï¸  äºˆæœŸã—ãªã„ãƒ‡ãƒ¼ã‚¿å‹: {type(interlocutors_raw)}\")\n",
    "    interlocutor_dict = {}\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º\n",
    "if interlocutor_dict:\n",
    "    first_speaker_id = list(interlocutor_dict.keys())[0]\n",
    "    sample_speaker = interlocutor_dict[first_speaker_id]\n",
    "    print(f\"\\nğŸ“Š è©±è€…ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆè©±è€…ID: {first_speaker_id}ï¼‰:\")\n",
    "    print(f\"  åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ¼: {list(sample_speaker.keys())}\")\n",
    "    if 'personality' in sample_speaker:\n",
    "        print(f\"  Personality keys: {list(sample_speaker['personality'].keys())[:10]}\")  # æœ€åˆã®10å€‹\n",
    "\n",
    "# 2. Dialogue ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "print(\"ï¼ˆæ³¨ï¼šå…¨14,000ä»¶ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰\")\n",
    "print(\"ãƒ†ã‚¹ãƒˆç”¨ã«æœ€åˆã®1,000ä»¶ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "dialogue_data = []\n",
    "num_dialogues = 1000  # å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼ˆæœ€å¤§14000ï¼‰\n",
    "failed_downloads = 0\n",
    "\n",
    "for i in tqdm(range(1, num_dialogues + 1), desc=\"Downloading dialogues\"):\n",
    "    dialogue_id = f\"{i:05d}\"  # 00001å½¢å¼\n",
    "    dialogue_url = f\"{BASE_URL}/dialogues/{dialogue_id}.json\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(dialogue_url)\n",
    "        if response.status_code == 200:\n",
    "            dialogue = response.json()\n",
    "            dialogue_data.append(dialogue)\n",
    "        else:\n",
    "            failed_downloads += 1\n",
    "    except Exception as e:\n",
    "        failed_downloads += 1\n",
    "        if i <= 10:  # æœ€åˆã®10ä»¶ã®ã‚¨ãƒ©ãƒ¼ã ã‘è¡¨ç¤º\n",
    "            print(f\"Failed to download {dialogue_id}: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… å¯¾è©±ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(dialogue_data)} dialogues\")\n",
    "if failed_downloads > 0:\n",
    "    print(f\"âš ï¸  ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {failed_downloads} dialogues\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç¢ºèª\n",
    "print(\"\\nğŸ“Š å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®1ä»¶ï¼‰:\")\n",
    "if dialogue_data:\n",
    "    sample = dialogue_data[0]\n",
    "    print(f\"  åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ¼: {list(sample.keys())}\")\n",
    "    print(f\"  Dialogue ID: {sample.get('dialogue_id', 'N/A')}\")\n",
    "    print(f\"  Interlocutors: {sample.get('interlocutors', [])}\")\n",
    "    print(f\"  Utterances: {len(sample.get('utterances', []))} utterances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "P71557N92eB2",
    "outputId": "09412397-e1a7-459c-a34d-082c7d5a2c3f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230,
     "referenced_widgets": [
      "19d48b23355a498d84433523db78564d",
      "a24c1eeff9914e55b645bfd02f8b1882",
      "24a69d6f930d4dba96abc3b4a633b9a7",
      "7a1ccabba91646a4824292d21501495d",
      "294248cc6392435a8f457e8b59007ec1",
      "9ce220cf57e5447d931e4f5f59d53c2f",
      "f107e032aed444dd94371182a042bab2",
      "274827f228ff4a3390e8e51f80be71d0",
      "6d3c1038e6bd4c2d82afb3926b254cd9",
      "1405869b76754b2a8591149402ba197a",
      "aedcbccd31d54613bef83a610ec883f0"
     ]
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "ãƒ¢ãƒãƒ­ãƒ¼ã‚°æŠ½å‡ºä¸­...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Processing dialogues:   0%|          | 0/972 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19d48b23355a498d84433523db78564d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "âœ… ãƒ¢ãƒãƒ­ãƒ¼ã‚°ã‚µãƒ³ãƒ—ãƒ«æ•°: 1944\n",
      "\n",
      "ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ä¾‹:\n",
      "  è©±è€…ID: AA\n",
      "  ãƒ†ã‚­ã‚¹ãƒˆé•·: 248 æ–‡å­—\n",
      "  Big Five: {'Openness': 4.25, 'Conscientiousness': 3.5, 'Extraversion': 2.9, 'Agreeableness': 5.083333333333333, 'Neuroticism': 4.416666666666667}\n"
     ]
    }
   ],
   "source": [
    "# ãƒ¢ãƒãƒ­ãƒ¼ã‚°ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ\n",
    "# å„è©±è€…ã®ç™ºè©±ã®ã¿ã‚’æŠ½å‡ºã—ã¦ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆ\n",
    "\n",
    "def create_monologue_dataset(dialogue_data, interlocutor_dict):\n",
    "    \"\"\"\n",
    "    å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„è©±è€…ã®ãƒ¢ãƒãƒ­ãƒ¼ã‚°ã‚’æŠ½å‡º\n",
    "\n",
    "    Args:\n",
    "        dialogue_data: List of dialogue dictionaries\n",
    "        interlocutor_dict: è©±è€…ID -> è©±è€…æƒ…å ±ã®è¾æ›¸\n",
    "\n",
    "    Returns:\n",
    "        monologues: List[Dict]\n",
    "            - 'speaker_id': str\n",
    "            - 'text': str (å…¨ç™ºè©±ã‚’é€£çµ)\n",
    "            - 'personality': Dict[str, float] (Big Five scores)\n",
    "    \"\"\"\n",
    "    monologues = []\n",
    "\n",
    "    print(\"\\nãƒ¢ãƒãƒ­ãƒ¼ã‚°æŠ½å‡ºä¸­...\")\n",
    "\n",
    "    for dialogue in tqdm(dialogue_data, desc=\"Processing dialogues\"):\n",
    "        # å„è©±è€…ã®ç™ºè©±ã‚’åé›†\n",
    "        speaker_utterances = {}\n",
    "\n",
    "        for utterance in dialogue.get('utterances', []):\n",
    "            speaker_id = utterance.get('interlocutor_id')\n",
    "            text = utterance.get('text', '')\n",
    "\n",
    "            if not speaker_id or not text:\n",
    "                continue\n",
    "\n",
    "            if speaker_id not in speaker_utterances:\n",
    "                speaker_utterances[speaker_id] = []\n",
    "            speaker_utterances[speaker_id].append(text)\n",
    "\n",
    "        # å„è©±è€…ã®ãƒ¢ãƒãƒ­ãƒ¼ã‚°ã‚’ä½œæˆ\n",
    "        for speaker_id, utterances in speaker_utterances.items():\n",
    "            # è©±è€…æƒ…å ±ã‚’å–å¾—\n",
    "            if speaker_id not in interlocutor_dict:\n",
    "                continue  # è©±è€…æƒ…å ±ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "\n",
    "            speaker_info = interlocutor_dict[speaker_id]\n",
    "\n",
    "            # Big Five ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º\n",
    "            personality_data = speaker_info.get('personality', {})\n",
    "\n",
    "            # Big Fiveç‰¹æ€§ã®ã‚¹ã‚³ã‚¢ã‚’å–å¾—ï¼ˆ1-7ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰\n",
    "            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç¢ºèªã—ã¦é©åˆ‡ãªã‚­ãƒ¼åã‚’ä½¿ç”¨\n",
    "            big_five_scores = {\n",
    "                'Openness': personality_data.get('big_five_openness', personality_data.get('openness', 4.0)),\n",
    "                'Conscientiousness': personality_data.get('big_five_conscientiousness', personality_data.get('conscientiousness', 4.0)),\n",
    "                'Extraversion': personality_data.get('big_five_extraversion', personality_data.get('extraversion', 4.0)),\n",
    "                'Agreeableness': personality_data.get('big_five_agreeableness', personality_data.get('agreeableness', 4.0)),\n",
    "                'Neuroticism': personality_data.get('big_five_neuroticism', personality_data.get('neuroticism', 4.0)),\n",
    "            }\n",
    "\n",
    "            # ã‚¹ã‚³ã‚¢ãŒå–å¾—ã§ãã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "            if all(score == 4.0 for score in big_five_scores.values()):\n",
    "                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®ã¾ã¾ã®å ´åˆã¯ä»–ã®ã‚­ãƒ¼åã‚’è©¦ã™\n",
    "                for key, value in personality_data.items():\n",
    "                    if 'openness' in key.lower():\n",
    "                        big_five_scores['Openness'] = value\n",
    "                    elif 'conscientiousness' in key.lower():\n",
    "                        big_five_scores['Conscientiousness'] = value\n",
    "                    elif 'extraversion' in key.lower():\n",
    "                        big_five_scores['Extraversion'] = value\n",
    "                    elif 'agreeableness' in key.lower():\n",
    "                        big_five_scores['Agreeableness'] = value\n",
    "                    elif 'neuroticism' in key.lower():\n",
    "                        big_five_scores['Neuroticism'] = value\n",
    "\n",
    "            monologues.append({\n",
    "                'speaker_id': speaker_id,\n",
    "                'text': ' '.join(utterances),\n",
    "                'personality': big_five_scores\n",
    "            })\n",
    "\n",
    "    return monologues\n",
    "\n",
    "# ãƒ¢ãƒãƒ­ãƒ¼ã‚°ãƒ‡ãƒ¼ã‚¿ä½œæˆ\n",
    "monologue_data = create_monologue_dataset(dialogue_data, interlocutor_dict)\n",
    "\n",
    "print(f\"\\nâœ… ãƒ¢ãƒãƒ­ãƒ¼ã‚°ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(monologue_data)}\")\n",
    "if monologue_data:\n",
    "    print(f\"\\nğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ä¾‹:\")\n",
    "    print(f\"  è©±è€…ID: {monologue_data[0]['speaker_id']}\")\n",
    "    print(f\"  ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(monologue_data[0]['text'])} æ–‡å­—\")\n",
    "    print(f\"  Big Five: {monologue_data[0]['personality']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ka9sYMGW2eB2",
    "outputId": "847a76ce-9f3c-476f-bba5-f08768e71c5d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²çµæœ:\n",
      "  Train: 56 speakers, 1482 samples\n",
      "  Val:   7 speakers, 143 samples\n",
      "  Test:  7 speakers, 319 samples\n",
      "\n",
      "âœ… ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†\n"
     ]
    }
   ],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆè©±è€…ãƒ™ãƒ¼ã‚¹ï¼‰\n",
    "# è«–æ–‡æº–æ‹ : Train/Val/Test = 8:1:1, è©±è€…ãŒé‡è¤‡ã—ãªã„ã‚ˆã†ã«åˆ†å‰²\n",
    "\n",
    "def split_by_speaker(monologue_data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    è©±è€…ãŒé‡è¤‡ã—ãªã„ã‚ˆã†ã«ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n",
    "    \"\"\"\n",
    "    # è©±è€…IDã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n",
    "    speaker_groups = {}\n",
    "    for sample in monologue_data:\n",
    "        speaker_id = sample['speaker_id']\n",
    "        if speaker_id not in speaker_groups:\n",
    "            speaker_groups[speaker_id] = []\n",
    "        speaker_groups[speaker_id].append(sample)\n",
    "\n",
    "    # è©±è€…ãƒªã‚¹ãƒˆ\n",
    "    speakers = list(speaker_groups.keys())\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(speakers)\n",
    "\n",
    "    # åˆ†å‰²\n",
    "    n_speakers = len(speakers)\n",
    "    n_train = int(n_speakers * train_ratio)\n",
    "    n_val = int(n_speakers * val_ratio)\n",
    "\n",
    "    train_speakers = speakers[:n_train]\n",
    "    val_speakers = speakers[n_train:n_train+n_val]\n",
    "    test_speakers = speakers[n_train+n_val:]\n",
    "\n",
    "    # ã‚µãƒ³ãƒ—ãƒ«æŠ½å‡º\n",
    "    train_data = [s for spk in train_speakers for s in speaker_groups[spk]]\n",
    "    val_data = [s for spk in val_speakers for s in speaker_groups[spk]]\n",
    "    test_data = [s for spk in test_speakers for s in speaker_groups[spk]]\n",
    "\n",
    "    print(f\"\\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²çµæœ:\")\n",
    "    print(f\"  Train: {len(train_speakers)} speakers, {len(train_data)} samples\")\n",
    "    print(f\"  Val:   {len(val_speakers)} speakers, {len(val_data)} samples\")\n",
    "    print(f\"  Test:  {len(test_speakers)} speakers, {len(test_data)} samples\")\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Ÿè¡Œ\n",
    "train_data, val_data, test_data = split_by_speaker(monologue_data)\n",
    "\n",
    "print(f\"\\nâœ… ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GV566u7_2eB2"
   },
   "source": [
    "## 3. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†\n",
    "\n",
    "- LUKEãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "- Big Fiveã‚¹ã‚³ã‚¢ã‚’æ­£è¦åŒ–ï¼ˆ1-7 â†’ 0-1ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJX4RnZ-2eB2",
    "outputId": "a8674c97-c2fe-4ca5-f9b9-8a03ca63a2ab",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245,
     "referenced_widgets": [
      "75a35ded60e14c808d30e1aa34deaf21",
      "37d8c688def642fd82d42cc2d9a5596a",
      "3cc59d8c4a8f4e5196d7a50e7b2bc7a8",
      "2b1e1e0a40534d8f9e8da461b9cd553c",
      "5de5e2385c274dcd8cd5db3dad22ddbc",
      "cfd48a54d7e84e3097a4addc1b60a26b",
      "1197f1dbde094b7ca5ed306563df6a54",
      "7f66d87c60d74ad68c2b5735fca22a03",
      "d48297d248ce4ef49a970a48befa4160",
      "ce2206154e124bb9be4f1cf62d302b98",
      "a858b2b2667449bc9aa7424ec193e72c",
      "ac2fc283071b403197529c05b3abaf5b",
      "f7b585d985d443038124bd4d1304e0ed",
      "34b18acc56c74d3e889bd9acb90a231e",
      "8269f7e4cb2f4b3280f13576cc8a9067",
      "2d50ea127381455e8d7087d286ed3d20",
      "f66af1fbccb04908a56cd7368511984e",
      "f71b84d7ed6c4c2fbeee3589d6d7bc0b",
      "5e269279a2984893abef447bf7fb83aa",
      "4ddea5a3fc844e0b83a19066bd864497",
      "5d162464fa974a478f1905f85afa8fdb",
      "009698e6dd0449088018ec89e7e860ea",
      "e5c8dba9d5e24866aa24d306eb24c5c7",
      "4ea4e265c1614d03b5607ecd5bee5238",
      "ac7efa2870894d589ef40439be24cefb",
      "70621441faec49269faeb0bce389030c",
      "05c81806aeb04c22b37fadb32eb39f81",
      "6c841fa9eecc45ccb37be28ad1d7d9f7",
      "fefd716796ac4f7aa7d7811d4bd1b143",
      "0a8d9203c073495481ab2563165922b0",
      "6ead8437c0ed4a7e8b58d88441a30c06",
      "7c75e7cee224402eb9264a5d6d2fece8",
      "a51bd7ef950e45c0a2ec74f85a9acba3",
      "1b900fe53a904f40941269536ae34cd9",
      "9102faf79ad4469298373707c36f043e",
      "b2b62472255c4115bfc275d120643268",
      "d10c2d2f4d1746dfb4f6d5432f41aad3",
      "7e76ff6433c14cf2b5791e7368fe1374",
      "3467d3c29e92473794ab7c649ac9dc8c",
      "626973975b8e4ae5a7c58fdadc027fc7",
      "61f3cac7ddab4397be0fe7cd61a6b2a8",
      "56699d9d97c44325884844a005214fef",
      "23a06c0f189e4fa79b21f3f1df444494",
      "a163ee10499b46dd8e74ec8b99b78ca4",
      "2793fe8dc84d48ff933b06d8a3a420c5",
      "d064a00f082946d7a309ef9b78a8a539",
      "973ce485786749b08aea4c957558a605",
      "36dc5a931b4645b29d7630a010d025af",
      "451947b6e980488f8a251cd719e5e274",
      "2e260159c16f483da0362e699d130970",
      "ff7a609e77e34585bd9545b6e01454d4",
      "5293ef9264f1448db087d651a50ef922",
      "4d82e06da71447d5b343c5067ff623f5",
      "346659e76a69418e8eadb1bdf2468847",
      "5384f1c07a46425aa2848ebb2515e1df",
      "0d99fc2b221446939b7f1a7188e9753c",
      "8479d3087bf0402b9a59fec1021007ca",
      "c385f952ee9a4f57b3a9f35a093761e0",
      "5bcf7c04c7864726a17fd259f5b54e3a",
      "a2104007458b4f18ae8c5bef8cd4446f",
      "de9866967c574a068327cbce2215da65",
      "31615ffb4b574603970f840a090e9a38",
      "a129819275474f2fbc41d44922acff2f",
      "de9e6efb43594e9ba99bb5def2165ecc",
      "103ed724b09c4d09a62fb4b892c12692",
      "fa00c020c3f944f0bcb0f602d319be46"
     ]
    }
   },
   "outputs": [],
   "source": "# PyTorch Dataset ã‚¯ãƒ©ã‚¹\nclass PersonalityDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=256):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        \n        # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³\n        encoding = self.tokenizer(\n            sample['text'],\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        # Big Five ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–ï¼ˆ1-7 â†’ 0-1ï¼‰\n        personality_scores = torch.tensor([\n            (sample['personality'][trait] - 1) / 6  # 1-7 ã‚’ 0-1 ã«æ­£è¦åŒ–\n            for trait in BIG_FIVE_TRAITS\n        ], dtype=torch.float32)\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'labels': personality_scores\n        }\n\n# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ\nBATCH_SIZE = 4  # 8 â†’ 4ã«å¤‰æ›´ï¼ˆã•ã‚‰ã«ãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰\n\ntrain_dataset = PersonalityDataset(train_data, tokenizer)\nval_dataset = PersonalityDataset(val_data, tokenizer)\ntest_dataset = PersonalityDataset(test_data, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\nprint(f\"\\nâœ… Datasetæº–å‚™å®Œäº†\")\nprint(f\"  Max Length: 256 tokens (ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–)\")\nprint(f\"  Batch Size: {BATCH_SIZE} (æ¥µå°ãƒ¡ãƒ¢ãƒªãƒ¢ãƒ¼ãƒ‰)\")\nprint(f\"  Train batches: {len(train_loader)}\")\nprint(f\"  Val batches: {len(val_loader)}\")\nprint(f\"  Test batches: {len(test_loader)}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "9K06k2jZ2eB3",
    "outputId": "c6963d1e-a16b-4257-a0a6-b4818bb81a6f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "## 4. ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰\n\nLUKE + 5ã¤ã®å›å¸°ãƒ˜ãƒƒãƒ‰ï¼ˆå„Big Fiveç‰¹æ€§ã«1ã¤ï¼‰\n\n**æ³¨æ„**: ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰å‰ã«GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vOr3Le32eB3"
   },
   "source": "# GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰å‰ï¼‰\nimport gc\ntorch.cuda.empty_cache()\ngc.collect()\nprint(\"âœ… GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ\")\n\n# LUKE Personality Recognition Model\nclass LukePersonalityModel(nn.Module):\n    def __init__(self, model_name=MODEL_NAME, num_traits=5):\n        super().__init__()\n        self.luke = LukeModel.from_pretrained(model_name)\n        \n        # å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æœ‰åŠ¹åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰\n        self.luke.gradient_checkpointing_enable()\n        \n        self.hidden_size = self.luke.config.hidden_size\n        \n        # å„Big Fiveç‰¹æ€§ã«å¯¾ã™ã‚‹å›å¸°ãƒ˜ãƒƒãƒ‰\n        self.regression_heads = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(self.hidden_size, 256),\n                nn.ReLU(),\n                nn.Dropout(0.1),\n                nn.Linear(256, 1)\n            )\n            for _ in range(num_traits)\n        ])\n    \n    def forward(self, input_ids, attention_mask):\n        # LUKE encoding\n        outputs = self.luke(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        # [CLS] token ã® hidden state\n        pooled_output = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n        \n        # å„ç‰¹æ€§ã®äºˆæ¸¬\n        predictions = []\n        for head in self.regression_heads:\n            pred = head(pooled_output)  # [batch_size, 1]\n            predictions.append(pred)\n        \n        # [batch_size, 5]\n        predictions = torch.cat(predictions, dim=1)\n        \n        # 0-1 ã®ç¯„å›²ã«åˆ¶é™ï¼ˆSigmoidï¼‰\n        predictions = torch.sigmoid(predictions)\n        \n        return predictions\n\n# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–\nprint(\"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...\")\nmodel = LukePersonalityModel().to(device)\n\nprint(f\"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\nprint(f\"âœ… å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: æœ‰åŠ¹åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰\")\n\n# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç¢ºèª\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated(0) / 1e9\n    reserved = torch.cuda.memory_reserved(0) / 1e9\n    print(f\"âœ… GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "G8IieDwU2eB3",
    "outputId": "3e24a1a0-eaf9-48e0-cb7b-7d8025519ba1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131,
     "referenced_widgets": [
      "05a2945690a44c70814762d3f577e633",
      "69bb444338084752897b868e8ed80595",
      "efd2d4f9b0424deab6db01bb4ad18acb",
      "711c809e04eb4d12a7b6636ffd9b8809",
      "0cd0ecffea5046c6a733660d7cd013e2",
      "d7118175d0154e9e8496eac3a7b403f3",
      "802de0008e3f4ef4be35841f3a02aaef",
      "3d84cea03d2b4f11aa8e3ea8e27e196e",
      "06be7c6c8d3a4676ab661b138ce8058f",
      "326ce7eb4c0648f39ecb6dde4882dd79",
      "3e409493448b4212bf98f78be8f269f3",
      "94d6934561cd4536a8485d0db6053239",
      "3f911538733c48c399a40ec089723132",
      "b2012536d22c4e9ea5e2ace1b5ff0681",
      "ff585c08b2d14e7889a3364de5a1eae4",
      "d54985a179964db3978c7e8ec3d15b03",
      "c3ba19c74b42430791861502a26047a1",
      "906b69502ef447ba9e12ed6759bc1b51",
      "52a4df9161a14f3e872b1c23ac022e24",
      "107a175d56ff40a6973cc59e0b6b0fe3",
      "486bb05e08804f49a3ff2c7370bbd77d",
      "12b7b5a1080a446aa677ed2d4e3215dc",
      "84d93cfe71064b7686d0326ebd3c62bd",
      "af430dd2e77f4e8ba0c0af4380f8b74f",
      "370e361870fd46259179813f14e590d5",
      "5244aec4a46343be846550818aada200",
      "089c8b07bbff418fbfc2f8953b60b39b",
      "5f2c007902cd4916ac6a2d4642a9cbec",
      "d741c5446ed14b25a1c8a75a46ad42d5",
      "6115519883324610931052fe47501348",
      "3972429c71f2445487fa1af3709d9b0e",
      "89e344aa6dfb4d27896153df4efe2e13",
      "80452d9a8d6247bfbb5192b84b2d0bfd"
     ]
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05a2945690a44c70814762d3f577e633"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94d6934561cd4536a8485d0db6053239"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84d93cfe71064b7686d0326ebd3c62bd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 280.10M\n"
     ]
    }
   ],
   "source": [
    "# LUKE Personality Recognition Model\n",
    "class LukePersonalityModel(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME, num_traits=5):\n",
    "        super().__init__()\n",
    "        self.luke = LukeModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.luke.config.hidden_size\n",
    "\n",
    "        # å„Big Fiveç‰¹æ€§ã«å¯¾ã™ã‚‹å›å¸°ãƒ˜ãƒƒãƒ‰\n",
    "        self.regression_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(256, 1)\n",
    "            )\n",
    "            for _ in range(num_traits)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # LUKE encoding\n",
    "        outputs = self.luke(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        # [CLS] token ã® hidden state\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "\n",
    "        # å„ç‰¹æ€§ã®äºˆæ¸¬\n",
    "        predictions = []\n",
    "        for head in self.regression_heads:\n",
    "            pred = head(pooled_output)  # [batch_size, 1]\n",
    "            predictions.append(pred)\n",
    "\n",
    "        # [batch_size, 5]\n",
    "        predictions = torch.cat(predictions, dim=1)\n",
    "\n",
    "        # 0-1 ã®ç¯„å›²ã«åˆ¶é™ï¼ˆSigmoidï¼‰\n",
    "        predictions = torch.sigmoid(predictions)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–\n",
    "model = LukePersonalityModel().to(device)\n",
    "print(f\"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31mAYDid2eB3"
   },
   "source": [
    "## 5. å­¦ç¿’è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eUmOPdck2eB3",
    "outputId": "78ad2972-ae59-4409-a8ee-eaa96dc22130",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "âœ… å­¦ç¿’è¨­å®šå®Œäº†\n",
      "  Learning Rate: 1e-05\n",
      "  Total Steps: 940\n",
      "  Warmup Steps: 150\n"
     ]
    }
   ],
   "source": [
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 20\n",
    "WARMUP_STEPS = 150\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# æå¤±é–¢æ•°ï¼ˆMAEï¼‰\n",
    "criterion = nn.L1Loss()  # Mean Absolute Error\n",
    "\n",
    "print(f\"\\nâœ… å­¦ç¿’è¨­å®šå®Œäº†\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Total Steps: {total_steps}\")\n",
    "print(f\"  Warmup Steps: {WARMUP_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EO2lf4K72eB3",
    "outputId": "ab90abbb-cbdd-40da-b23a-5e442aea60a2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "è©•ä¾¡é–¢æ•°æº–å‚™å®Œäº†\n"
     ]
    }
   ],
   "source": [
    "# è©•ä¾¡é–¢æ•°\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    return all_predictions, all_labels, avg_loss\n",
    "\n",
    "print(\"è©•ä¾¡é–¢æ•°æº–å‚™å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdWKfyCN2eB3"
   },
   "source": "# å­¦ç¿’å±¥æ­´\nhistory = {\n    'train_loss': [],\n    'val_loss': [],\n    'val_mae': []\n}\n\nbest_val_loss = float('inf')\npatience_counter = 0\n\n# å‹¾é…è“„ç©ï¼ˆå®Ÿè³ªçš„ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§ããã—ã¤ã¤ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ï¼‰\nACCUMULATION_STEPS = 8  # 4 * 8 = å®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚º32ç›¸å½“\n\n# Mixed Precision (FP16) ã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒ¢ãƒªå‰Šæ¸›\nfrom torch.cuda.amp import autocast, GradScaler\nscaler = GradScaler()\n\nprint(\"=\"*70)\nprint(\"å­¦ç¿’é–‹å§‹\")\nprint(f\"å®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH_SIZE * ACCUMULATION_STEPS} (å‹¾é…è“„ç©)\")\nprint(f\"Mixed Precision: æœ‰åŠ¹åŒ– (FP16)\")\nprint(\"=\"*70)\n\n# å­¦ç¿’ãƒ«ãƒ¼ãƒ—\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\n{'='*70}\")\n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n    print(f\"{'='*70}\")\n    \n    # Training\n    model.train()\n    train_loss = 0\n    \n    optimizer.zero_grad()  # æœ€åˆã«ã‚¼ãƒ­åŒ–\n    \n    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        # Mixed Precision (FP16) ã§ forward pass\n        with autocast():\n            predictions = model(input_ids, attention_mask)\n            loss = criterion(predictions, labels)\n            loss = loss / ACCUMULATION_STEPS  # å‹¾é…è“„ç©ã®ãŸã‚ã«ã‚¹ã‚±ãƒ¼ãƒ«\n        \n        # Backward pass with scaled gradients\n        scaler.scale(loss).backward()\n        \n        # å‹¾é…è“„ç©ï¼šACCUMULATION_STEPSå›ã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°\n        if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            optimizer.zero_grad()\n        \n        train_loss += loss.item() * ACCUMULATION_STEPS  # å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™\n        \n        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ10ãƒãƒƒãƒã”ã¨ï¼‰\n        if (batch_idx + 1) % 10 == 0:\n            torch.cuda.empty_cache()\n    \n    # æ®‹ã‚Šã®å‹¾é…ã‚’å‡¦ç†\n    if (len(train_loader) % ACCUMULATION_STEPS) != 0:\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n    \n    avg_train_loss = train_loss / len(train_loader)\n    history['train_loss'].append(avg_train_loss)\n    \n    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n    torch.cuda.empty_cache()\n    \n    # Validation\n    val_predictions, val_labels, val_loss = evaluate_model(model, val_loader, device)\n    val_mae = mean_absolute_error(val_labels, val_predictions)\n    \n    history['val_loss'].append(val_loss)\n    history['val_mae'].append(val_mae)\n    \n    print(f\"\\nğŸ“Š Results:\")\n    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n    print(f\"  Val Loss: {val_loss:.4f}\")\n    print(f\"  Val MAE: {val_mae:.4f}\")\n    \n    # GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated(0) / 1e9\n        reserved = torch.cuda.memory_reserved(0) / 1e9\n        print(f\"  GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n    \n    # Early Stopping & Checkpoint\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience_counter = 0\n        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': val_loss,\n        }, f\"{CHECKPOINT_DIR}/best_model.pt\")\n        print(\"  âœ… Best model saved!\")\n    else:\n        patience_counter += 1\n        print(f\"  âš ï¸  Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n        if patience_counter >= EARLY_STOPPING_PATIENCE:\n            print(f\"\\nğŸ›‘ Early stopping at epoch {epoch+1}\")\n            break\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… å­¦ç¿’å®Œäº†\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VXbK6TV32eB3",
    "outputId": "144afd58-e487-4dfd-edad-5159c2be7c5d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593,
     "referenced_widgets": [
      "dd2caf3387764a0e851b2aaddc1a6157",
      "e5aef812a0b74d45a60654eb9c24bf88",
      "82518c9499344f09bbb0d0c44ec8aef7",
      "111a301f5ac64358bad00458836c0d0f",
      "628ae484095f4d1695ab37b45293ba4c",
      "862acba8c4c84655852edb64abd0c0c8",
      "e8f4d90e718846a99f467fd40072dce7",
      "662695c169b64b1185954a810b27bf16",
      "5f1d7fd287964dcaa0c6c88f36a201c8",
      "1d8c4350cc40488092887c7074b25295",
      "583f2f53772648c486c9dc1b4e24b097"
     ]
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "å­¦ç¿’é–‹å§‹\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Epoch 1/20\n",
      "======================================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/47 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd2caf3387764a0e851b2aaddc1a6157"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 246.12 MiB is free. Process 2468 has 14.50 GiB memory in use. Of the allocated memory 14.33 GiB is allocated by PyTorch, and 42.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-192856117.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3358208549.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# LUKE encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         outputs = self.luke(\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/luke/modeling_luke.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, entity_ids, entity_attention_mask, entity_token_type_ids, entity_position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0;31m# Fourth, send embeddings through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    972\u001b[0m             \u001b[0mword_embedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mentity_embedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/luke/modeling_luke.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_hidden_states, entity_hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    692\u001b[0m                 \u001b[0mword_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m                 \u001b[0mentity_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/luke/modeling_luke.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_hidden_states, entity_hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mword_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_hidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mword_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mentity_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/luke/modeling_luke.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_hidden_states, entity_hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    551\u001b[0m     ):\n\u001b[1;32m    552\u001b[0m         \u001b[0mword_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_hidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0mword_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mentity_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/luke/modeling_luke.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_hidden_states, entity_hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 246.12 MiB is free. Process 2468 has 14.50 GiB memory in use. Of the allocated memory 14.33 GiB is allocated by PyTorch, and 42.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# å­¦ç¿’å±¥æ­´\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_mae': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"å­¦ç¿’é–‹å§‹\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# å­¦ç¿’ãƒ«ãƒ¼ãƒ—\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(input_ids, attention_mask)\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    val_predictions, val_labels, val_loss = evaluate_model(model, val_loader, device)\n",
    "    val_mae = mean_absolute_error(val_labels, val_predictions)\n",
    "\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_mae'].append(val_mae)\n",
    "\n",
    "    print(f\"\\nğŸ“Š Results:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "    # Early Stopping & Checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, f\"{CHECKPOINT_DIR}/best_model.pt\")\n",
    "        print(\"  âœ… Best model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  âš ï¸  Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\nğŸ›‘ Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… å­¦ç¿’å®Œäº†\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-q5Bk_v2eB3"
   },
   "source": [
    "## 7. ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogvVgxMJ2eB3"
   },
   "outputs": [],
   "source": [
    "# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
    "print(\"=\"*70)\n",
    "print(\"ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "checkpoint = torch.load(f\"{CHECKPOINT_DIR}/best_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"âœ… Best model loaded! (Epoch {checkpoint['epoch']+1}, Val Loss: {checkpoint['val_loss']:.4f})\")\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡ä¸­...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_predictions, test_labels, test_loss = evaluate_model(model, test_loader, device)\n",
    "print(f\"\\nğŸ“Š Test Loss: {test_loss:.4f}\")\n",
    "print(f\"ğŸ“Š Test MAE: {mean_absolute_error(test_labels, test_predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBy0NRIg2eB3"
   },
   "outputs": [],
   "source": [
    "# è©³ç´°è©•ä¾¡é–¢æ•°\n",
    "def compute_metrics(predictions, labels, trait_names=BIG_FIVE_TRAITS):\n",
    "    \"\"\"\n",
    "    å›å¸°æŒ‡æ¨™ã¨åˆ†é¡æŒ‡æ¨™ã®ä¸¡æ–¹ã‚’è¨ˆç®—\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # ä¸­å¤®å€¤ã‚’è¨ˆç®—ï¼ˆåˆ†é¡ç”¨ï¼‰\n",
    "    medians = np.median(labels, axis=0)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Personality Recognition Results\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # å›å¸°æŒ‡æ¨™\n",
    "    print(\"\\nRegression Metrics:\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Trait':<18} {'MAE':>8} {'RMSE':>8} {'Pearson':>10} {'Spearman':>10}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    regression_metrics = []\n",
    "    for i, trait in enumerate(trait_names):\n",
    "        pred = predictions[:, i]\n",
    "        true = labels[:, i]\n",
    "\n",
    "        mae = mean_absolute_error(true, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(true, pred))\n",
    "\n",
    "        # ç›¸é–¢ä¿‚æ•°ï¼ˆpå€¤ã‚‚è¨ˆç®—ï¼‰\n",
    "        pearson_corr, pearson_p = pearsonr(true, pred)\n",
    "        spearman_corr, spearman_p = spearmanr(true, pred)\n",
    "\n",
    "        regression_metrics.append({\n",
    "            'trait': trait,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'pearson': pearson_corr,\n",
    "            'spearman': spearman_corr\n",
    "        })\n",
    "\n",
    "        print(f\"{trait:<18} {mae:>8.4f} {rmse:>8.4f} {pearson_corr:>10.3f} {spearman_corr:>10.3f}\")\n",
    "\n",
    "    # å¹³å‡\n",
    "    avg_mae = np.mean([m['mae'] for m in regression_metrics])\n",
    "    avg_rmse = np.mean([m['rmse'] for m in regression_metrics])\n",
    "    avg_pearson = np.mean([m['pearson'] for m in regression_metrics])\n",
    "    avg_spearman = np.mean([m['spearman'] for m in regression_metrics])\n",
    "\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Average':<18} {avg_mae:>8.4f} {avg_rmse:>8.4f} {avg_pearson:>10.3f} {avg_spearman:>10.3f}\")\n",
    "\n",
    "    # åˆ†é¡æŒ‡æ¨™ï¼ˆä¸­å¤®å€¤ã§2å€¤åŒ–ï¼‰\n",
    "    print(\"\\n\\nClassification Metrics (High/Low by median):\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Trait':<18} {'Acc':>6} {'Bal Acc':>8} {'Prec':>6} {'Rec':>6} {'F1':>6}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    classification_metrics = []\n",
    "    for i, trait in enumerate(trait_names):\n",
    "        pred = predictions[:, i]\n",
    "        true = labels[:, i]\n",
    "        median = medians[i]\n",
    "\n",
    "        # 2å€¤åŒ–\n",
    "        pred_binary = (pred > median).astype(int)\n",
    "        true_binary = (true > median).astype(int)\n",
    "\n",
    "        acc = accuracy_score(true_binary, pred_binary)\n",
    "        bal_acc = balanced_accuracy_score(true_binary, pred_binary)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "            true_binary, pred_binary, average='binary', zero_division=0\n",
    "        )\n",
    "\n",
    "        classification_metrics.append({\n",
    "            'trait': trait,\n",
    "            'accuracy': acc,\n",
    "            'balanced_accuracy': bal_acc,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'f1': f1\n",
    "        })\n",
    "\n",
    "        print(f\"{trait:<18} {acc:>6.1%} {bal_acc:>8.1%} {prec:>6.2f} {rec:>6.2f} {f1:>6.2f}\")\n",
    "\n",
    "    # å¹³å‡\n",
    "    avg_acc = np.mean([m['accuracy'] for m in classification_metrics])\n",
    "    avg_bal_acc = np.mean([m['balanced_accuracy'] for m in classification_metrics])\n",
    "    avg_prec = np.mean([m['precision'] for m in classification_metrics])\n",
    "    avg_rec = np.mean([m['recall'] for m in classification_metrics])\n",
    "    avg_f1 = np.mean([m['f1'] for m in classification_metrics])\n",
    "\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Average':<18} {avg_acc:>6.1%} {avg_bal_acc:>8.1%} {avg_prec:>6.2f} {avg_rec:>6.2f} {avg_f1:>6.2f}\")\n",
    "\n",
    "    # è«–æ–‡ã¨ã®æ¯”è¼ƒ\n",
    "    print(\"\\n\\nComparison with Paper (Fu et al. 2024):\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Paper Balanced Accuracy (Monologue): 60.4%\")\n",
    "    print(f\"Our Balanced Accuracy:                {avg_bal_acc:.1%}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    results['regression'] = regression_metrics\n",
    "    results['classification'] = classification_metrics\n",
    "    results['averages'] = {\n",
    "        'mae': avg_mae,\n",
    "        'rmse': avg_rmse,\n",
    "        'pearson': avg_pearson,\n",
    "        'spearman': avg_spearman,\n",
    "        'accuracy': avg_acc,\n",
    "        'balanced_accuracy': avg_bal_acc\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# è©•ä¾¡å®Ÿè¡Œ\n",
    "results = compute_metrics(test_predictions, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k1m9orP2eB3"
   },
   "source": [
    "## 8. å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFBqHgx_2eB3"
   },
   "outputs": [],
   "source": [
    "# å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–\n",
    "def plot_training_history(history):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss (MAE)')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # MAE\n",
    "    axes[1].plot(history['val_mae'], label='Val MAE', marker='o', color='green')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('MAE')\n",
    "    axes[1].set_title('Validation MAE')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{CHECKPOINT_DIR}/training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyUhZgMa2eB3"
   },
   "outputs": [],
   "source": [
    "# æ•£å¸ƒå›³: äºˆæ¸¬ vs çœŸå€¤\n",
    "def plot_predictions(predictions, labels, trait_names=BIG_FIVE_TRAITS):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, trait in enumerate(trait_names):\n",
    "        ax = axes[i]\n",
    "\n",
    "        pred = predictions[:, i]\n",
    "        true = labels[:, i]\n",
    "\n",
    "        # æ•£å¸ƒå›³\n",
    "        ax.scatter(true, pred, alpha=0.5, s=20)\n",
    "\n",
    "        # ç†æƒ³ç·š\n",
    "        min_val = min(true.min(), pred.min())\n",
    "        max_val = max(true.max(), pred.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Ideal')\n",
    "\n",
    "        # ç›¸é–¢ä¿‚æ•°\n",
    "        pearson, _ = pearsonr(true, pred)\n",
    "        mae = mean_absolute_error(true, pred)\n",
    "\n",
    "        ax.set_xlabel('True Score')\n",
    "        ax.set_ylabel('Predicted Score')\n",
    "        ax.set_title(f'{trait}\\nPearson: {pearson:.3f}, MAE: {mae:.3f}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # æœ€å¾Œã®axesã‚’éè¡¨ç¤º\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{CHECKPOINT_DIR}/predictions_scatter.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# æ•£å¸ƒå›³ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "plot_predictions(test_predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zj5mQ0Ij2eB3"
   },
   "outputs": [],
   "source": [
    "# æ··åŒè¡Œåˆ—ï¼ˆåˆ†é¡ã¨ã—ã¦è©•ä¾¡ï¼‰\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrices(predictions, labels, trait_names=BIG_FIVE_TRAITS):\n",
    "    medians = np.median(labels, axis=0)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, trait in enumerate(trait_names):\n",
    "        ax = axes[i]\n",
    "\n",
    "        pred = predictions[:, i]\n",
    "        true = labels[:, i]\n",
    "        median = medians[i]\n",
    "\n",
    "        # 2å€¤åŒ–\n",
    "        pred_binary = (pred > median).astype(int)\n",
    "        true_binary = (true > median).astype(int)\n",
    "\n",
    "        # æ··åŒè¡Œåˆ—\n",
    "        cm = confusion_matrix(true_binary, pred_binary)\n",
    "\n",
    "        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                    xticklabels=['Low', 'High'],\n",
    "                    yticklabels=['Low', 'High'])\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('True')\n",
    "        ax.set_title(f'{trait} - Confusion Matrix')\n",
    "\n",
    "    # æœ€å¾Œã®axesã‚’éè¡¨ç¤º\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{CHECKPOINT_DIR}/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# æ··åŒè¡Œåˆ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "plot_confusion_matrices(test_predictions, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv3ZhqGH2eB3"
   },
   "source": [
    "## 9. ã¾ã¨ã‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ä»¥ä¸‹ã‚’å®Ÿè£…ã—ã¾ã—ãŸï¼š\n",
    "\n",
    "1. âœ… RealPersonaChatãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\n",
    "2. âœ… LUKEå›å¸°ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰\n",
    "3. âœ… ãƒ¢ãƒãƒ­ãƒ¼ã‚°è¨­å®šã§ã®å­¦ç¿’\n",
    "4. âœ… å›å¸°æŒ‡æ¨™ï¼ˆMAE, RMSE, ç›¸é–¢ï¼‰ã¨åˆ†é¡æŒ‡æ¨™ï¼ˆAcc, F1ï¼‰ã®è©•ä¾¡\n",
    "5. âœ… è«–æ–‡ï¼ˆFu et al. 2024ï¼‰ã¨ã®æ¯”è¼ƒ\n",
    "\n",
    "**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:**\n",
    "- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å®Ÿéš›ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦å®Ÿè¡Œ\n",
    "- å­¦ç¿’ã‚’å®Ÿè¡Œã—ã€çµæœã‚’åˆ†æ\n",
    "- ç™ºè¡¨è³‡æ–™ã®ä½œæˆ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}